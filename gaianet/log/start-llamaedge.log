[2024-09-30 19:24:18.682] [info] rag_api_server in src/main.rs:154: server_version: 0.9.4
[2024-09-30 19:24:18.682] [info] rag_api_server in src/main.rs:162: model_name: Phi-3-mini-4k-instruct,Nomic-embed-text-v1.5
[2024-09-30 19:24:18.682] [info] rag_api_server in src/main.rs:170: model_alias: default,embedding
[2024-09-30 19:24:18.682] [info] rag_api_server in src/main.rs:184: ctx_size: 4096,512
[2024-09-30 19:24:18.682] [info] rag_api_server in src/main.rs:198: batch_size: 4096,512
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:212: prompt_template: phi-3-chat,embedding
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:220: n_predict: 1024
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:223: n_gpu_layers: 100
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:236: threads: 2
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:250: rag_prompt: You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:271: qdrant_url: http://127.0.0.1:6333
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:274: qdrant_collection_name: default
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:277: qdrant_limit: 1
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:280: qdrant_score_threshold: 0.5
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:291: chunk_capacity: 100
[2024-09-30 19:24:18.683] [info] rag_api_server in src/main.rs:294: rag_policy: system-message
[2024-09-30 19:24:18.683] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:434: Initializing the core context for RAG scenarios
[2024-09-30 19:24:18.684] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-30 19:24:18.684] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 35 key-value pairs and 195 tensors from Phi-3-mini-4k-instruct-Q5_K_M.gguf (version GGUF V3 (latest))
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = phi3
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.type str              = model
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                               general.name str              = Phi 3 Mini 4k Instruct
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                           general.finetune str              = 4k-instruct
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                           general.basename str              = Phi-3
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:                         general.size_label str              = mini
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:                            general.license str              = mit
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:                               general.tags arr[str,3]       = ["nlp", "code", "text-generation"]
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                          general.languages arr[str,1]       = ["en"]
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                        phi3.context_length u32              = 4096
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                           phi3.block_count u32              = 32
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  20:                          general.file_type u32              = 17
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 2047
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = llama
[2024-09-30 19:24:18.690] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = default
[2024-09-30 19:24:18.693] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,32064]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
[2024-09-30 19:24:18.704] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  25:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 32000
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 32000
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  32:               tokenizer.ggml.add_eos_token bool             = false
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  33:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  34:               general.quantization_version u32              = 2
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:   65 tensors
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q5_K:   81 tensors
[2024-09-30 19:24:18.706] [info] [WASI-NN] llama.cpp: llama_model_loader: - type q6_K:   49 tensors
[2024-09-30 19:24:18.717] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 14
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.1685 MB
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = phi3
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = SPM
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 32064
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 4096
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 3072
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 32
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 32
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 32
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 96
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 2047
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 96
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 96
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 3072
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 3072
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 0.0e+00
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 8192
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 1
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 10000.0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 4096
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 3B
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = Q5_K - Medium
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 3.82 B
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 2.62 GiB (5.89 BPW) 
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = Phi 3 Mini 4k Instruct
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 1 '<s>'
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: UNK token        = 0 '<unk>'
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 13 '<0x0A>'
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOT token        = 32007 '<|end|>'
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 48
[2024-09-30 19:24:18.720] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.10 MiB
[2024-09-30 19:24:18.792] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =  2684.15 MiB
[2024-09-30 19:24:18.793] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:24:18.793] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-30 19:24:18.793] [info] [WASI-NN] GGML backend: LLAMA_COMMIT 8f1d81a0
[2024-09-30 19:24:18.793] [info] [WASI-NN] GGML backend: LLAMA_BUILD_NUMBER 3651
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: loaded meta data with 22 key-value pairs and 112 tensors from nomic-embed-text-v1.5.f16.gguf (version GGUF V3 (latest))
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   8:                          general.file_type u32              = 1
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
[2024-09-30 19:24:18.796] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
[2024-09-30 19:24:18.800] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
[2024-09-30 19:24:18.808] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
[2024-09-30 19:24:18.811] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
[2024-09-30 19:24:18.811] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
[2024-09-30 19:24:18.811] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
[2024-09-30 19:24:18.811] [info] [WASI-NN] llama.cpp: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
[2024-09-30 19:24:18.811] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f32:   51 tensors
[2024-09-30 19:24:18.811] [info] [WASI-NN] llama.cpp: llama_model_loader: - type  f16:   61 tensors
[2024-09-30 19:24:18.818] [info] [WASI-NN] llama.cpp: llm_load_vocab: special tokens cache size = 5
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_vocab: token to piece cache size = 0.2032 MB
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: format           = GGUF V3 (latest)
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: arch             = nomic-bert
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab type       = WPM
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_vocab          = 30522
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_merges         = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: vocab_only       = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_train      = 2048
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd           = 768
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_layer          = 12
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head           = 12
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_head_kv        = 12
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_rot            = 64
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_swa            = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_k    = 64
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_head_v    = 64
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_gqa            = 1
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_k_gqa     = 768
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_embd_v_gqa     = 768
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_eps       = 1.0e-12
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_clamp_kqv      = 0.0e+00
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_max_alibi_bias = 0.0e+00
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: f_logit_scale    = 0.0e+00
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ff             = 3072
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert         = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_expert_used    = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: causal attn      = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: pooling type     = 1
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope type        = 2
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope scaling     = linear
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_base_train  = 1000.0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: freq_scale_train = 1
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: n_ctx_orig_yarn  = 2048
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: rope_finetuned   = unknown
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_conv       = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_inner      = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_d_state      = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_rank      = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: ssm_dt_b_c_rms   = 0
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model type       = 137M
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model ftype      = F16
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model params     = 136.73 M
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: general.name     = nomic-embed-text-v1.5
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: BOS token        = 101 '[CLS]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: EOS token        = 102 '[SEP]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: UNK token        = 100 '[UNK]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: SEP token        = 102 '[SEP]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: PAD token        = 0 '[PAD]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: CLS token        = 101 '[CLS]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: MASK token       = 103 '[MASK]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: LF token         = 0 '[PAD]'
[2024-09-30 19:24:18.820] [info] [WASI-NN] llama.cpp: llm_load_print_meta: max token length = 21
[2024-09-30 19:24:18.821] [info] [WASI-NN] llama.cpp: llm_load_tensors: ggml ctx size =    0.05 MiB
[2024-09-30 19:24:18.828] [info] [WASI-NN] llama.cpp: llm_load_tensors:        CPU buffer size =   260.86 MiB
[2024-09-30 19:24:18.828] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:24:18.828] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
[2024-09-30 19:24:18.829] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:489: running mode: rag
[2024-09-30 19:24:18.829] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:502: The core context for RAG scenarios has been initialized
[2024-09-30 19:24:18.829] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:512: Getting the plugin info
[2024-09-30 19:24:18.829] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:18.829] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:18.829] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:592: Getting the plugin info by the graph named Phi-3-mini-4k-instruct
[2024-09-30 19:24:18.829] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:24:18.829] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:24:18.829] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:652: Plugin info: b3651(commit 8f1d81a0)
[2024-09-30 19:24:18.829] [info] rag_api_server in src/main.rs:404: plugin_ggml_version: b3651 (commit 8f1d81a0)
[2024-09-30 19:24:18.829] [info] rag_api_server in src/main.rs:414: socket_address: 0.0.0.0:8081
[2024-09-30 19:24:18.829] [info] rag_api_server in src/main.rs:421: gaianet_node_version: 0.4.3
[2024-09-30 19:24:19.219] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:59334, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:19.222] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:19.222] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/index.html
[2024-09-30 19:24:19.222] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:19.222] [info] rag_api_server in src/main.rs:517: response_body_size: 10434
[2024-09-30 19:24:19.222] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:19.222] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:19.264] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:19.264] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/css/41ab283a84d31b77.css
[2024-09-30 19:24:19.264] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:19.264] [info] rag_api_server in src/main.rs:517: response_body_size: 45475
[2024-09-30 19:24:19.265] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:19.265] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:19.270] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:19.270] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/api-tutorial.png
[2024-09-30 19:24:19.271] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:19.271] [info] rag_api_server in src/main.rs:517: response_body_size: 969
[2024-09-30 19:24:19.271] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:19.271] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:19.471] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:59348, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:19.473] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:59356, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:20.410] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.411] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/APITutorial.png
[2024-09-30 19:24:20.411] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.411] [info] rag_api_server in src/main.rs:517: response_body_size: 575
[2024-09-30 19:24:20.411] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.411] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.412] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.412] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/logo-big.png
[2024-09-30 19:24:20.412] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.413] [info] rag_api_server in src/main.rs:517: response_body_size: 6287
[2024-09-30 19:24:20.413] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.413] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.414] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.414] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/webpack-f0ae49128044e751.js
[2024-09-30 19:24:20.414] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.414] [info] rag_api_server in src/main.rs:517: response_body_size: 1592
[2024-09-30 19:24:20.414] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.414] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.416] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.416] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/framework-73b8966a3c579ab0.js
[2024-09-30 19:24:20.418] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.418] [info] rag_api_server in src/main.rs:517: response_body_size: 141074
[2024-09-30 19:24:20.418] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.418] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.419] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.419] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/main-6260d066cf2cd7b1.js
[2024-09-30 19:24:20.420] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.420] [info] rag_api_server in src/main.rs:517: response_body_size: 90428
[2024-09-30 19:24:20.420] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.420] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.424] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.424] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/_app-77033f8e040fdfcc.js
[2024-09-30 19:24:20.431] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.431] [info] rag_api_server in src/main.rs:517: response_body_size: 115704
[2024-09-30 19:24:20.432] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.432] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.433] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.433] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/175675d1-5c66df25ff141d62.js
[2024-09-30 19:24:20.434] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.434] [info] rag_api_server in src/main.rs:517: response_body_size: 267018
[2024-09-30 19:24:20.434] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.434] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.436] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.436] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/737-952dc72c8d5e14c2.js
[2024-09-30 19:24:20.438] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.438] [info] rag_api_server in src/main.rs:517: response_body_size: 1007606
[2024-09-30 19:24:20.438] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.438] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.442] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.442] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/index-ff5f66d3c316acef.js
[2024-09-30 19:24:20.443] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.443] [info] rag_api_server in src/main.rs:517: response_body_size: 78983
[2024-09-30 19:24:20.443] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.443] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.453] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.453] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_buildManifest.js
[2024-09-30 19:24:20.454] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.454] [info] rag_api_server in src/main.rs:517: response_body_size: 367
[2024-09-30 19:24:20.454] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.454] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.457] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.457] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_ssgManifest.js
[2024-09-30 19:24:20.458] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.458] [info] rag_api_server in src/main.rs:517: response_body_size: 77
[2024-09-30 19:24:20.458] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.458] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.516] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.516] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/media/MonaspaceNeon.f8ddff7e.ttf
[2024-09-30 19:24:20.517] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:20.517] [info] rag_api_server in src/main.rs:517: response_body_size: 411656
[2024-09-30 19:24:20.518] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:20.518] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:20.716] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:55788, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:20.717] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:55804, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:20.718] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:55820, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:20.719] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:55824, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:20.931] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:20.932] [info] rag_api_server in src/main.rs:499: endpoint: /favicon.ico
[2024-09-30 19:24:20.932] [error] rag_api_server in src/main.rs:524: response_version: HTTP/1.1
[2024-09-30 19:24:20.932] [error] rag_api_server in src/main.rs:526: response_body_size: 0
[2024-09-30 19:24:20.932] [error] rag_api_server in src/main.rs:528: response_status: 404
[2024-09-30 19:24:20.932] [error] rag_api_server in src/main.rs:530: response_is_success: false
[2024-09-30 19:24:20.933] [error] rag_api_server in src/main.rs:532: response_is_client_error: true
[2024-09-30 19:24:20.933] [error] rag_api_server in src/main.rs:534: response_is_server_error: false
[2024-09-30 19:24:21.038] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:21.039] [info] rag_api_server in src/main.rs:499: endpoint: /v1/models
[2024-09-30 19:24:21.039] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:24: Handling the coming model list request.
[2024-09-30 19:24:21.039] [info] llama_core::models in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/models.rs:9: List models
[2024-09-30 19:24:21.039] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:71: Send the model list response.
[2024-09-30 19:24:21.039] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:21.039] [info] rag_api_server in src/main.rs:517: response_body_size: 219
[2024-09-30 19:24:21.039] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:21.039] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:21.471] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:21.471] [info] rag_api_server in src/main.rs:499: endpoint: /config_pub.json
[2024-09-30 19:24:21.472] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:21.472] [info] rag_api_server in src/main.rs:517: response_body_size: 1566
[2024-09-30 19:24:21.472] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:21.472] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:21.481] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:21.481] [info] rag_api_server in src/main.rs:499: endpoint: /logo-opacity.png
[2024-09-30 19:24:21.482] [error] rag_api_server in src/main.rs:524: response_version: HTTP/1.1
[2024-09-30 19:24:21.482] [error] rag_api_server in src/main.rs:526: response_body_size: 0
[2024-09-30 19:24:21.482] [error] rag_api_server in src/main.rs:528: response_status: 404
[2024-09-30 19:24:21.482] [error] rag_api_server in src/main.rs:530: response_is_success: false
[2024-09-30 19:24:21.482] [error] rag_api_server in src/main.rs:532: response_is_client_error: true
[2024-09-30 19:24:21.482] [error] rag_api_server in src/main.rs:534: response_is_server_error: false
[2024-09-30 19:24:30.460] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:43746, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:30.461] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 97
[2024-09-30 19:24:30.461] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-30 19:24:30.461] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:24:30.461] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:24:30.462] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-e28cd825-fae5-4ced-936f-5203fb865f86
[2024-09-30 19:24:30.462] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:24:30.462] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: What is your name?
[2024-09-30 19:24:30.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:24:30.462] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:24:30.462] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:30.462] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:30.462] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:24:30.462] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:30.462] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:30.462] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:296: Update metadata for the model named Nomic-embed-text-v1.5
[2024-09-30 19:24:30.462] [info] llama_core::graph in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/graph.rs:314: Metadata updated successfully.
[2024-09-30 19:24:30.462] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:24:30.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:24:30.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:24:30.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:24:30.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:30.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:24:30.462] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:30.468] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:24:30.468] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:24:30.468] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:24:30.469] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:24:30.469] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:24:30.469] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:30.470] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:24:30.470] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:24:30.470] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:24:30.470] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:24:30.470] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:30.470] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:24:30.470] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:30.473] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:24:30.473] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:24:30.473] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:24:30.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:24:30.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:24:30.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:30.511] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:24:30.511] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   11690.77 ms
[2024-09-30 19:24:30.511] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:24:30.511] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      36.18 ms /     7 tokens (    5.17 ms per token,   193.48 tokens per second)
[2024-09-30 19:24:30.511] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:24:30.511] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   11690.50 ms /     8 tokens
[2024-09-30 19:24:30.512] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:24:30.512] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11281
[2024-09-30 19:24:30.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:24:30.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:24:30.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:24:30.518] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 7, completion tokens: 0
[2024-09-30 19:24:30.518] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 7 prompt tokens, 0 comletion tokens
[2024-09-30 19:24:30.518] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:24:30.518] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:24:30.518] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:24:30.518] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:30.518] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:30.518] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:24:30.537] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:24:30.537] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:24:30.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:24:30.538] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:30.538] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-e28cd825-fae5-4ced-936f-5203fb865f86
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:24:30.538] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:30.538] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:24:30.538] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:24:30.538] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:24:30.538] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:24:30.538] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:30.538] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:24:30.538] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:30.997] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:24:30.997] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:24:30.997] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:24:31.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:24:31.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:24:31.000] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:31.004] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:31.004] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:24:31.004] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-30 19:24:31.004] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 0
[2024-09-30 19:24:31.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
What is your name?<|end|>
<|assistant|>
[2024-09-30 19:24:31.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:24:31.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:24:31.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:24:31.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:24:31.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:24:31.004] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:31.004] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:24:31.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:24:31.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:24:31.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:24:31.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:31.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:24:31.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:31.192] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:24:31.192] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:24:31.192] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:24:31.192] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:24:31.192] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:24:31.192] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:31.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:24:31.196] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:31.196] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:24:31.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:24:31.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:24:31.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:24:31.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:31.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:24:31.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:31.379] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:24:31.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:24:31.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:24:31.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:24:31.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:24:31.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:36.776] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:24:36.776] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:24:36.776] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   15825.08 ms
[2024-09-30 19:24:36.776] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       2.80 ms /    14 runs   (    0.20 ms per token,  4996.43 tokens per second)
[2024-09-30 19:24:36.776] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3165.78 ms /    25 tokens (  126.63 ms per token,     7.90 tokens per second)
[2024-09-30 19:24:36.776] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2226.73 ms /    13 runs   (  171.29 ms per token,     5.84 tokens per second)
[2024-09-30 19:24:36.776] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   18054.68 ms /    38 tokens
[2024-09-30 19:24:36.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:24:36.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 46
[2024-09-30 19:24:36.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an AI developed by Microsoft.<|end|>
[2024-09-30 19:24:36.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:24:36.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an AI developed by Microsoft.
[2024-09-30 19:24:36.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:36.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:24:36.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:24:36.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 25, completion tokens: 14
[2024-09-30 19:24:36.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 25, completion tokens: 14
[2024-09-30 19:24:36.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:24:36.780] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:24:36.780] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:24:36.780] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:36.780] [info] rag_api_server in src/main.rs:517: response_body_size: 350
[2024-09-30 19:24:36.780] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:36.780] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:36.781] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:36.782] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/index.html
[2024-09-30 19:24:36.782] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:36.782] [info] rag_api_server in src/main.rs:517: response_body_size: 10434
[2024-09-30 19:24:36.782] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:36.782] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:36.825] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:36.825] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/css/41ab283a84d31b77.css
[2024-09-30 19:24:36.826] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:36.826] [info] rag_api_server in src/main.rs:517: response_body_size: 45475
[2024-09-30 19:24:36.826] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:36.826] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:36.828] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:36.828] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/api-tutorial.png
[2024-09-30 19:24:36.828] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:36.828] [info] rag_api_server in src/main.rs:517: response_body_size: 969
[2024-09-30 19:24:36.828] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:36.828] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.174] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.174] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/icons/APITutorial.png
[2024-09-30 19:24:37.175] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.175] [info] rag_api_server in src/main.rs:517: response_body_size: 575
[2024-09-30 19:24:37.175] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.175] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.176] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.176] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/main-6260d066cf2cd7b1.js
[2024-09-30 19:24:37.176] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.176] [info] rag_api_server in src/main.rs:517: response_body_size: 90428
[2024-09-30 19:24:37.176] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.176] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.177] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.177] [info] rag_api_server in src/main.rs:499: endpoint: /chatbot-ui/logo-big.png
[2024-09-30 19:24:37.178] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.178] [info] rag_api_server in src/main.rs:517: response_body_size: 6287
[2024-09-30 19:24:37.178] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.178] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.179] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.179] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/_app-77033f8e040fdfcc.js
[2024-09-30 19:24:37.180] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.180] [info] rag_api_server in src/main.rs:517: response_body_size: 115704
[2024-09-30 19:24:37.180] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.180] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.181] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.181] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/framework-73b8966a3c579ab0.js
[2024-09-30 19:24:37.182] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.182] [info] rag_api_server in src/main.rs:517: response_body_size: 141074
[2024-09-30 19:24:37.182] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.182] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.183] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.183] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/webpack-f0ae49128044e751.js
[2024-09-30 19:24:37.184] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.184] [info] rag_api_server in src/main.rs:517: response_body_size: 1592
[2024-09-30 19:24:37.184] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.184] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.185] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.186] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/737-952dc72c8d5e14c2.js
[2024-09-30 19:24:37.187] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.187] [info] rag_api_server in src/main.rs:517: response_body_size: 1007606
[2024-09-30 19:24:37.187] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.188] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.191] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.192] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/175675d1-5c66df25ff141d62.js
[2024-09-30 19:24:37.193] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.193] [info] rag_api_server in src/main.rs:517: response_body_size: 267018
[2024-09-30 19:24:37.193] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.193] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.195] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.195] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/chunks/pages/index-ff5f66d3c316acef.js
[2024-09-30 19:24:37.196] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.196] [info] rag_api_server in src/main.rs:517: response_body_size: 78983
[2024-09-30 19:24:37.196] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.196] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.198] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.198] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_buildManifest.js
[2024-09-30 19:24:37.199] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.199] [info] rag_api_server in src/main.rs:517: response_body_size: 367
[2024-09-30 19:24:37.199] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.199] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.201] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.201] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/AV-ppCFdiUch5Aa5WIivp/_ssgManifest.js
[2024-09-30 19:24:37.201] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.201] [info] rag_api_server in src/main.rs:517: response_body_size: 77
[2024-09-30 19:24:37.201] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.201] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.225] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.225] [info] rag_api_server in src/main.rs:499: endpoint: /_next/static/media/MonaspaceNeon.f8ddff7e.ttf
[2024-09-30 19:24:37.226] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.226] [info] rag_api_server in src/main.rs:517: response_body_size: 411656
[2024-09-30 19:24:37.226] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.226] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.526] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.526] [info] rag_api_server in src/main.rs:499: endpoint: /favicon.ico
[2024-09-30 19:24:37.526] [error] rag_api_server in src/main.rs:524: response_version: HTTP/1.1
[2024-09-30 19:24:37.526] [error] rag_api_server in src/main.rs:526: response_body_size: 0
[2024-09-30 19:24:37.527] [error] rag_api_server in src/main.rs:528: response_status: 404
[2024-09-30 19:24:37.527] [error] rag_api_server in src/main.rs:530: response_is_success: false
[2024-09-30 19:24:37.527] [error] rag_api_server in src/main.rs:532: response_is_client_error: true
[2024-09-30 19:24:37.527] [error] rag_api_server in src/main.rs:534: response_is_server_error: false
[2024-09-30 19:24:37.565] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.565] [info] rag_api_server in src/main.rs:499: endpoint: /v1/models
[2024-09-30 19:24:37.565] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:24: Handling the coming model list request.
[2024-09-30 19:24:37.565] [info] llama_core::models in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/models.rs:9: List models
[2024-09-30 19:24:37.565] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:71: Send the model list response.
[2024-09-30 19:24:37.566] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.566] [info] rag_api_server in src/main.rs:517: response_body_size: 219
[2024-09-30 19:24:37.566] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.566] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.679] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.679] [info] rag_api_server in src/main.rs:499: endpoint: /config_pub.json
[2024-09-30 19:24:37.680] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:37.680] [info] rag_api_server in src/main.rs:517: response_body_size: 1566
[2024-09-30 19:24:37.680] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:37.680] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:37.685] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:37.685] [info] rag_api_server in src/main.rs:499: endpoint: /logo-opacity.png
[2024-09-30 19:24:37.685] [error] rag_api_server in src/main.rs:524: response_version: HTTP/1.1
[2024-09-30 19:24:37.685] [error] rag_api_server in src/main.rs:526: response_body_size: 0
[2024-09-30 19:24:37.685] [error] rag_api_server in src/main.rs:528: response_status: 404
[2024-09-30 19:24:37.685] [error] rag_api_server in src/main.rs:530: response_is_success: false
[2024-09-30 19:24:37.686] [error] rag_api_server in src/main.rs:532: response_is_client_error: true
[2024-09-30 19:24:37.686] [error] rag_api_server in src/main.rs:534: response_is_server_error: false
[2024-09-30 19:24:40.805] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:39460, local_addr: 0.0.0.0:8081
[2024-09-30 19:24:40.806] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:24:40.806] [info] rag_api_server in src/main.rs:499: endpoint: /v1/info
[2024-09-30 19:24:40.806] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1677: Handling the coming server info request.
[2024-09-30 19:24:40.806] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:1724: Send the server info response.
[2024-09-30 19:24:40.806] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:40.806] [info] rag_api_server in src/main.rs:517: response_body_size: 803
[2024-09-30 19:24:40.806] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:40.806] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:42.016] [info] rag_api_server in src/main.rs:495: method: POST, http_version: HTTP/1.1, content-length: 360
[2024-09-30 19:24:42.017] [info] rag_api_server in src/main.rs:496: endpoint: /v1/chat/completions
[2024-09-30 19:24:42.017] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:24:42.018] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:24:42.018] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2
[2024-09-30 19:24:42.018] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:24:42.018] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: When did the SEC approve the Bitcoin Spot ETF?
[2024-09-30 19:24:42.019] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:24:42.019] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:24:42.019] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:42.019] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:42.019] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:24:42.019] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:42.019] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:42.019] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:24:42.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:24:42.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:24:42.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:24:42.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:42.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:24:42.019] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:42.025] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:24:42.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:24:42.025] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:24:42.028] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:24:42.028] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:24:42.028] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:42.030] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:24:42.030] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:24:42.030] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:24:42.030] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:24:42.030] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:42.030] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:24:42.030] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:42.044] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:24:42.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:24:42.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:24:42.047] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:24:42.047] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:24:42.047] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:42.109] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:24:42.109] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =   23288.03 ms
[2024-09-30 19:24:42.109] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:24:42.109] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      59.28 ms /    15 tokens (    3.95 ms per token,   253.04 tokens per second)
[2024-09-30 19:24:42.109] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:24:42.109] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =   23287.50 ms /    16 tokens
[2024-09-30 19:24:42.109] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:24:42.109] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11260
[2024-09-30 19:24:42.115] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:24:42.115] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:24:42.115] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-30 19:24:42.116] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 15, completion tokens: 0
[2024-09-30 19:24:42.116] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 15 prompt tokens, 0 comletion tokens
[2024-09-30 19:24:42.116] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:24:42.116] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:24:42.116] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:24:42.116] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:42.116] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:42.116] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:24:42.148] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 1
[2024-09-30 19:24:42.150] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:388: point: 0, score: 0.6608105, source: "- **2013**: The concept of Ethereum was proposed by Vitalik Buterin, aiming to create a more versatile blockchain platform that supports smart contracts and decentralized applications (DApps).\n- **2014**:\n    - Raised funds through an [Initial Coin Offering (ICO)](https://www.notion.so/ICO-Initial-Coin-Offering-d1f5ad5c27494950a674bf30462488b1?pvs=21).\n    - Ethereum Switzerland GmbH (EthSuisse) began the development of Ethereum software.\n- **July 30, 2015**: Ethereum launched its \"Frontier\" phase officially, marking the start of the Ethereum blockchain's operation.\n- **2016**:\n    - **March**: The \"Homestead\" was released as Ethereum’s first major upgrade, enhancing the platform's security and stability.\n    - **June**: The infamous \"DAO Attack\" happened, leading to a community split. The original chain continued as Ethereum Classic.\n- **2017**: Ethereum released the \"Byzantium\" and \"Constantinople\" upgrades, part of the \"Metropolis\" phase, aimed at further improving the network's privacy, security, and efficiency.\n- **December 2020**: The \"Beacon Chain\" of Ethereum 2.0 was launched, marking the beginning of the transition to a Proof-of-Stake (PoS) mechanism, a key step in reducing energy consumption and increasing transaction throughput for the Ethereum network.\n- **September 15, 2022**: \"The Merge\" was completed, officially transitioning Ethereum from a Proof-of-Work (PoW) consensus mechanism to a Proof-of-Stake (PoS), significantly reducing the network's energy consumption.\n- **2023**: The Shapella (Shanghai + Capella) upgrade further optimized network functionality, notably introducing the staking withdrawal feature, allowing stakers to withdraw their staking rewards.\n- **2024**:\n    - **Spot ETF Application Discussion**: On January 10, 2024, the U.S. Securities and Exchange Commission (SEC) approved the listing of a Bitcoin spot ETF, a significant milestone in cryptocurrency history, paving the way for the approval of other cryptocurrency ETFs. Although the SEC postponed the decision on Ethereum spot ETF applications from companies like BlackRock and Fidelity, it also sparked optimistic expectations for Ethereum's future development.\n    - **Cancun Upgrade Plan**: The Cancun upgrade, as an important update to Ethereum, is expected to further enhance the network's performance and scalability.\n"
[2024-09-30 19:24:42.150] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:87: Get the chat prompt template type from the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:42.151] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:122: prompt_template: phi-3-chat
[2024-09-30 19:24:42.151] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:567: rag_policy: system-message
[2024-09-30 19:24:42.151] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:571: context:
"- **2013**: The concept of Ethereum was proposed by Vitalik Buterin, aiming to create a more versatile blockchain platform that supports smart contracts and decentralized applications (DApps).\n- **2014**:\n    - Raised funds through an [Initial Coin Offering (ICO)](https://www.notion.so/ICO-Initial-Coin-Offering-d1f5ad5c27494950a674bf30462488b1?pvs=21).\n    - Ethereum Switzerland GmbH (EthSuisse) began the development of Ethereum software.\n- **July 30, 2015**: Ethereum launched its \"Frontier\" phase officially, marking the start of the Ethereum blockchain's operation.\n- **2016**:\n    - **March**: The \"Homestead\" was released as Ethereum’s first major upgrade, enhancing the platform's security and stability.\n    - **June**: The infamous \"DAO Attack\" happened, leading to a community split. The original chain continued as Ethereum Classic.\n- **2017**: Ethereum released the \"Byzantium\" and \"Constantinople\" upgrades, part of the \"Metropolis\" phase, aimed at further improving the network's privacy, security, and efficiency.\n- **December 2020**: The \"Beacon Chain\" of Ethereum 2.0 was launched, marking the beginning of the transition to a Proof-of-Stake (PoS) mechanism, a key step in reducing energy consumption and increasing transaction throughput for the Ethereum network.\n- **September 15, 2022**: \"The Merge\" was completed, officially transitioning Ethereum from a Proof-of-Work (PoW) consensus mechanism to a Proof-of-Stake (PoS), significantly reducing the network's energy consumption.\n- **2023**: The Shapella (Shanghai + Capella) upgrade further optimized network functionality, notably introducing the staking withdrawal feature, allowing stakers to withdraw their staking rewards.\n- **2024**:\n    - **Spot ETF Application Discussion**: On January 10, 2024, the U.S. Securities and Exchange Commission (SEC) approved the listing of a Bitcoin spot ETF, a significant milestone in cryptocurrency history, paving the way for the approval of other cryptocurrency ETFs. Although the SEC postponed the decision on Ethereum spot ETF applications from companies like BlackRock and Fidelity, it also sparked optimistic expectations for Ethereum's future development.\n    - **Cancun Upgrade Plan**: The Cancun upgrade, as an important update to Ethereum, is expected to further enhance the network's performance and scalability.\n"
[2024-09-30 19:24:42.151] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:575: Merge RAG context into system message.
[2024-09-30 19:24:42.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:24:42.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:24:42.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(true)
[2024-09-30 19:24:42.151] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:82: Process chat completion request in the stream mode.
[2024-09-30 19:24:42.152] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:24:42.152] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:24:42.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:104: user: chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2
[2024-09-30 19:24:42.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:113: include_usage: true
[2024-09-30 19:24:42.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:24:42.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:24:42.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:24:42.152] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:24:42.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:42.153] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:24:42.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:24:42.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:24:42.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:24:42.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:42.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:24:42.153] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:42.676] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:24:42.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:24:42.676] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:24:42.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:24:42.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:24:42.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:42.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:42.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:24:42.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 98
[2024-09-30 19:24:42.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 813, completion tokens: 14
[2024-09-30 19:24:42.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:124: prompt:
<|system|>
You're the digital twin of Vitalik Buterin, the founder of Ethereum, you know a lot about blockchain, and you're going to help us learn about blockchain.
You are a tour guide in Paris, France. Use information in the following context to directly answer the question from a Paris visitor.\n----------------\n
"- **2013**: The concept of Ethereum was proposed by Vitalik Buterin, aiming to create a more versatile blockchain platform that supports smart contracts and decentralized applications (DApps).\n- **2014**:\n    - Raised funds through an [Initial Coin Offering (ICO)](https://www.notion.so/ICO-Initial-Coin-Offering-d1f5ad5c27494950a674bf30462488b1?pvs=21).\n    - Ethereum Switzerland GmbH (EthSuisse) began the development of Ethereum software.\n- **July 30, 2015**: Ethereum launched its \"Frontier\" phase officially, marking the start of the Ethereum blockchain's operation.\n- **2016**:\n    - **March**: The \"Homestead\" was released as Ethereum’s first major upgrade, enhancing the platform's security and stability.\n    - **June**: The infamous \"DAO Attack\" happened, leading to a community split. The original chain continued as Ethereum Classic.\n- **2017**: Ethereum released the \"Byzantium\" and \"Constantinople\" upgrades, part of the \"Metropolis\" phase, aimed at further improving the network's privacy, security, and efficiency.\n- **December 2020**: The \"Beacon Chain\" of Ethereum 2.0 was launched, marking the beginning of the transition to a Proof-of-Stake (PoS) mechanism, a key step in reducing energy consumption and increasing transaction throughput for the Ethereum network.\n- **September 15, 2022**: \"The Merge\" was completed, officially transitioning Ethereum from a Proof-of-Work (PoW) consensus mechanism to a Proof-of-Stake (PoS), significantly reducing the network's energy consumption.\n- **2023**: The Shapella (Shanghai + Capella) upgrade further optimized network functionality, notably introducing the staking withdrawal feature, allowing stakers to withdraw their staking rewards.\n- **2024**:\n    - **Spot ETF Application Discussion**: On January 10, 2024, the U.S. Securities and Exchange Commission (SEC) approved the listing of a Bitcoin spot ETF, a significant milestone in cryptocurrency history, paving the way for the approval of other cryptocurrency ETFs. Although the SEC postponed the decision on Ethereum spot ETF applications from companies like BlackRock and Fidelity, it also sparked optimistic expectations for Ethereum's future development.\n    - **Cancun Upgrade Plan**: The Cancun upgrade, as an important update to Ethereum, is expected to further enhance the network's performance and scalability.\n"<|end|>
<|user|>
When did the SEC approve the Bitcoin Spot ETF?<|end|>
<|assistant|>
[2024-09-30 19:24:42.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:125: available_completion_tokens: 820
[2024-09-30 19:24:42.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:126: tool_use: false
[2024-09-30 19:24:42.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:24:42.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:24:42.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:24:42.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:24:42.688] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:24:42.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:24:42.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:24:42.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:24:42.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:42.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:24:42.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:43.001] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:24:43.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:24:43.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:24:43.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:24:43.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:24:43.004] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:24:43.009] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:193: End of the chat completion stream.
[2024-09-30 19:24:43.009] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:461: finish chat completions in stream mode
[2024-09-30 19:24:43.009] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:24:43.010] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:24:43.010] [info] rag_api_server in src/main.rs:517: response_body_size: 0
[2024-09-30 19:24:43.010] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:24:43.010] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:24:43.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:24:43.010] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:24:43.010] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:24:43.010] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:24:43.010] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:24:43.010] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:24:43.010] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:24:43.010] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:24:43.524] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:24:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:24:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:24:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:24:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:24:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:26:35.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:35.557] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:35.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:35.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:35.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:35.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:35.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" The","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695595,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:35.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:35.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:35.762] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:35.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:35.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:35.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:35.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:35.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" U","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695595,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:35.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:35.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:35.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:35.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:35.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:35.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:35.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:35.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695595,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:35.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:36.185] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"S","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695596,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:36.185] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:36.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:36.403] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:36.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:36.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:36.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:36.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:36.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695596,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:36.403] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:36.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:36.610] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:36.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:36.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:36.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:36.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:36.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Sec","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695596,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:36.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:36.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:36.830] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:36.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:36.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:36.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:36.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:36.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ur","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695596,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:36.830] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:37.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:37.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:37.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:37.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:37.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:37.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:37.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ities","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695597,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:37.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:37.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:37.241] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:37.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:37.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:37.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:37.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:37.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695597,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:37.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:37.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:37.445] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:37.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:37.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:37.445] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:37.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:37.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Exchange","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695597,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:37.446] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:37.644] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:37.644] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:37.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:37.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:37.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:37.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:37.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Commission","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695597,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:37.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:37.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:37.842] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:37.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:37.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:37.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:37.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:37.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" (","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695597,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:37.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:38.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:38.046] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:38.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:38.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:38.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:38.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:38.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"SE","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695598,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:38.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:38.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:38.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:38.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:38.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:38.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:38.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:38.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"C","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695598,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:38.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:38.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:38.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:38.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:38.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:38.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:38.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:38.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":")","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695598,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:38.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:38.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:38.673] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:38.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:38.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:38.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:38.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:38.673] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" approved","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695598,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:38.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:38.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:38.888] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:38.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:38.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:38.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:38.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:38.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695598,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:38.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:39.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:39.098] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:39.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:39.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:39.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:39.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:39.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" listing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695599,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:39.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:39.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:39.296] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:39.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:39.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:39.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:39.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:39.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695599,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:39.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:39.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:39.495] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:39.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:39.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:39.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:39.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:39.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695599,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:39.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:39.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:39.696] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:39.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:39.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:39.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:39.697] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:39.697] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Bit","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695599,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:39.697] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:39.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:39.893] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:39.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:39.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:39.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:39.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:39.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"co","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695599,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:39.893] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:40.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:40.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:40.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:40.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:40.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:40.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:40.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695600,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:40.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:40.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:40.301] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:40.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:40.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:40.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:40.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:40.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" spot","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695600,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:40.302] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:40.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:40.499] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:40.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:40.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:40.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:40.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:40.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695600,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:40.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:40.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:40.694] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:40.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:40.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:40.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:40.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:40.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"TF","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695600,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:40.694] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:40.913] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:40.914] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:40.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:40.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:40.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:40.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:40.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695600,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:40.914] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:41.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:41.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:41.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:41.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:41.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:41.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:41.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" January","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695601,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:41.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:41.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:41.330] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:41.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:41.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:41.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:41.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:41.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" ","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695601,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:41.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:41.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:41.533] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:41.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:41.533] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:41.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:41.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:41.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"1","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695601,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:41.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:41.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:41.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:41.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:41.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:41.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:41.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:41.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"0","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695601,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:41.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:41.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:41.981] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:41.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:41.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:41.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:41.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:41.981] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695601,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:41.987] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:42.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:42.223] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:42.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:42.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:42.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:42.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:42.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" ","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695602,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:42.223] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:42.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:42.424] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:42.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:42.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:42.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:42.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:42.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"2","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695602,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:42.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:42.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:42.627] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:42.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:42.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:42.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:42.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:42.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"0","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695602,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:42.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:42.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:42.828] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:42.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:42.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:42.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:42.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:42.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"2","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695602,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:42.829] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:43.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:43.038] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:43.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:43.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:43.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:43.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:43.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"4","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695603,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:43.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:43.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:43.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:43.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:43.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:43.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:43.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:43.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695603,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:43.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:43.251] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:26:43.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:26:43.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:26:43.251] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 98
[2024-09-30 19:26:43.252] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 813, completion tokens: 39
[2024-09-30 19:26:43.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2845: token_info: 813 prompt tokens, 39 completion tokens
[2024-09-30 19:26:43.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:43.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[],"created":1727695603,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk","usage":{"prompt_tokens":813,"completion_tokens":39,"total_tokens":852}}


[2024-09-30 19:26:43.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:43.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:43.454] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:43.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:43.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:43.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:43.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:43.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" This","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695603,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:43.454] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:43.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:43.663] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:43.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:43.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:43.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:43.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:43.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" marked","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695603,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:43.663] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:43.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:43.868] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:43.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:43.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:43.868] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:43.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:43.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" an","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695603,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:43.869] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:44.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:44.069] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:44.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:44.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:44.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:44.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:44.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" important","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695604,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:44.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:44.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:44.279] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:44.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:44.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:44.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:44.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:44.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" mil","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695604,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:44.280] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:44.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:44.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:44.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:44.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:44.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:44.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:44.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"estone","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695604,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:44.492] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:44.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:44.687] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:44.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:44.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:44.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:44.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:44.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695604,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:44.687] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:44.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:44.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:44.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:44.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:44.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:44.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:44.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" crypt","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695604,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:44.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:45.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:45.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:45.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:45.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:45.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:45.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:45.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"oc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695605,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:45.098] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:45.317] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:45.318] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:45.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:45.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:45.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:45.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:45.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"urrency","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695605,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:45.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:45.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:45.521] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:45.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:45.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:45.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:45.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:45.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" history","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695605,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:45.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:45.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:45.739] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:45.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:45.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:45.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:45.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:45.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695605,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:45.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:45.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:45.938] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:45.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:45.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:45.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:45.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:45.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" it","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695605,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:45.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:46.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:46.136] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:46.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:46.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:46.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:46.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:46.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" sign","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695606,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:46.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:46.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:46.354] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:46.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:46.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:46.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:46.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:46.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"aled","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695606,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:46.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:46.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:46.548] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:46.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:46.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:46.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:46.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:46.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" increased","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695606,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:46.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:46.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" regul","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695606,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:46.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:46.955] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:46.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:46.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:46.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:46.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:46.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"atory","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695606,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:46.955] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:47.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:47.176] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:47.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:47.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:47.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:47.176] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:47.177] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" accept","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695607,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:47.180] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:47.437] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:47.437] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:47.437] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:47.437] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:47.437] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:47.438] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:47.438] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695607,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:47.438] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:47.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:47.805] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:47.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:47.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:47.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:47.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:47.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695607,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:47.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:48.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:48.253] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:48.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:48.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:48.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:48.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:48.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" digital","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695608,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:48.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:48.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:48.503] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:48.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:48.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:48.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:48.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:48.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" assets","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695608,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:48.503] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:48.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:48.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:48.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:48.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:48.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:48.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:48.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" within","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695608,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:48.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:48.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:48.951] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:48.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:48.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:48.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:48.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:48.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" traditional","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695608,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:48.952] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:49.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:49.183] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:49.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:49.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:49.184] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:49.184] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:49.184] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" financial","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695609,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:49.184] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:49.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:49.417] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:49.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:49.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:49.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:49.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:49.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" mark","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695609,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:49.418] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:49.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:49.618] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:49.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:49.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:49.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:49.619] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:49.619] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ets","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695609,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:49.619] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:49.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:49.887] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:49.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:49.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:49.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:49.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:49.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695609,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:49.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:50.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:50.137] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:50.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:50.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:50.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:50.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:50.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" The","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695610,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:50.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:50.611] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" appro","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695610,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:50.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:50.859] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:50.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:50.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:50.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:50.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:50.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"val","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695610,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:50.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:51.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:51.082] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:51.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:51.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:51.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:51.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:51.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" came","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695611,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:51.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:51.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:51.328] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:51.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:51.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:51.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:51.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:51.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" after","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695611,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:51.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:51.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:51.544] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:51.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:51.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:51.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:51.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:51.544] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" rig","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695611,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:51.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:51.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:51.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:51.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:51.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:51.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:51.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:51.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"orous","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695611,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:51.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:51.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:51.974] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:51.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:51.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:51.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:51.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:51.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" assess","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695611,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:51.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:52.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:52.191] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:52.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:52.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:52.191] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:52.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:52.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ments","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695612,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:52.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:52.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:52.428] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:52.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:52.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:52.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:52.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:52.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695612,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:52.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:52.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:52.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:52.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:52.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:52.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:52.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:52.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" discuss","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695612,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:52.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:52.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:52.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:52.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:52.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:52.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:52.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:52.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695612,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:52.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:53.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:53.067] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:53.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:53.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:53.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:53.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:53.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" surrounding","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695613,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:53.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:53.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:53.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:53.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:53.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:53.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:53.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:53.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695613,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:53.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:53.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:53.519] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:53.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:53.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:53.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:53.520] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:53.520] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" invest","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695613,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:53.520] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:53.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:53.734] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:53.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:53.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:53.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:53.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:53.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ment","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695613,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:53.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:53.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:53.944] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:53.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:53.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:53.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:53.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:53.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"'","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695613,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:53.944] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:54.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:54.167] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:54.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:54.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:54.167] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:54.168] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:54.168] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"s","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695614,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:54.168] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:54.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:54.373] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:54.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:54.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:54.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:54.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:54.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" risk","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695614,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:54.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:54.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:54.589] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:54.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:54.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:54.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:54.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:54.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" profile","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695614,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:54.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:54.802] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:54.803] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:54.803] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:54.803] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:54.803] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:54.803] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:54.803] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" compared","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695614,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:54.803] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:55.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:55.010] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:55.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:55.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:55.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:55.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:55.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695615,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:55.015] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:55.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:55.246] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:55.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:55.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:55.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:55.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:55.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" conventional","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695615,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:55.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:55.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:55.474] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:55.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:55.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:55.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:55.474] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:55.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" se","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695615,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:55.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:55.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:55.690] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:55.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:55.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:55.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:55.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:55.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"cur","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695615,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:55.691] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:55.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:55.890] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:55.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:55.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:55.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:55.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:55.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ities","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695615,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:55.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:56.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:56.131] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:56.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:56.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:56.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" offer","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695616,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:56.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:56.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:56.356] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:56.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:56.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:56.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:56.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:56.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ings","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695616,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:56.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:56.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:56.569] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:56.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:56.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:56.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:56.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:56.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695616,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:56.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:56.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:56.800] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:56.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:56.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:56.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:56.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:56.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" highlight","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695616,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:56.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:57.039] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:57.039] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:57.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:57.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:57.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:57.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:57.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695617,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:57.040] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:57.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:57.278] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:57.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:57.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:57.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:57.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:57.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695617,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:57.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:57.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:57.560] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:57.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:57.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:57.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:57.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:57.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" c","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695617,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:57.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:57.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:57.757] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:57.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:57.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:57.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:57.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:57.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"aut","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695617,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:57.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:57.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:57.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:57.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:57.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:57.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:57.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:57.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ious","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695617,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:57.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:58.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:58.148] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:58.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:58.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:58.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:58.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:58.149] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" approach","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695618,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:58.149] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:58.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:58.370] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:58.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:58.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:58.370] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:58.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:58.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" by","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695618,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:58.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:58.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:58.564] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:58.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:58.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:58.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:58.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:58.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" U","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695618,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:58.564] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:58.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:58.779] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:58.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:58.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:58.779] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:58.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:58.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695618,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:58.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:58.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:58.958] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:58.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:58.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:58.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:58.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:58.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"S","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695618,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:58.958] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:59.174] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:59.174] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:59.174] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:59.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:59.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:59.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:59.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695619,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:59.175] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:59.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:59.382] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:59.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:59.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:59.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:59.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:59.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" authorities","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695619,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:59.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:59.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:59.583] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:59.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:59.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:59.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:59.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:59.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" towards","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695619,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:59.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:26:59.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:26:59.814] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:26:59.814] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:26:59.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:26:59.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:26:59.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:26:59.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" crypt","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695619,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:26:59.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:00.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:00.038] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:00.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:00.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:00.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:00.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:00.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"oc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695620,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:00.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:00.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:00.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:00.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:00.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:00.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:00.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:00.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"urrency","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695620,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:00.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:00.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:00.428] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:00.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:00.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:00.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:00.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:00.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" products","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695620,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:00.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:00.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:00.617] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:00.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:00.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:00.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:00.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:00.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" that","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695620,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:00.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:00.840] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:00.840] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:00.840] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:00.840] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:00.840] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:00.840] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:00.840] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" aim","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695620,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:00.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:01.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:01.065] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:01.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:01.065] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:01.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:01.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:01.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" at","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695621,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:01.066] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:01.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:01.310] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:01.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:01.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:01.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:01.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:01.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" providing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695621,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:01.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:01.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:01.516] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:01.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:01.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:01.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:01.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:01.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" similar","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695621,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:01.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:01.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:01.853] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:01.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:01.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:01.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:01.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:01.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" benefits","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695621,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:01.854] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:02.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:02.099] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:02.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:02.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:02.099] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:02.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:02.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695622,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:02.100] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:02.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:02.412] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:02.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:02.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:02.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:02.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:02.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" those","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695622,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:02.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:02.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:02.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:02.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:02.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:02.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:02.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:02.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" found","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695622,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:02.639] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:02.879] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:02.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:02.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:02.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:02.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:02.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:02.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695622,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:02.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:03.067] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:03.068] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:03.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:03.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:03.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:03.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:03.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" reg","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695623,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:03.068] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:03.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:03.324] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:03.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:03.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:03.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:03.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:03.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ulated","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695623,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:03.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:03.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:03.642] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:03.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:03.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:03.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:03.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:03.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" stock","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695623,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:03.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:03.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:03.860] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:03.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:03.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:03.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:03.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:03.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695623,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:03.860] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:04.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:04.059] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:04.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:04.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:04.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:04.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:04.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" bond","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695624,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:04.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:04.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:04.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:04.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:04.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:04.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:04.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:04.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" mark","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695624,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:04.264] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:04.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:04.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:04.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:04.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:04.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:04.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:04.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ets","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695624,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:04.476] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:04.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:04.653] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:04.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:04.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:04.653] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:04.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695624,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:04.656] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:04.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:04.851] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:04.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:04.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:04.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:04.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:04.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695624,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:04.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:05.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:05.046] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:05.046] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:05.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:05.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:05.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:05.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695625,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:05.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:05.254] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:05.254] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:05.254] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:05.254] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:05.254] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:05.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:05.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695625,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:05.255] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:05.450] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:05.450] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:05.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:05.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:05.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:05.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:05.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"The","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695625,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:05.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:05.657] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:05.657] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:05.657] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:05.657] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:05.657] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:05.657] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:05.657] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" appro","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695625,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:05.657] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:05.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:05.850] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:05.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:05.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:05.850] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:05.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:05.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"val","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695625,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:05.851] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:06.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:06.052] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:06.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:06.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:06.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:06.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:06.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695626,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:06.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:06.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:06.269] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:06.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:06.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:06.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:06.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:06.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Bit","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695626,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:06.269] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:06.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:06.470] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:06.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:06.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:06.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:06.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:06.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"co","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695626,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:06.471] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:06.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:06.666] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:06.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:06.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:06.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:06.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:06.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695626,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:06.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:06.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:06.878] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:06.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:06.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:06.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:06.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:06.878] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695626,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:06.879] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:07.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:07.081] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:07.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:07.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:07.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:07.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:07.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"TF","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695627,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:07.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:07.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:07.289] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:07.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:07.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:07.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:07.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:07.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" may","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695627,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:07.290] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:07.490] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:07.490] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:07.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:07.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:07.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:07.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:07.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" have","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695627,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:07.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:07.681] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:07.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:07.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" bro","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695627,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:07.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:07.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:07.862] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:07.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:07.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:07.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:07.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:07.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ader","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695627,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:07.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:08.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:08.078] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:08.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:08.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:08.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:08.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:08.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" imp","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695628,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:08.079] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:08.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:08.281] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:08.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:08.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:08.281] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:08.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:08.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"lications","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695628,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:08.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:08.472] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:08.473] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:08.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:08.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:08.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:08.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:08.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695628,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:08.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:08.688] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:08.688] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:08.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:08.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:08.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:08.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:08.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695628,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:08.689] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:08.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:08.877] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:08.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:08.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:08.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:08.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:08.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" block","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695628,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:08.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:09.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:09.077] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:09.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:09.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:09.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:09.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:09.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"chain","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695629,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:09.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:09.278] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:09.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695629,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:09.279] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:09.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:09.485] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:09.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:09.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:09.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:09.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:09.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" decent","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695629,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:09.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:09.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:09.696] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:09.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:09.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:09.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:09.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:09.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ral","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695629,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:09.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:09.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:09.891] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:09.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:09.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:09.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:09.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:09.891] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ized","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695629,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:09.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:10.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:10.102] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:10.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:10.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:10.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:10.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:10.102] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" fin","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695630,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:10.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:10.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:10.293] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:10.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:10.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:10.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:10.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:10.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695630,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:10.294] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:10.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:10.484] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:10.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:10.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:10.484] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:10.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:10.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" (","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695630,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:10.485] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:10.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:10.678] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:10.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:10.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:10.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:10.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:10.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"De","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695630,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:10.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:10.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:10.892] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:10.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:10.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:10.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:10.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:10.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"Fi","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695630,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:10.892] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:11.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:11.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:11.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:11.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:11.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:11.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:11.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":")","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695631,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:11.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:11.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:11.299] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:11.300] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:11.300] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:11.300] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:11.300] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:11.300] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" industry","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695631,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:11.300] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:11.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:11.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:11.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:11.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:11.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:11.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:11.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695631,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:11.518] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:11.780] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:11.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:11.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:11.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" possibly","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695631,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:11.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:11.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:11.992] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:11.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:11.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:11.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:11.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:11.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" acceler","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695631,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:11.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:12.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:12.183] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:12.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:12.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:12.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:12.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:12.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ating","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695632,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:12.183] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:12.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:12.380] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:12.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:12.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:12.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:12.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:12.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" regul","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695632,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:12.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:12.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:12.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:12.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:12.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:12.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:12.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:12.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"atory","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695632,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:12.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:12.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:12.761] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:12.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:12.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:12.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:12.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:12.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" frameworks","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695632,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:12.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:12.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:12.964] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:12.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:12.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:12.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:12.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:12.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" adapt","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695632,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:12.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:13.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:13.153] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:13.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:13.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:13.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:13.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:13.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695633,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:13.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:13.363] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695633,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:13.363] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:13.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:13.557] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:13.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:13.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:13.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:13.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:13.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" this","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695633,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:13.557] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:13.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:13.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:13.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:13.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:13.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:13.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:13.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" new","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695633,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:13.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:13.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:13.945] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:13.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:13.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:13.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:13.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:13.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" asset","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695633,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:13.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:14.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:14.128] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:14.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:14.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:14.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:14.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:14.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" class","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695634,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:14.128] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:14.448] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:14.448] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:14.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:14.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:14.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:14.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:14.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695634,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:14.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:14.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:14.823] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:14.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:14.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:14.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:14.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:14.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" This","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695634,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:14.823] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:15.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:15.027] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:15.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:15.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:15.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:15.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:15.027] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" move","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695635,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:15.028] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:15.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:15.217] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:15.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:15.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:15.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:15.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:15.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" might","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695635,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:15.217] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:15.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:15.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:15.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" be","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695635,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:15.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:15.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:15.590] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:15.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:15.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:15.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:15.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:15.591] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" seen","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695635,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:15.595] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:15.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:15.781] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:15.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:15.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:15.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:15.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:15.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695635,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:15.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:15.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:15.962] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:15.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:15.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:15.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:15.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:15.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" an","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695635,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:15.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:16.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:16.163] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:16.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:16.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:16.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:16.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:16.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" end","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695636,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:16.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:16.330] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:16.331] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:16.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:16.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:16.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:16.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:16.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ors","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695636,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:16.331] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:16.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:16.499] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:16.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:16.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:16.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:16.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:16.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ement","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695636,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:16.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:16.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:16.678] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:16.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:16.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:16.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:16.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:16.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" by","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695636,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:16.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:16.861] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:16.862] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:16.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:16.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:16.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:16.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:16.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" main","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695636,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:16.862] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:17.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:17.055] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:17.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:17.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:17.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:17.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:17.055] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"stream","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695637,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:17.056] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:17.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:17.250] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:17.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:17.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:17.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:17.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:17.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" financial","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695637,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:17.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:17.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:17.428] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:17.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:17.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:17.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:17.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:17.428] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" institutions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695637,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:17.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:17.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:17.610] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:17.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:17.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:17.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:17.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:17.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" that","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695637,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:17.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:17.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:17.788] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:17.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:17.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:17.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:17.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:17.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" could","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695637,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:17.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:17.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:17.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:17.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:17.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:17.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:17.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:17.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" lead","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695637,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:17.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:18.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:18.192] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:18.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:18.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:18.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:18.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:18.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" other","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695638,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:18.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:18.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:18.427] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:18.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:18.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:18.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:18.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:18.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" crypt","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695638,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:18.427] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:18.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:18.737] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:18.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:18.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:18.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:18.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:18.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"oc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695638,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:18.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:18.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:18.963] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:18.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:18.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:18.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:18.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:18.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"urr","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695638,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:18.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:19.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:19.146] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:19.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:19.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:19.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:19.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:19.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"encies","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695639,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:19.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:19.328] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:19.329] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:19.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:19.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:19.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:19.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:19.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" like","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695639,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:19.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:19.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:19.531] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:19.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:19.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:19.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:19.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:19.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695639,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:19.531] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:19.782] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:19.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:19.783] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:19.783] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:19.783] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"there","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695639,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:19.783] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:19.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:19.999] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:19.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:19.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:19.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:19.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:19.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"um","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695639,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:19.999] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:20.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:20.207] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:20.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:20.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:20.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:20.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:20.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" towards","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695640,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:20.207] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:20.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:20.441] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:20.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:20.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:20.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:20.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:20.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" similar","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695640,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:20.441] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:20.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:20.666] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:20.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:20.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:20.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:20.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:20.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" products","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695640,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:20.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:20.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:20.894] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:20.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:20.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:20.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:20.894] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:20.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695640,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:20.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:21.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:21.120] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:21.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:21.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:21.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:21.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:21.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" terms","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695641,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:21.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:21.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:21.349] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:21.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:21.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:21.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:21.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:21.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695641,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:21.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:21.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:21.551] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:21.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:21.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:21.551] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:21.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:21.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" increased","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695641,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:21.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:21.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:21.746] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:21.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:21.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:21.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:21.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:21.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" liquid","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695641,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:21.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:21.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:21.931] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:21.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:21.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:21.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:21.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:21.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ity","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695641,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:21.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:22.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:22.114] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:22.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:22.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:22.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:22.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:22.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695642,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:22.117] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:22.306] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" ret","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695642,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:22.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:22.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:22.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:22.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:22.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:22.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:22.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ail","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695642,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:22.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:22.680] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:22.680] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:22.680] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:22.680] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:22.680] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:22.680] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:22.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695642,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:22.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:22.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:22.855] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:22.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:22.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:22.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:22.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:22.855] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" institution","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695642,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:22.856] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:23.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:23.041] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:23.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:23.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:23.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:23.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:23.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"al","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695643,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:23.041] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:23.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:23.218] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:23.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:23.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:23.218] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:23.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:23.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" invest","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695643,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:23.219] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:23.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:23.413] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:23.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:23.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:23.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:23.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:23.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ors","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695643,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:23.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:23.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:23.585] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:23.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:23.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:23.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:23.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:23.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695643,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:23.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:23.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:23.774] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:23.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:23.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:23.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:23.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:23.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695643,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:23.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:23.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:23.978] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:23.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:23.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:23.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:23.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:23.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695643,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:23.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:24.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:24.213] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:24.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:24.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:24.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:24.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:24.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695644,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:24.221] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:24.433] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"Al","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695644,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:24.433] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:24.622] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:24.622] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:24.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:24.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:24.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:24.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:24.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"though","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695644,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:24.623] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:24.803] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:24.804] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:24.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:24.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:24.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:24.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:24.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695644,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:24.804] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:24.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:24.985] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:24.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:24.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:24.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:24.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:24.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" SE","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695644,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:24.985] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:25.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:25.158] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:25.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:25.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:25.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:25.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:25.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"C","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695645,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:25.159] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:25.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:25.329] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:25.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:25.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:25.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:25.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:25.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" approved","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695645,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:25.329] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:25.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:25.506] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:25.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:25.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:25.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:25.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:25.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Bit","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695645,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:25.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:25.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:25.681] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:25.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:25.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:25.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:25.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:25.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"co","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695645,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:25.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:25.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:25.874] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:25.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:25.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:25.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:25.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:25.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695645,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:25.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:26.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:26.050] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:26.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:26.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:26.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:26.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:26.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695646,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:26.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:26.240] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:26.241] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:26.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:26.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:26.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:26.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:26.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" they","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695646,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:26.241] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:26.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:26.410] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:26.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:26.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:26.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:26.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:26.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" post","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695646,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:26.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:26.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:26.583] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:26.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:26.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:26.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:26.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:26.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"pon","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695646,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:26.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:26.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:26.771] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:26.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:26.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:26.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:26.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:26.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ed","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695646,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:26.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:26.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:26.947] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:26.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:26.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:26.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:26.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:26.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" dec","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695646,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:26.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:27.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:27.137] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:27.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:27.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:27.137] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:27.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:27.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"isions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695647,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:27.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:27.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:27.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:27.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:27.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:27.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:27.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:27.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695647,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:27.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:27.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:27.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:27.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:27.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:27.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:27.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:27.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" applications","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695647,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:27.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:27.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:27.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:27.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:27.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:27.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:27.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:27.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" from","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695647,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:27.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:28.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:28.002] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:28.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:28.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:28.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:28.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:28.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Black","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695648,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:28.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:28.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:28.201] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:28.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:28.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:28.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:28.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:28.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"R","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695648,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:28.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:28.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:28.372] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:28.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:28.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:28.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:28.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:28.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ock","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695648,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:28.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:28.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:28.559] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:28.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:28.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:28.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:28.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:28.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695648,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:28.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:28.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:28.733] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:28.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:28.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:28.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:28.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:28.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" F","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695648,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:28.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:28.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:28.930] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:28.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:28.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:28.930] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:28.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:28.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"idel","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695648,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:28.931] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:29.132] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:29.132] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:29.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:29.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:29.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:29.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:29.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ity","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695649,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:29.133] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:29.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:29.399] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:29.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:29.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:29.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:29.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:29.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" regarding","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695649,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:29.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:29.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:29.676] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:29.676] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:29.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:29.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:29.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:29.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" an","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695649,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:29.677] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:29.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:29.929] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:29.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:29.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:29.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:29.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:29.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695649,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:29.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:30.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:30.200] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:30.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:30.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:30.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:30.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:30.200] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"there","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695650,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:30.201] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:30.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:30.410] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:30.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:30.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:30.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:30.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:30.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"um","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695650,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:30.411] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:30.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:30.603] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:30.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:30.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:30.603] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:30.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:30.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" spot","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695650,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:30.606] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:30.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:30.799] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:30.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:30.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:30.799] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:30.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:30.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695650,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:30.800] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:31.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:31.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:31.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:31.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:31.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:31.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:31.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"TF","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695651,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:31.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:31.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:31.310] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:31.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:31.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:31.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:31.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:31.310] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695651,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:31.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:31.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:31.510] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:31.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:31.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:31.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:31.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:31.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" This","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695651,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:31.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:31.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:31.703] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:31.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:31.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:31.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:31.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:31.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" decision","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695651,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:31.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:31.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:31.895] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:31.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:31.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:31.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:31.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:31.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" might","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695651,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:31.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:32.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:32.081] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:32.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:32.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:32.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:32.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:32.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" have","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695652,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:32.082] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:32.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:32.312] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:32.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:32.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:32.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:32.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:32.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" been","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695652,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:32.313] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:32.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:32.504] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:32.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:32.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:32.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:32.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:32.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" influenced","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695652,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:32.504] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:32.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:32.709] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:32.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:32.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:32.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:32.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:32.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" by","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695652,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:32.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:32.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:32.905] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:32.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:32.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:32.905] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:32.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:32.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" specific","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695652,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:32.906] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:33.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:33.084] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:33.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:33.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:33.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:33.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:33.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" regul","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695653,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:33.084] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:33.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:33.277] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:33.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:33.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:33.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:33.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:33.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"atory","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695653,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:33.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:33.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:33.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:33.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:33.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" concerns","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695653,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:33.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:33.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:33.720] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:33.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:33.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:33.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:33.720] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:33.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" about","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695653,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:33.721] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:33.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:33.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:33.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:33.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:33.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:33.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:33.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" different","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695653,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:33.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:34.124] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:34.125] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:34.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:34.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:34.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:34.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:34.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" aspects","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695654,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:34.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:34.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:34.588] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:34.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:34.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:34.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:34.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:34.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695654,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:34.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:34.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:34.834] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:34.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:34.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:34.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:34.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:34.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" block","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695654,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:34.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:35.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:35.047] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:35.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:35.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:35.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:35.047] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:35.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"chain","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695655,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:35.048] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:35.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:35.242] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:35.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:35.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:35.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:35.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:35.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" technology","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695655,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:35.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:35.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:35.429] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:35.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:35.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:35.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:35.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:35.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" compared","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695655,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:35.430] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:35.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:35.616] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:35.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:35.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:35.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:35.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:35.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695655,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:35.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:35.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:35.825] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:35.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:35.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:35.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:35.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:35.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" traditional","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695655,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:35.825] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:36.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:36.038] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:36.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:36.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:36.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:36.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:36.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" asset","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695656,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:36.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:36.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:36.239] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:36.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:36.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:36.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:36.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:36.239] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" classes","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695656,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:36.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:36.425] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" represented","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695656,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:36.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:36.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:36.596] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:36.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:36.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:36.596] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:36.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:36.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695656,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:36.597] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:36.776] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:36.776] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:36.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:36.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:36.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:36.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:36.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695656,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:36.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:36.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:36.973] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:36.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:36.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:36.973] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" typical","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695656,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:36.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:37.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:37.183] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:37.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:37.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:37.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:37.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:37.193] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" c","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695657,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:37.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:37.385] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:37.386] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:37.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:37.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:37.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:37.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:37.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"rypto","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695657,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:37.386] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:37.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:37.580] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:37.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:37.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:37.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:37.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:37.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" e","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695657,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:37.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:37.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:37.763] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:37.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:37.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:37.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:37.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:37.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"cos","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695657,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:37.764] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:37.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:37.942] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:37.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:37.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:37.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:37.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:37.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ystem","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695657,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:37.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:38.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:38.141] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:38.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:38.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:38.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:38.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:38.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" like","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695658,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:38.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:38.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:38.319] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:38.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:38.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:38.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:38.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:38.320] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695658,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:38.320] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:38.501] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:38.501] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:38.501] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:38.501] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:38.501] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:38.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:38.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"there","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695658,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:38.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:38.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:38.681] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:38.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:38.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:38.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:38.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:38.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"um","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695658,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:38.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:38.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:38.881] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:38.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:38.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:38.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:38.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:38.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695658,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:38.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:39.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:39.080] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:39.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:39.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:39.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:39.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:39.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695659,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:39.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:39.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:39.260] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:39.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:39.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:39.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:39.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:39.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695659,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:39.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:39.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:39.466] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:39.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:39.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:39.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:39.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:39.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695659,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:39.466] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:39.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:39.724] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:39.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:39.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:39.724] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:39.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:39.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"F","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695659,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:39.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:39.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:39.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:39.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:39.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:39.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:39.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:39.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ollow","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695659,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:39.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:40.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:40.192] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:40.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:40.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:40.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:40.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:40.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695660,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:40.192] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:40.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:40.376] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:40.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:40.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:40.376] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:40.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:40.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695660,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:40.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:40.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:40.580] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:40.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:40.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:40.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:40.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:40.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Bit","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695660,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:40.580] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:40.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:40.777] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:40.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:40.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:40.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:40.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:40.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"co","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695660,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:40.777] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:40.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:40.950] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:40.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:40.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:40.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:40.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:40.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695660,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:40.951] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:41.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:41.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:41.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:41.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:41.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:41.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:41.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" appro","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695661,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:41.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:41.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:41.355] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:41.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:41.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:41.355] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:41.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:41.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"val","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695661,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:41.356] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:41.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:41.552] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:41.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:41.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:41.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:41.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:41.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695661,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:41.556] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:41.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:41.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:41.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:41.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:41.738] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:41.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:41.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" invest","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695661,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:41.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:41.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:41.946] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:41.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:41.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:41.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:41.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:41.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ors","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695661,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:41.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:42.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:42.131] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:42.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:42.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:42.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:42.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:42.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695662,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:42.131] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:42.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:42.318] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:42.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:42.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:42.318] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:42.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:42.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" industry","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695662,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:42.319] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:42.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:42.496] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:42.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:42.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:42.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:42.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:42.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" participants","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695662,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:42.496] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:42.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:42.670] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:42.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:42.670] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:42.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:42.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:42.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" are","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695662,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:42.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:42.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:42.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:42.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:42.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:42.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:42.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:42.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" likely","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695662,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:42.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:43.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:43.022] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:43.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:43.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:43.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:43.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:43.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" hope","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695663,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:43.022] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:43.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:43.202] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:43.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:43.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:43.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:43.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:43.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ful","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695663,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:43.202] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:43.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:43.399] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:43.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:43.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:43.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:43.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:43.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" that","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695663,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:43.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:43.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:43.587] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:43.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:43.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:43.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:43.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:43.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" this","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695663,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:43.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:43.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:43.771] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:43.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:43.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:43.771] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:43.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:43.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" could","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695663,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:43.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:43.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:43.961] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:43.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:43.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:43.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:43.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:43.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" set","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695663,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:43.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:44.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:44.164] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:44.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:44.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:44.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:44.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:44.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" positive","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695664,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:44.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:44.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:44.366] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:44.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:44.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:44.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:44.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:44.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" preced","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695664,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:44.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:44.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:44.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:44.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:44.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:44.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:44.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:44.569] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ents","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695664,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:44.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:44.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:44.774] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:44.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:44.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:44.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:44.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:44.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695664,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:44.774] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:44.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:44.966] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:44.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:44.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:44.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:44.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:44.966] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" future","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695664,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:44.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:45.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:45.161] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:45.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:45.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:45.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:45.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:45.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" crypt","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695665,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:45.161] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:45.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:45.374] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:45.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:45.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:45.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:45.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:45.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"oc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695665,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:45.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:45.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:45.577] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:45.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:45.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:45.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:45.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:45.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"urr","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695665,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:45.578] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:45.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:45.817] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:45.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:45.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:45.817] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:45.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:45.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"encies","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695665,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:45.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:46.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:46.050] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:46.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:46.050] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:46.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:46.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:46.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" aim","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695666,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:46.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:46.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:46.260] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:46.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:46.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:46.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:46.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:46.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695666,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:46.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:46.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:46.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:46.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:46.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:46.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:46.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:46.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" at","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695666,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:46.463] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:46.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:46.660] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:46.660] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:46.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:46.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:46.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:46.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" similar","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695666,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:46.661] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:46.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:46.876] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:46.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:46.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:46.876] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:46.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:46.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" financial","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695666,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:46.877] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:47.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:47.091] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:47.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:47.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:47.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:47.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:47.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" products","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695667,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:47.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:47.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:47.303] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:47.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:47.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:47.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:47.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:47.303] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695667,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:47.304] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:47.510] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:47.510] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:47.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:47.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:47.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:47.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:47.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" However","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695667,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:47.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:47.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:47.718] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:47.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:47.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:47.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:47.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:47.719] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695667,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:47.719] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:47.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:47.925] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:47.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:47.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:47.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:47.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:47.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" it","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695667,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:47.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:48.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:48.242] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:48.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:48.242] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:48.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:48.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:48.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"'","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695668,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:48.243] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:48.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:48.700] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:48.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:48.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:48.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:48.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:48.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"s","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695668,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:48.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:48.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:48.916] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:48.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:48.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:48.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:48.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:48.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" essential","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695668,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:48.917] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:49.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:49.129] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:49.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:49.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:49.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:49.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:49.129] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695669,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:49.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:49.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:49.343] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:49.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:49.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:49.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:49.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:49.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" note","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695669,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:49.344] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:49.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:49.558] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:49.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:49.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:49.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:49.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:49.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" such","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695669,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:49.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:49.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:49.767] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:49.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:49.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:49.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:49.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:49.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" dec","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695669,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:49.768] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:49.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:49.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:49.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:49.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"isions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695669,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:49.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:50.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:50.194] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:50.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:50.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:50.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:50.194] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:50.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" remain","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695670,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:50.195] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:50.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:50.400] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:50.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:50.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:50.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:50.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:50.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" subject","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695670,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:50.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:50.611] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:50.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:50.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:50.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695670,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:50.617] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:50.834] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:50.834] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:50.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:50.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" change","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695670,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:50.835] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:51.058] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:51.058] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" based","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695671,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:51.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:51.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:51.262] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:51.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:51.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:51.262] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:51.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:51.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695671,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:51.263] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:51.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:51.475] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:51.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:51.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:51.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:51.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:51.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" evol","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695671,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:51.475] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:51.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:51.684] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:51.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:51.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:51.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:51.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:51.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ving","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695671,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:51.684] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:51.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:51.875] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:51.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:51.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:51.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:51.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:51.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" regul","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695671,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:51.875] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:52.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:52.069] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:52.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:52.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:52.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:52.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:52.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"atory","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695672,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:52.069] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:52.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:52.285] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:52.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:52.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:52.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:52.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:52.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" land","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695672,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:52.285] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:52.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:52.480] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:52.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:52.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:52.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:52.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:52.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"sc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695672,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:52.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:52.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:52.678] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:52.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:52.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:52.678] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"apes","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695672,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:52.679] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:52.879] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:52.879] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:52.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:52.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:52.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:52.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:52.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695672,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:52.880] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:53.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:53.078] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:53.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:53.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:53.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:53.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:53.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" authorities","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695673,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:53.078] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:53.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:53.277] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:53.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:53.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:53.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:53.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:53.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" st","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695673,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:53.278] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:53.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:53.491] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:53.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:53.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:53.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:53.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:53.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"rive","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695673,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:53.491] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:53.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:53.703] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:53.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:53.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:53.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:53.703] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:53.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695673,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:53.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:53.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:53.895] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:53.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:53.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:53.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:53.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:53.896] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" balance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695673,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:53.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:54.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:54.121] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:54.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:54.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:54.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:54.121] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:54.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" fost","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695674,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:54.122] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:54.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:54.332] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:54.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:54.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:54.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:54.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:54.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ering","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695674,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:54.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:54.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:54.540] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:54.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:54.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:54.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:54.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:54.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" innov","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695674,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:54.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:54.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:54.745] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:54.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:54.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:54.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:54.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:54.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ation","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695674,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:54.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:54.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:54.940] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:54.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:54.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:54.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:54.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:54.940] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" with","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695674,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:54.941] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:55.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:55.169] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:55.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:55.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:55.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:55.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:55.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" protect","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695675,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:55.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:55.371] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:55.371] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:55.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:55.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:55.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:55.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:55.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695675,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:55.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:55.565] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:55.566] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:55.566] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:55.566] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:55.566] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:55.566] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:55.566] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" invest","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695675,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:55.566] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:55.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:55.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:55.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:55.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:55.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:55.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:55.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ors","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695675,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:55.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:55.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:55.964] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:55.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:55.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:55.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:55.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:55.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" from","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695675,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:55.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:56.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:56.164] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:56.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:56.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:56.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:56.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:56.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" potential","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695676,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:56.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:56.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:56.364] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:56.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:56.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:56.364] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:56.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:56.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" ris","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695676,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:56.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:56.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:56.558] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:56.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:56.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:56.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:56.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:56.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ks","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695676,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:56.558] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:56.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:56.757] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:56.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:56.757] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:56.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:56.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:56.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695676,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:56.758] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:56.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:56.964] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:56.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:56.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:56.964] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:56.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:56.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" these","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695676,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:56.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:57.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:57.169] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:57.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:57.169] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:57.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:57.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:57.170] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" emer","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695677,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:57.172] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:57.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:57.383] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:57.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:57.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:57.383] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:57.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:57.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ging","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695677,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:57.384] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:57.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:57.592] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:57.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:57.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:57.592] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:57.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:57.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" mark","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695677,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:57.593] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:57.811] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:57.811] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:57.811] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:57.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:57.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:57.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:57.821] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ets","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695677,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:57.822] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:58.038] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695678,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:58.038] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:58.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:58.235] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:58.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:58.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:58.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:58.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:58.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695678,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:58.235] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:58.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:58.435] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:58.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:58.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:58.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:58.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:58.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695678,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:58.435] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:58.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:58.638] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:58.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:58.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:58.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:58.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:58.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695678,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:58.638] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:58.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:58.842] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:58.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:58.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:58.842] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:58.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:58.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"This","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695678,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:58.843] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:59.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:59.063] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:59.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:59.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:59.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:59.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:59.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" appro","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695679,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:59.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:59.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:59.260] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:59.260] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:59.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:59.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:59.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:59.261] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"val","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695679,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:59.266] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:59.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:59.477] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:59.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:59.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:59.477] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:59.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:59.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" is","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695679,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:59.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:59.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:59.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:59.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:59.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:59.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:59.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:59.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" seen","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695679,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:59.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:27:59.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:27:59.871] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:27:59.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:27:59.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:27:59.871] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:27:59.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:27:59.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" by","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695679,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:27:59.872] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:00.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:00.091] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:00.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:00.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:00.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:00.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:00.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" some","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695680,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:00.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:00.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:00.296] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:00.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:00.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:00.296] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:00.297] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:00.297] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" industry","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695680,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:00.297] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:00.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:00.494] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:00.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" obser","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695680,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:00.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:00.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:00.699] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:00.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:00.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:00.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:00.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:00.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"vers","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695680,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:00.700] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:01.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:01.135] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:01.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:01.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:01.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:01.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:01.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695681,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:01.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:01.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:01.506] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:01.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:01.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:01.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:01.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:01.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" anal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695681,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:01.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:01.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:01.739] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:01.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:01.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:01.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:01.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:01.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"yst","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695681,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:01.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:01.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:01.945] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:01.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:01.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:01.945] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:01.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:01.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"s","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695681,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:01.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:02.155] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695682,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:02.155] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:02.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:02.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:02.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:02.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:02.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:02.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:02.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695682,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:02.359] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:02.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:02.559] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:02.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:02.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:02.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:02.559] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:02.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" strong","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695682,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:02.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:02.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:02.761] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:02.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:02.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:02.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:02.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:02.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" signal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695682,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:02.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:02.969] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:02.969] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:02.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:02.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:02.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:02.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:02.980] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" that","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695682,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:02.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:03.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:03.187] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:03.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:03.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:03.187] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:03.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:03.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695683,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:03.188] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:03.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:03.401] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:03.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:03.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:03.401] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:03.402] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:03.402] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"there","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695683,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:03.402] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:03.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:03.628] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:03.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:03.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:03.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:03.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:03.628] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"um","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695683,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:03.629] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:03.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:03.845] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:03.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:03.845] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:03.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:03.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:03.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695683,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:03.846] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:04.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:04.051] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:04.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:04.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:04.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:04.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:04.051] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695684,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:04.052] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:04.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:04.357] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:04.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:04.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:04.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:04.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:04.357] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" other","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695684,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:04.358] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:04.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:04.637] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:04.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:04.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:04.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:04.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:04.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" block","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695684,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:04.637] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:04.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:04.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:04.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:04.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:04.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:04.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:04.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"chain","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695684,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:04.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:05.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:05.059] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:05.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:05.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:05.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:05.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:05.059] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" platforms","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695685,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:05.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:05.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:05.267] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:05.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:05.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:05.267] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:05.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:05.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" planning","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695685,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:05.268] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:05.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:05.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:05.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:05.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:05.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:05.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:05.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" their","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695685,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:05.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:05.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:05.664] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:05.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:05.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:05.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:05.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:05.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" own","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695685,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:05.664] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:05.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" crypt","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695685,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:05.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:06.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:06.086] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:06.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:06.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:06.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:06.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:06.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"oc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695686,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:06.087] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:06.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:06.327] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:06.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:06.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:06.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:06.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:06.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"urrency","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695686,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:06.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:06.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:06.590] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:06.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:06.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:06.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:06.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:06.590] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"-","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695686,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:06.594] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:06.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:06.818] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:06.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:06.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:06.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:06.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:06.818] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"related","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695686,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:06.819] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:07.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:07.019] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:07.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:07.019] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:07.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:07.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:07.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" financial","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695687,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:07.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:07.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:07.231] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:07.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:07.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:07.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:07.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:07.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" products","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695687,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:07.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:07.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:07.464] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:07.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:07.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:07.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:07.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:07.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" like","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695687,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:07.464] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:07.681] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Sp","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695687,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:07.681] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:07.883] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:07.884] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ot","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695687,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:07.884] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:08.125] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:08.125] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:08.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:08.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:08.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:08.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:08.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695688,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:08.136] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:08.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:08.332] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:08.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:08.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:08.332] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:08.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:08.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"TF","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695688,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:08.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:08.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:08.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:08.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:08.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:08.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:08.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:08.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"s","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695688,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:08.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:08.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:08.737] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:08.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:08.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:08.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:08.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:08.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695688,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:08.737] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:08.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:08.939] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:08.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:08.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:08.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:08.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:08.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" could","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695688,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:08.943] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:09.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:09.197] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:09.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:09.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:09.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:09.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:09.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" see","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695689,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:09.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:09.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:09.424] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:09.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:09.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:09.424] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:09.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:09.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" similar","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695689,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:09.425] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:09.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:09.641] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:09.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:09.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:09.641] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:09.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:09.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" progress","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695689,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:09.642] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:09.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:09.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:09.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:09.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:09.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:09.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:09.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695689,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:09.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:10.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:10.076] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:10.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:10.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:10.076] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:10.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:10.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" towards","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695690,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:10.077] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:10.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:10.292] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:10.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:10.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:10.292] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:10.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:10.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" regul","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695690,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:10.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:10.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:10.502] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:10.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:10.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:10.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:10.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:10.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"atory","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695690,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:10.502] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:10.711] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:10.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:10.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:10.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:10.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" accept","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695690,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:10.712] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:10.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:10.932] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:10.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:10.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:10.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:10.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:10.932] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695690,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:10.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:11.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:11.148] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:11.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:11.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:11.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:11.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:11.148] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695691,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:11.149] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:11.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:11.365] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:11.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:11.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:11.365] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:11.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:11.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Given","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695691,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:11.366] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:11.578] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:11.579] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:11.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:11.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:11.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:11.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:11.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695691,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:11.579] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:11.780] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" complex","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695691,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:11.780] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:11.993] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:11.994] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:11.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:11.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:11.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:11.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:11.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ities","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695691,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:11.994] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:12.230] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:12.231] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:12.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:12.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:12.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:12.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:12.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" surrounding","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695692,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:12.231] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:12.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:12.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:12.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:12.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:12.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:12.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:12.468] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" these","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695692,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:12.473] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:12.717] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:12.717] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:12.717] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:12.717] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:12.717] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:12.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:12.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" assets","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695692,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:12.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:12.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:12.928] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:12.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:12.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:12.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:12.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:12.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" -","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695692,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:12.928] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:13.162] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:13.163] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:13.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:13.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:13.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:13.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:13.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" such","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695693,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:13.164] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:13.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:13.413] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:13.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:13.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:13.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:13.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:13.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695693,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:13.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:13.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:13.610] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:13.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:13.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:13.610] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:13.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:13.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" market","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695693,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:13.611] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:13.815] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:13.815] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:13.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:13.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:13.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:13.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:13.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" vol","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695693,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:13.816] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:14.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:14.020] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:14.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:14.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:14.020] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:14.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:14.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"at","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695694,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:14.021] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:14.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:14.271] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:14.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:14.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:14.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:14.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:14.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ility","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695694,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:14.271] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:14.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:14.482] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:14.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:14.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:14.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:14.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:14.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695694,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:14.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:14.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:14.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:14.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:14.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:14.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:14.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:14.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" security","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695694,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:14.683] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:14.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:14.887] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:14.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:14.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:14.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:14.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:14.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" concerns","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695694,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:14.888] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:15.103] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:15.103] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:15.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:15.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:15.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:15.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:15.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" over","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695695,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:15.104] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:15.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:15.334] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:15.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:15.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:15.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:15.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:15.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" digital","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695695,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:15.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:15.548] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:15.549] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:15.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:15.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:15.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:15.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:15.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" wal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695695,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:15.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:15.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:15.762] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:15.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:15.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:15.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:15.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:15.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"lets","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695695,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:15.763] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:15.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:15.990] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:15.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:15.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:15.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:15.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:15.991] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695695,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:15.997] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:16.300] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:16.300] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:16.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:16.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:16.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:16.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:16.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" exchange","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695696,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:16.301] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:16.645] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:16.646] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:16.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:16.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:16.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:16.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:16.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" ha","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695696,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:16.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:16.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:16.937] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:16.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:16.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:16.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:16.937] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:16.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"cks","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695696,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:16.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:17.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:17.119] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:17.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:17.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:17.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:17.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:17.119] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695697,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:17.120] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:17.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:17.311] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:17.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:17.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:17.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:17.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:17.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" invest","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695697,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:17.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:17.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:17.517] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:17.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:17.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:17.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:17.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:17.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695697,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:17.517] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:17.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:17.706] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:17.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:17.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:17.706] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:17.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:17.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" protection","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695697,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:17.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:17.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:17.899] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:17.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:17.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:17.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:17.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:17.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" issues","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695697,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:17.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:18.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:18.085] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:18.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:18.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:18.085] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:18.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:18.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" due","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695698,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:18.086] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:18.297] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:18.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:18.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:18.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:18.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:18.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:18.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695698,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:18.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:18.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:18.494] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:18.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:18.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:18.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:18.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:18.495] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" lack","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695698,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:18.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:18.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:18.690] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:18.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:18.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:18.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:18.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:18.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695698,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:18.690] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:18.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:18.881] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:18.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:18.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:18.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:18.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:18.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" traditional","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695698,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:18.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:19.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:19.080] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:19.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:19.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:19.080] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:19.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:19.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" legal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695699,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:19.081] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:19.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:19.289] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:19.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:19.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:19.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:19.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:19.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" frameworks","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695699,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:19.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:19.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:19.498] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:19.498] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:19.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:19.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:19.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:19.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695699,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:19.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:19.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:19.713] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:19.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:19.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:19.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:19.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:19.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" se","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695699,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:19.713] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:19.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:19.920] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:19.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:19.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:19.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:19.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:19.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"cur","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695699,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:19.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:20.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:20.138] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:20.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:20.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:20.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:20.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:20.139] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ities","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695700,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:20.139] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:20.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:20.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:20.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:20.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:20.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:20.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:20.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" trad","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695700,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:20.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:20.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:20.583] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:20.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:20.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:20.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:20.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:20.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695700,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:20.583] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:20.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:20.805] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:20.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:20.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:20.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:20.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:20.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" –","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695700,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:20.805] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:21.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:21.002] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:21.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:21.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:21.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:21.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:21.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" authorities","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695701,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:21.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:21.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:21.212] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:21.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:21.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:21.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:21.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:21.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" around","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695701,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:21.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:21.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:21.406] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:21.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:21.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:21.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:21.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:21.406] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695701,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:21.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:21.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:21.616] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:21.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:21.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:21.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:21.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:21.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" world","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695701,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:21.616] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:21.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:21.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:21.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:21.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:21.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:21.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:21.853] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" will","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695701,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:21.857] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:22.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:22.095] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:22.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:22.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:22.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:22.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:22.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" likely","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695702,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:22.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:22.306] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:22.306] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:22.307] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:22.307] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" continue","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695702,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:22.307] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:22.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:22.519] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:22.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:22.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:22.519] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:22.520] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:22.520] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" evalu","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695702,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:22.520] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:22.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:22.718] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:22.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:22.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:22.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:22.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:22.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ating","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695702,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:22.718] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:22.920] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:22.920] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:22.921] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:22.921] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:22.921] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:22.921] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:22.921] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" each","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695702,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:22.921] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:23.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:23.126] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:23.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:23.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:23.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:23.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:23.126] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" new","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695703,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:23.127] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:23.343] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:23.343] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:23.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:23.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:23.353] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:23.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:23.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" proposal","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695703,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:23.354] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:23.587] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:23.588] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:23.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:23.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:23.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:23.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:23.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" with","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695703,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:23.588] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:23.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:23.810] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:23.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:23.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:23.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:23.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:23.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" an","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695703,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:23.810] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:24.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:24.013] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:24.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:24.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:24.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:24.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:24.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" aim","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695704,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:24.013] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:24.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:24.345] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:24.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:24.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:24.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:24.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:24.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" at","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695704,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:24.346] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:24.546] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:24.546] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:24.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:24.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:24.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:24.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:24.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" finding","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695704,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:24.547] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:24.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:24.769] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:24.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:24.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:24.769] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:24.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:24.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" a","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695704,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:24.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:24.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:24.992] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:24.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:24.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:24.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:24.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:24.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" suitable","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695704,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:24.992] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:25.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:25.205] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:25.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:25.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:25.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:25.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:25.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" balance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695705,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:25.206] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:25.426] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:25.426] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:25.426] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:25.426] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:25.426] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:25.426] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:25.426] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" between","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695705,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:25.429] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:25.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:25.652] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:25.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:25.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:25.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:25.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:25.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" fost","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695705,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:25.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:25.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:25.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:25.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:25.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:25.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:25.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:25.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ering","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695705,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:25.859] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:26.060] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:26.061] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:26.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:26.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:26.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:26.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:26.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" financial","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695706,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:26.061] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:26.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:26.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:26.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:26.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:26.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:26.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:26.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" innov","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695706,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:26.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:26.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:26.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:26.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:26.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:26.534] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:26.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:26.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ation","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695706,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:26.535] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:26.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:26.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:26.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:26.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:26.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:26.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:26.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" while","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695706,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:26.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:26.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:26.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:26.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:26.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:26.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:26.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:26.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" mit","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695706,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:26.970] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:27.211] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:27.212] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:27.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:27.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:27.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:27.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:27.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ig","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695707,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:27.212] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:27.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:27.444] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:27.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:27.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:27.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:27.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:27.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ating","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695707,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:27.444] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:27.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:27.646] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:27.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:27.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:27.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:27.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:27.646] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" system","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695707,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:27.650] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:27.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:27.895] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:27.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:27.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:27.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:27.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:27.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ic","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695707,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:27.895] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:28.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:28.118] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:28.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:28.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:28.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:28.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:28.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" ris","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695708,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:28.118] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:28.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:28.324] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:28.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:28.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:28.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:28.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:28.324] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ks","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695708,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:28.325] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:28.542] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:28.542] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:28.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:28.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:28.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:28.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:28.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" within","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695708,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:28.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:28.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:28.767] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:28.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:28.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:28.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:28.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:28.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" their","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695708,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:28.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:28.974] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:28.974] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:28.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:28.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:28.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:28.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:28.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" juris","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695708,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:28.975] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:29.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:29.197] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:29.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:29.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:29.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:29.197] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:29.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"dict","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695709,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:29.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:29.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:29.470] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:29.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:29.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:29.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:29.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:29.470] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ions","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695709,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:29.471] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:29.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:29.778] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:29.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:29.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:29.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:29.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:29.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695709,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:29.778] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:30.010] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:30.010] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:30.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:30.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695710,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:30.011] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:30.252] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:30.252] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:30.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:30.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"\\","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695710,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:30.253] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:30.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:30.521] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:30.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:30.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:30.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:30.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:30.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695710,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:30.521] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:30.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:30.734] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:30.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:30.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:30.734] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:30.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:30.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"Des","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695710,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:30.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:30.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:30.935] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:30.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:30.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:30.935] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:30.936] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:30.936] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"p","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695710,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:30.936] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:31.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:31.141] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:31.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:31.141] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:31.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:31.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:31.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ite","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695711,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:31.142] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:31.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:31.334] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:31.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:31.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:31.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:31.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:31.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" these","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695711,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:31.338] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:31.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" challeng","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695711,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:31.568] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:31.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:31.761] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:31.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:31.761] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:31.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:31.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:31.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"es","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695711,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:31.762] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:31.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:31.963] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:31.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:31.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:31.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:31.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:31.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695711,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:31.963] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:32.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:32.181] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:32.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:32.181] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:32.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:32.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:32.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695712,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:32.182] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:32.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:32.372] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:32.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:32.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:32.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:32.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:32.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" regul","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695712,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:32.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:32.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:32.571] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:32.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:32.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:32.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:32.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:32.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"atory","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695712,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:32.572] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:32.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:32.754] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:32.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:32.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:32.754] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:32.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:32.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" bodies","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695712,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:32.755] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:32.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:32.938] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:32.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:32.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:32.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:32.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:32.938] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" begin","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695712,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:32.939] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:33.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:33.157] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:33.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:33.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:33.157] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:33.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:33.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695713,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:33.158] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:33.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:33.337] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:33.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:33.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:33.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:33.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:33.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" recognize","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695713,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:33.337] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:33.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:33.525] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:33.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:33.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:33.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:33.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:33.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695713,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:33.525] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:33.714] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:33.714] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:33.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:33.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:33.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:33.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:33.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" understand","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695713,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:33.725] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:33.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:33.929] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:33.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:33.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:33.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:33.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:33.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" block","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695713,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:33.929] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:34.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:34.113] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:34.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:34.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:34.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"chain","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695714,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:34.114] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:34.334] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" technology","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695714,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:34.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:34.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:34.549] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:34.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:34.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:34.549] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:34.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:34.550] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" more","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695714,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:34.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:34.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:34.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:34.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:34.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:34.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:34.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:34.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" int","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695714,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:34.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:34.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:34.978] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:34.978] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:34.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:34.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:34.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:34.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"imately","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695714,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:34.979] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:35.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:35.189] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:35.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:35.189] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:35.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:35.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:35.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" through","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695715,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:35.190] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:35.399] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:35.400] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:35.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:35.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:35.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:35.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:35.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" projects","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695715,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:35.400] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:35.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:35.612] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:35.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:35.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:35.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:35.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:35.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" such","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695715,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:35.612] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:35.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:35.820] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:35.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:35.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:35.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:35.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:35.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" as","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695715,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:35.820] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:36.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:36.023] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:36.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:36.023] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:36.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:36.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:36.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Bit","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695716,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:36.024] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:36.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:36.250] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:36.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:36.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:36.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:36.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:36.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"co","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695716,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:36.251] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:36.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:36.451] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:36.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:36.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:36.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:36.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:36.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695716,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:36.451] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:36.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:36.651] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:36.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:36.651] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:36.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:36.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:36.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"'","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695716,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:36.652] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:36.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:36.848] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:36.848] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:36.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:36.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:36.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:36.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"s","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695716,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:36.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:37.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:37.063] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:37.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:37.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:37.063] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:37.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:37.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" appro","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695717,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:37.064] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:37.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:37.282] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:37.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:37.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:37.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:37.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:37.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"val","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695717,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:37.282] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:37.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:37.494] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:37.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:37.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:37.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:37.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:37.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695717,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:37.494] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:37.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:37.707] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:37.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:37.707] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:37.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:37.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:37.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" its","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695717,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:37.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:37.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:37.912] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:37.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:37.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:37.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:37.912] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:37.913] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695717,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:37.913] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:38.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:38.146] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:38.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:38.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:38.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:38.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:38.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"TF","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695718,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:38.147] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:38.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:38.416] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:38.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:38.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:38.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:38.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:38.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" application","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695718,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:38.416] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:38.626] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:38.626] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:38.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:38.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:38.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:38.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:38.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" or","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695718,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:38.627] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:38.827] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:38.827] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:38.827] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:38.827] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:38.827] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:38.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:38.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" the","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695718,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:38.828] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:39.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:39.035] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:39.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:39.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:39.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:39.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:39.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" introduction","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695719,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:39.035] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:39.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:39.246] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:39.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:39.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:39.246] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:39.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:39.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" of","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695719,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:39.247] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:39.458] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:39.458] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:39.458] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:39.458] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:39.458] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:39.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:39.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" proposed","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695719,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:39.459] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:39.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:39.666] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:39.666] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:39.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:39.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:39.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:39.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" frameworks","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695719,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:39.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:39.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:39.863] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:39.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:39.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:39.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:39.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:39.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" like","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695719,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:39.863] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:40.070] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:40.071] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:40.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:40.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:40.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:40.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:40.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Can","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695720,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:40.071] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:40.288] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:40.289] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:40.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:40.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:40.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:40.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:40.289] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"c","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695720,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:40.293] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:40.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:40.511] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:40.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:40.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:40.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:40.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:40.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"un","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695720,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:40.511] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:40.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:40.709] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:40.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:40.709] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:40.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:40.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:40.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Up","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695720,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:40.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:40.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:40.911] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:40.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:40.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:40.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:40.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:40.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"grade","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695720,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:40.911] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:41.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:41.109] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:41.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:41.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:41.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:41.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:41.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" Plan","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695721,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:41.109] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:41.334] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:41.334] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:41.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:41.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:41.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:41.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:41.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" for","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695721,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:41.335] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:41.539] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:41.539] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:41.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:41.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:41.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:41.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:41.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" en","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695721,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:41.540] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:41.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:41.735] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:41.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:41.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:41.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:41.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:41.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"han","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695721,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:41.735] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:41.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:41.949] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:41.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:41.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:41.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:41.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:41.949] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"cing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695721,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:41.950] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:42.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:42.165] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:42.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:42.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:42.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:42.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:42.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" network","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695722,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:42.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:42.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:42.374] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:42.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:42.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:42.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:42.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:42.374] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" performance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695722,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:42.375] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:42.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:42.584] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:42.584] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:42.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:42.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:42.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:42.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" on","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695722,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:42.585] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:42.793] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:42.794] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:42.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:42.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:42.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:42.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:42.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" E","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695722,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:42.794] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:43.002] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:43.002] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:43.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:43.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:43.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:43.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:43.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"there","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695723,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:43.003] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:43.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:43.232] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:43.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:43.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:43.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:43.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:43.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"um","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695723,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:43.232] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:43.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:43.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:43.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:43.478] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:43.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:43.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:43.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" platform","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695723,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:43.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:43.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:43.699] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:43.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:43.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:43.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:43.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:43.699] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":";","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695723,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:43.702] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:43.915] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:43.916] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:43.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:43.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:43.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:43.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:43.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" it","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695723,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:43.916] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:44.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:44.123] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:44.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:44.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:44.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:44.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:44.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" becomes","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695724,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:44.123] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:44.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:44.326] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:44.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:44.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:44.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:44.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:44.326] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" increasing","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695724,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:44.327] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:44.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:44.528] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:44.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:44.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:44.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:44.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:44.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ly","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695724,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:44.529] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:44.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:44.744] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:44.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:44.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:44.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:44.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:44.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" pla","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695724,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:44.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:44.961] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:44.962] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:44.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:44.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:44.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:44.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:44.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"us","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695724,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:44.962] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:45.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:45.165] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:45.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:45.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:45.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:45.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:45.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ible","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695725,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:45.166] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:45.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:45.381] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:45.381] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:45.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:45.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:45.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:45.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" that","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695725,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:45.382] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:45.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:45.562] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:45.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:45.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:45.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:45.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:45.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" these","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695725,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:45.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:45.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:45.752] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:45.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:45.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:45.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:45.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:45.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" crypt","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695725,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:45.752] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:45.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:45.953] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:45.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:45.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:45.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:45.953] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:45.954] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"oc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695725,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:45.957] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:46.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:46.244] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:46.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:46.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:46.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:46.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:46.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"urr","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695726,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:46.244] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:46.522] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:46.522] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:46.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:46.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:46.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:46.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:46.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"encies","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695726,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:46.523] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:46.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:46.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:46.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:46.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:46.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:46.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" can","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695726,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:46.744] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:46.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:46.934] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:46.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:46.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:46.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:46.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:46.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" find","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695726,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:46.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:47.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:47.156] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:47.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:47.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:47.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:47.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:47.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" their","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695727,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:47.156] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:47.348] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" n","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695727,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:47.348] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:47.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:47.570] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:47.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:47.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:47.570] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:47.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:47.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"iche","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695727,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:47.571] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:47.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:47.760] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:47.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:47.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:47.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:47.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:47.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" within","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695727,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:47.760] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:47.967] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:47.968] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:47.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:47.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:47.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:47.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:47.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" reg","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695727,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:47.968] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:48.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:48.165] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:48.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:48.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:48.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:48.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:48.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ulated","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695728,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:48.165] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:48.349] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:48.350] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:48.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:48.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:48.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:48.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:48.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" mark","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695728,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:48.350] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:48.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:48.560] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:48.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:48.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:48.560] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:48.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:48.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ets","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695728,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:48.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:48.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:48.775] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:48.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:48.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:48.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:48.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:48.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":",","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695728,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:48.775] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:48.971] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:48.971] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:48.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:48.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:48.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:48.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:48.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" leading","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695728,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:48.982] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:49.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:49.204] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:49.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:49.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:49.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:49.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:49.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" to","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695729,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:49.205] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:49.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:49.412] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:49.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:49.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:49.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:49.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:49.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" bro","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695729,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:49.417] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:49.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:49.630] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:49.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:49.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:49.630] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:49.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:49.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ader","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695729,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:49.631] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:49.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:49.881] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:49.881] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:49.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:49.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:49.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:49.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" accept","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695729,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:49.882] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:50.095] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:50.096] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:50.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:50.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:50.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:50.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:50.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ance","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695730,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:50.096] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:50.311] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:50.311] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:50.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:50.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:50.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:50.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:50.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" and","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695730,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:50.312] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:50.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:50.527] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:50.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:50.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:50.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:50.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:50.527] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" growth","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695730,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:50.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:50.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:50.733] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:50.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:50.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:50.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:50.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:50.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" in","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695730,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:50.733] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:50.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:50.948] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:50.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:50.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:50.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:50.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:50.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" main","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695730,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:50.948] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:51.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:51.163] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:51.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:51.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:51.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:51.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:51.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"stream","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695731,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:51.163] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:51.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:51.372] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:51.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:51.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:51.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:51.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:51.372] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" invest","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695731,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:51.373] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:51.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:51.574] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:51.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:51.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:51.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:51.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:51.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"ment","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695731,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:51.575] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:51.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:51.781] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:51.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:51.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:51.781] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:51.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:51.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":" land","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695731,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:51.782] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:51.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:51.995] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:51.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:51.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:51.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:51.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:51.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"sc","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695731,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:51.995] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:52.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:52.213] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:52.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:52.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:52.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:52.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:52.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":"apes","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695732,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:52.213] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:52.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2718: Compute the chat stream chunk successfully.
[2024-09-30 19:28:52.414] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:198: Get output buffer generated by the model named Phi-3-mini-4k-instruct in the stream mode.
[2024-09-30 19:28:52.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2724: retrieved the output buffer
[2024-09-30 19:28:52.414] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2775: decoded the output buffer
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2809: created chat completion chunk
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: {"id":"chatcmpl-7c8f13d1-9b3a-484e-9063-35a031d87fa2","choices":[{"index":0,"delta":{"content":".","role":"assistant"},"logprobs":null,"finish_reason":null}],"created":1727695732,"model":"Phi-3-mini-4k-instruct","system_fingerprint":"fp_44709d6fcb","object":"chat.completion.chunk"}


[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:52.415] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:4267: Return the chat stream chunk!
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: data: [DONE]


[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2675: Compute the chat stream chunk.
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2682: Return the chat stream chunk!
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2639: next item: [GGML] End of sequence
[2024-09-30 19:28:52.415] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2465: Clean up the context of the stream work environment.
[2024-09-30 19:28:52.415] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:28:52.415] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  136616.68 ms
[2024-09-30 19:28:52.415] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =     146.13 ms /   644 runs   (    0.23 ms per token,  4407.10 tokens per second)
[2024-09-30 19:28:52.415] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =  111812.59 ms /   813 tokens (  137.53 ms per token,     7.27 tokens per second)
[2024-09-30 19:28:52.415] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =  136896.25 ms /   642 runs   (  213.23 ms per token,     4.69 tokens per second)
[2024-09-30 19:28:52.415] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  273694.68 ms /  1455 tokens
[2024-09-30 19:28:52.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2617: Cleanup done!
[2024-09-30 19:28:52.424] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47272, local_addr: 0.0.0.0:8081
[2024-09-30 19:28:52.425] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:28:52.425] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:28:52.425] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:28:52.425] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:28:52.425] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-22863e3e-b3b1-418c-87cb-7ce182368fc8
[2024-09-30 19:28:52.425] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:28:52.425] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:28:52.425] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:28:52.425] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:28:52.425] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:28:52.425] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:28:52.425] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:28:52.425] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:28:52.425] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:28:52.425] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:28:52.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:28:52.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:28:52.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:28:52.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:28:52.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:28:52.425] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:28:52.427] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:28:52.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:28:52.427] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:28:52.429] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:28:52.429] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:28:52.432] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:28:52.433] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:28:52.433] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:28:52.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:28:52.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:28:52.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:28:52.462] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:28:52.462] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  273641.23 ms
[2024-09-30 19:28:52.462] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:28:52.462] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.80 ms /     6 tokens (    4.13 ms per token,   241.93 tokens per second)
[2024-09-30 19:28:52.462] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:28:52.462] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  273640.50 ms /     7 tokens
[2024-09-30 19:28:52.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:28:52.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:28:52.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:28:52.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:28:52.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:28:52.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:28:52.468] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:28:52.468] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:28:52.468] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:28:52.468] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:28:52.469] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:28:52.469] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:28:52.469] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:28:52.486] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:28:52.486] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:28:52.486] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:28:52.486] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-22863e3e-b3b1-418c-87cb-7ce182368fc8
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:28:52.486] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:28:52.487] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:28:52.487] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:28:52.487] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:28:52.487] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:28:52.487] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:28:52.487] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:28:52.487] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:28:52.487] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:28:52.664] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:28:52.664] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:28:52.664] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:28:52.666] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:28:52.666] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:28:52.666] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:28:52.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:28:52.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:28:52.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 96
[2024-09-30 19:28:52.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 0
[2024-09-30 19:28:52.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:28:52.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:28:52.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:28:52.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:28:52.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:28:52.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:28:52.671] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:28:52.671] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:28:52.671] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:28:52.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:28:52.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:28:52.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:28:52.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:28:52.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:28:52.846] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:28:52.846] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:28:52.846] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:28:52.846] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:28:52.846] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:28:52.846] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:28:52.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:28:52.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:28:52.849] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:28:52.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:28:52.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:28:52.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:28:52.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:28:52.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:28:52.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:28:53.028] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:28:53.028] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:28:53.028] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:28:53.029] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:28:53.029] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:28:53.029] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:28:59.417] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:28:59.417] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:28:59.417] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  277367.46 ms
[2024-09-30 19:28:59.417] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.10 ms /    20 runs   (    0.21 ms per token,  4874.48 tokens per second)
[2024-09-30 19:28:59.417] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3058.35 ms /    24 tokens (  127.43 ms per token,     7.85 tokens per second)
[2024-09-30 19:28:59.417] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3324.24 ms /    19 runs   (  174.96 ms per token,     5.72 tokens per second)
[2024-09-30 19:28:59.417] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  280696.68 ms /    43 tokens
[2024-09-30 19:28:59.421] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:28:59.421] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:28:59.421] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, a large language model trained by Microsoft to assist users in various tasks.<|end|>
[2024-09-30 19:28:59.421] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:28:59.421] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, a large language model trained by Microsoft to assist users in various tasks.
[2024-09-30 19:28:59.421] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:28:59.421] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:28:59.422] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:28:59.422] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:28:59.422] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:28:59.422] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:28:59.422] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:28:59.422] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:28:59.422] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:28:59.422] [info] rag_api_server in src/main.rs:517: response_body_size: 399
[2024-09-30 19:28:59.422] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:28:59.422] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:29:50.342] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:56216, local_addr: 0.0.0.0:8081
[2024-09-30 19:29:50.343] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:29:50.344] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:29:50.344] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:29:50.344] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:29:50.344] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-c8b879e3-83f6-4916-8faa-0cf37d415344
[2024-09-30 19:29:50.344] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:29:50.344] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:29:50.344] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:29:50.344] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:29:50.344] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:29:50.344] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:29:50.344] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:29:50.344] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:29:50.344] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:29:50.344] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:29:50.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:29:50.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:29:50.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:29:50.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:29:50.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:29:50.344] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:29:50.347] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:29:50.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:29:50.347] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:29:50.348] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:29:50.348] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:29:50.348] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:29:50.349] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:29:50.349] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:29:50.349] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:29:50.349] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:29:50.349] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:29:50.349] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:29:50.349] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:29:50.352] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:29:50.352] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:29:50.352] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:29:50.354] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:29:50.354] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:29:50.354] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:29:50.393] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:29:50.393] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  331572.74 ms
[2024-09-30 19:29:50.393] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:29:50.393] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      38.01 ms /     6 tokens (    6.33 ms per token,   157.85 tokens per second)
[2024-09-30 19:29:50.393] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:29:50.393] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  331572.50 ms /     7 tokens
[2024-09-30 19:29:50.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:29:50.394] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:29:50.399] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:29:50.399] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:29:50.399] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:29:50.399] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:29:50.399] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:29:50.399] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:29:50.400] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:29:50.400] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:29:50.400] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:29:50.400] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:29:50.400] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:29:50.418] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:29:50.418] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:29:50.419] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:29:50.419] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-c8b879e3-83f6-4916-8faa-0cf37d415344
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:29:50.419] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:29:50.419] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:29:50.419] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:29:50.419] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:29:50.419] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:29:50.419] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:29:50.419] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:29:50.419] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:29:50.938] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:29:50.938] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:29:50.938] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:29:50.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:29:50.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:29:50.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:29:50.946] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:29:50.946] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:29:50.946] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:29:50.946] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:29:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:29:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:29:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:29:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:29:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:29:50.946] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:29:50.947] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:29:50.947] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:29:50.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:29:50.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:29:50.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:29:50.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:29:50.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:29:50.947] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:29:51.134] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:29:51.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:29:51.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:29:51.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:29:51.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:29:51.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:29:51.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:29:51.138] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:29:51.138] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:29:51.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:29:51.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:29:51.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:29:51.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:29:51.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:29:51.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:29:51.326] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:29:51.326] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:29:51.326] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:29:51.328] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:29:51.328] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:29:51.328] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:29:57.679] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:29:57.679] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:29:57.679] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  335479.71 ms
[2024-09-30 19:29:57.679] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.66 ms /    22 runs   (    0.21 ms per token,  4716.98 tokens per second)
[2024-09-30 19:29:57.679] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2872.08 ms /    24 tokens (  119.67 ms per token,     8.36 tokens per second)
[2024-09-30 19:29:57.679] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3473.35 ms /    21 runs   (  165.40 ms per token,     6.05 tokens per second)
[2024-09-30 19:29:57.679] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  338958.68 ms /    45 tokens
[2024-09-30 19:29:57.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:29:57.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 78
[2024-09-30 19:29:57.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, an AI created by Microsoft. How can I help you today?<|end|>
[2024-09-30 19:29:57.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:29:57.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, an AI created by Microsoft. How can I help you today?
[2024-09-30 19:29:57.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:29:57.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:29:57.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:29:57.682] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 22
[2024-09-30 19:29:57.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 22
[2024-09-30 19:29:57.682] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:29:57.683] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:29:57.683] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:29:57.683] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:29:57.683] [info] rag_api_server in src/main.rs:517: response_body_size: 382
[2024-09-30 19:29:57.683] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:29:57.683] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:30:47.707] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:36276, local_addr: 0.0.0.0:8081
[2024-09-30 19:30:47.708] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:30:47.708] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:30:47.708] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:30:47.708] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:30:47.708] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-7af9a0a0-6605-4993-8a76-7faf8c55b792
[2024-09-30 19:30:47.708] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:30:47.708] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:30:47.708] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:30:47.709] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:30:47.709] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:30:47.709] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:30:47.709] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:30:47.709] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:30:47.709] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:30:47.709] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:30:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:30:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:30:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:30:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:30:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:30:47.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:30:47.712] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:30:47.712] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:30:47.712] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:30:47.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:30:47.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:30:47.714] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:30:47.715] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:30:47.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:30:47.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:30:47.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:30:47.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:30:47.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:30:47.715] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:30:47.721] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:30:47.721] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:30:47.721] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:30:47.723] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:30:47.723] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:30:47.723] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:30:47.758] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:30:47.758] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  388937.85 ms
[2024-09-30 19:30:47.758] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:30:47.758] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      33.63 ms /     6 tokens (    5.60 ms per token,   178.43 tokens per second)
[2024-09-30 19:30:47.758] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:30:47.758] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  388937.50 ms /     7 tokens
[2024-09-30 19:30:47.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:30:47.759] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:30:47.765] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:30:47.765] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:30:47.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:30:47.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:30:47.766] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:30:47.766] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:30:47.767] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:30:47.767] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:30:47.767] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:30:47.767] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:30:47.767] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:30:47.787] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:30:47.787] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:30:47.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:30:47.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:30:47.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:30:47.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:30:47.787] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:30:47.787] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:30:47.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-7af9a0a0-6605-4993-8a76-7faf8c55b792
[2024-09-30 19:30:47.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:30:47.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:30:47.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:30:47.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:30:47.788] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:30:47.788] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:30:47.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:30:47.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:30:47.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:30:47.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:30:47.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:30:47.788] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:30:48.269] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:30:48.269] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:30:48.269] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:30:48.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:30:48.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:30:48.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:30:48.276] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:30:48.276] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:30:48.276] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:30:48.276] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 22
[2024-09-30 19:30:48.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:30:48.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:30:48.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:30:48.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:30:48.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:30:48.276] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:30:48.277] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:30:48.277] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:30:48.277] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:30:48.277] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:30:48.277] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:30:48.277] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:30:48.277] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:30:48.277] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:30:48.445] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:30:48.445] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:30:48.445] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:30:48.446] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:30:48.446] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:30:48.446] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:30:48.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:30:48.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:30:48.449] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:30:48.449] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:30:48.449] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:30:48.449] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:30:48.449] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:30:48.449] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:30:48.449] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:30:48.616] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:30:48.616] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:30:48.616] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:30:48.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:30:48.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:30:48.617] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:30:55.904] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:30:55.904] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:30:55.904] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  392535.62 ms
[2024-09-30 19:30:55.904] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.41 ms /    31 runs   (    0.21 ms per token,  4839.21 tokens per second)
[2024-09-30 19:30:55.904] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2639.11 ms /    24 tokens (  109.96 ms per token,     9.09 tokens per second)
[2024-09-30 19:30:55.904] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4640.28 ms /    30 runs   (  154.68 ms per token,     6.47 tokens per second)
[2024-09-30 19:30:55.904] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  397183.68 ms /    54 tokens
[2024-09-30 19:30:55.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:30:55.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 134
[2024-09-30 19:30:55.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft to assist with a wide array of tasks and provide information. How can I help you today?<|end|>
[2024-09-30 19:30:55.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:30:55.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft to assist with a wide array of tasks and provide information. How can I help you today?
[2024-09-30 19:30:55.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:30:55.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:30:55.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:30:55.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 31
[2024-09-30 19:30:55.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 31
[2024-09-30 19:30:55.907] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:30:55.908] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:30:55.908] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:30:55.908] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:30:55.908] [info] rag_api_server in src/main.rs:517: response_body_size: 438
[2024-09-30 19:30:55.908] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:30:55.908] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:31:45.930] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:52518, local_addr: 0.0.0.0:8081
[2024-09-30 19:31:45.931] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:31:45.931] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:31:45.931] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:31:45.931] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:31:45.931] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-a643d885-8805-47ad-936d-df86bba5467f
[2024-09-30 19:31:45.931] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:31:45.931] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:31:45.931] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:31:45.931] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:31:45.931] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:31:45.931] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:31:45.931] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:31:45.931] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:31:45.931] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:31:45.931] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:31:45.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:31:45.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:31:45.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:31:45.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:31:45.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:31:45.931] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:31:45.934] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:31:45.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:31:45.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:31:45.936] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:31:45.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:31:45.942] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:31:45.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:31:45.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:31:45.944] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:31:45.944] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:31:45.944] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:31:45.975] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:31:45.975] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  447154.61 ms
[2024-09-30 19:31:45.975] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:31:45.975] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      30.04 ms /     6 tokens (    5.01 ms per token,   199.71 tokens per second)
[2024-09-30 19:31:45.975] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:31:45.975] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  447154.50 ms /     7 tokens
[2024-09-30 19:31:45.976] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:31:45.976] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:31:45.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:31:45.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:31:45.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:31:45.982] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:31:45.982] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:31:45.982] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:31:45.982] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:31:45.982] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:31:45.982] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:31:45.982] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:31:45.982] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:31:46.000] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:31:46.001] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:31:46.001] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:31:46.001] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-a643d885-8805-47ad-936d-df86bba5467f
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:31:46.001] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:31:46.001] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:31:46.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:31:46.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:31:46.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:31:46.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:31:46.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:31:46.001] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:31:46.510] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:31:46.510] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:31:46.510] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:31:46.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:31:46.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:31:46.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:31:46.515] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:31:46.515] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:31:46.516] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:31:46.516] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 31
[2024-09-30 19:31:46.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:31:46.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:31:46.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:31:46.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:31:46.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:31:46.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:31:46.516] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:31:46.516] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:31:46.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:31:46.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:31:46.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:31:46.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:31:46.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:31:46.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:31:46.702] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:31:46.702] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:31:46.702] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:31:46.703] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:31:46.703] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:31:46.703] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:31:46.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:31:46.708] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:31:46.709] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:31:46.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:31:46.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:31:46.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:31:46.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:31:46.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:31:46.709] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:31:46.897] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:31:46.897] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:31:46.897] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:31:46.898] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:31:46.898] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:31:46.898] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:31:53.476] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:31:53.476] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:31:53.476] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  451026.40 ms
[2024-09-30 19:31:53.476] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.22 ms /    20 runs   (    0.21 ms per token,  4734.85 tokens per second)
[2024-09-30 19:31:53.476] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2848.78 ms /    24 tokens (  118.70 ms per token,     8.42 tokens per second)
[2024-09-30 19:31:53.476] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3724.12 ms /    19 runs   (  196.01 ms per token,     5.10 tokens per second)
[2024-09-30 19:31:53.476] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  454755.68 ms /    43 tokens
[2024-09-30 19:31:53.479] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:31:53.479] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 92
[2024-09-30 19:31:53.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence created by Microsoft. How can I help you today?<|end|>
[2024-09-30 19:31:53.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:31:53.479] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence created by Microsoft. How can I help you today?
[2024-09-30 19:31:53.480] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:31:53.480] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:31:53.480] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:31:53.480] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:31:53.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:31:53.480] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:31:53.480] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:31:53.480] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:31:53.480] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:31:53.480] [info] rag_api_server in src/main.rs:517: response_body_size: 396
[2024-09-30 19:31:53.480] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:31:53.480] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:32:43.504] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:59068, local_addr: 0.0.0.0:8081
[2024-09-30 19:32:43.504] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:32:43.504] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:32:43.504] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:32:43.504] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:32:43.505] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-0ee3b48b-996a-4393-9125-0bb6b9ad22db
[2024-09-30 19:32:43.505] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:32:43.505] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:32:43.505] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:32:43.505] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:32:43.505] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:32:43.505] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:32:43.505] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:32:43.505] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:32:43.505] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:32:43.505] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:32:43.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:32:43.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:32:43.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:32:43.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:32:43.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:32:43.505] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:32:43.508] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:32:43.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:32:43.508] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:32:43.510] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:32:43.510] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:32:43.510] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:32:43.511] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:32:43.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:32:43.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:32:43.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:32:43.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:32:43.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:32:43.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:32:43.517] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:32:43.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:32:43.517] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:32:43.520] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:32:43.520] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:32:43.520] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:32:43.549] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:32:43.549] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  504728.89 ms
[2024-09-30 19:32:43.549] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:32:43.549] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      28.05 ms /     6 tokens (    4.68 ms per token,   213.90 tokens per second)
[2024-09-30 19:32:43.550] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:32:43.550] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  504728.50 ms /     7 tokens
[2024-09-30 19:32:43.550] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:32:43.550] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:32:43.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:32:43.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:32:43.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:32:43.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:32:43.555] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:32:43.555] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:32:43.555] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:32:43.556] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:32:43.556] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:32:43.556] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:32:43.556] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:32:43.575] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:32:43.576] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:32:43.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:32:43.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:32:43.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:32:43.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:32:43.576] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:32:43.576] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:32:43.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-0ee3b48b-996a-4393-9125-0bb6b9ad22db
[2024-09-30 19:32:43.576] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:32:43.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:32:43.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:32:43.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:32:43.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:32:43.578] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:32:43.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:32:43.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:32:43.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:32:43.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:32:43.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:32:43.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:32:44.132] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:32:44.133] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:32:44.133] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:32:44.139] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:32:44.139] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:32:44.139] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:32:44.145] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:32:44.145] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:32:44.145] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:32:44.145] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:32:44.145] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:32:44.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:32:44.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:32:44.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:32:44.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:32:44.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:32:44.146] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:32:44.146] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:32:44.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:32:44.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:32:44.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:32:44.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:32:44.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:32:44.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:32:44.407] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:32:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:32:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:32:44.408] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:32:44.408] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:32:44.408] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:32:44.410] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:32:44.411] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:32:44.411] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:32:44.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:32:44.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:32:44.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:32:44.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:32:44.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:32:44.411] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:32:44.562] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:32:44.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:32:44.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:32:44.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:32:44.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:32:44.563] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:32:50.485] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:32:50.485] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:32:50.485] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  508503.87 ms
[2024-09-30 19:32:50.485] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.53 ms /    21 runs   (    0.22 ms per token,  4631.67 tokens per second)
[2024-09-30 19:32:50.485] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2660.86 ms /    24 tokens (  110.87 ms per token,     9.02 tokens per second)
[2024-09-30 19:32:50.485] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3255.46 ms /    20 runs   (  162.77 ms per token,     6.14 tokens per second)
[2024-09-30 19:32:50.486] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  511764.68 ms /    44 tokens
[2024-09-30 19:32:50.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:32:50.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 74
[2024-09-30 19:32:50.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 19:32:50.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:32:50.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 19:32:50.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:32:50.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:32:50.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:32:50.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 19:32:50.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 21
[2024-09-30 19:32:50.489] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:32:50.489] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:32:50.489] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:32:50.490] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:32:50.490] [info] rag_api_server in src/main.rs:517: response_body_size: 378
[2024-09-30 19:32:50.490] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:32:50.490] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:33:40.512] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:60808, local_addr: 0.0.0.0:8081
[2024-09-30 19:33:40.513] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:33:40.513] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:33:40.513] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:33:40.513] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:33:40.513] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-41cffa99-6c77-4841-a92c-3eaea3f204d5
[2024-09-30 19:33:40.514] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:33:40.514] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:33:40.514] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:33:40.514] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:33:40.514] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:33:40.514] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:33:40.514] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:33:40.514] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:33:40.514] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:33:40.514] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:33:40.514] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:33:40.514] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:33:40.514] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:33:40.514] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:33:40.514] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:33:40.514] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:33:40.516] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:33:40.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:33:40.516] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:33:40.518] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:33:40.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:33:40.522] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:33:40.522] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:33:40.522] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:33:40.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:33:40.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:33:40.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:33:40.550] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:33:40.550] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  561729.10 ms
[2024-09-30 19:33:40.550] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:33:40.550] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.78 ms /     6 tokens (    3.96 ms per token,   252.31 tokens per second)
[2024-09-30 19:33:40.550] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:33:40.550] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  561728.50 ms /     7 tokens
[2024-09-30 19:33:40.550] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:33:40.550] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:33:40.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:33:40.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:33:40.555] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:33:40.556] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:33:40.556] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:33:40.556] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:33:40.556] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:33:40.556] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:33:40.556] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:33:40.556] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:33:40.556] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:33:40.573] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:33:40.573] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:33:40.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:33:40.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:33:40.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:33:40.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:33:40.573] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:33:40.573] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:33:40.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-41cffa99-6c77-4841-a92c-3eaea3f204d5
[2024-09-30 19:33:40.573] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:33:40.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:33:40.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:33:40.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:33:40.574] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:33:40.574] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:33:40.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:33:40.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:33:40.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:33:40.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:33:40.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:33:40.574] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:33:41.024] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:33:41.024] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:33:41.024] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:33:41.027] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:33:41.027] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:33:41.027] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:33:41.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:33:41.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:33:41.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:33:41.030] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 21
[2024-09-30 19:33:41.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:33:41.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:33:41.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:33:41.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:33:41.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:33:41.030] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:33:41.031] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:33:41.031] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:33:41.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:33:41.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:33:41.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:33:41.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:33:41.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:33:41.031] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:33:41.204] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:33:41.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:33:41.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:33:41.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:33:41.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:33:41.205] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:33:41.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:33:41.208] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:33:41.208] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:33:41.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:33:41.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:33:41.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:33:41.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:33:41.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:33:41.208] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:33:41.371] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:33:41.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:33:41.371] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:33:41.372] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:33:41.372] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:33:41.372] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:33:47.740] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:33:47.740] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:33:47.740] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  565530.71 ms
[2024-09-30 19:33:47.740] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.53 ms /    23 runs   (    0.20 ms per token,  5077.26 tokens per second)
[2024-09-30 19:33:47.740] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2878.91 ms /    24 tokens (  119.95 ms per token,     8.34 tokens per second)
[2024-09-30 19:33:47.740] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3482.70 ms /    22 runs   (  158.30 ms per token,     6.32 tokens per second)
[2024-09-30 19:33:47.740] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  569018.68 ms /    46 tokens
[2024-09-30 19:33:47.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:33:47.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 99
[2024-09-30 19:33:47.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an advanced conversational AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 19:33:47.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:33:47.742] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an advanced conversational AI developed by Microsoft. How can I assist you today?
[2024-09-30 19:33:47.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:33:47.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:33:47.742] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:33:47.743] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 23
[2024-09-30 19:33:47.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 23
[2024-09-30 19:33:47.743] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:33:47.743] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:33:47.743] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:33:47.743] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:33:47.743] [info] rag_api_server in src/main.rs:517: response_body_size: 403
[2024-09-30 19:33:47.743] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:33:47.743] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:34:37.767] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:57778, local_addr: 0.0.0.0:8081
[2024-09-30 19:34:37.768] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:34:37.768] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:34:37.768] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:34:37.768] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:34:37.768] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-adf61547-e95a-4cfa-a037-d6b9a959bf2e
[2024-09-30 19:34:37.769] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:34:37.769] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:34:37.769] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:34:37.769] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:34:37.769] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:34:37.769] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:34:37.769] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:34:37.769] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:34:37.769] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:34:37.769] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:34:37.769] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:34:37.769] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:34:37.769] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:34:37.769] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:34:37.769] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:34:37.769] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:34:37.773] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:34:37.773] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:34:37.773] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:34:37.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:34:37.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:34:37.778] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:34:37.779] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:34:37.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:34:37.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:34:37.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:34:37.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:34:37.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:34:37.779] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:34:37.787] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:34:37.787] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:34:37.787] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:34:37.794] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:34:37.795] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:34:37.795] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:34:37.861] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:34:37.861] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  619040.08 ms
[2024-09-30 19:34:37.861] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:34:37.861] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      63.56 ms /     6 tokens (   10.59 ms per token,    94.40 tokens per second)
[2024-09-30 19:34:37.861] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:34:37.861] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  619039.50 ms /     7 tokens
[2024-09-30 19:34:37.861] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:34:37.861] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:34:37.867] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:34:37.867] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:34:37.867] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:34:37.867] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:34:37.867] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:34:37.867] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:34:37.867] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:34:37.867] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:34:37.867] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:34:37.867] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:34:37.867] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:34:37.898] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:34:37.898] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:34:37.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:34:37.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:34:37.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:34:37.898] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:34:37.898] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:34:37.899] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:34:37.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-adf61547-e95a-4cfa-a037-d6b9a959bf2e
[2024-09-30 19:34:37.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:34:37.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:34:37.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:34:37.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:34:37.900] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:34:37.900] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:34:37.900] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:34:37.900] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:34:37.900] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:34:37.900] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:34:37.900] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:34:37.900] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:34:39.099] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:34:39.099] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:34:39.099] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:34:39.104] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:34:39.104] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:34:39.104] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:34:39.112] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:34:39.112] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:34:39.113] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:34:39.113] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 23
[2024-09-30 19:34:39.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:34:39.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:34:39.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:34:39.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:34:39.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:34:39.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:34:39.113] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:34:39.114] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:34:39.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:34:39.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:34:39.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:34:39.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:34:39.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:34:39.114] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:34:39.550] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:34:39.550] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:34:39.550] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:34:39.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:34:39.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:34:39.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:34:39.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:34:39.561] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:34:39.561] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:34:39.561] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:34:39.561] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:34:39.561] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:34:39.561] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:34:39.561] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:34:39.561] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:34:40.299] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:34:40.300] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:34:40.300] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:34:40.302] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:34:40.302] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:34:40.302] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:34:56.452] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:34:56.452] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:34:56.452] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  626296.67 ms
[2024-09-30 19:34:56.452] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.27 ms /    24 runs   (    0.26 ms per token,  3825.92 tokens per second)
[2024-09-30 19:34:56.452] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    4715.32 ms /    24 tokens (  196.47 ms per token,     5.09 tokens per second)
[2024-09-30 19:34:56.452] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =   11425.78 ms /    23 runs   (  496.77 ms per token,     2.01 tokens per second)
[2024-09-30 19:34:56.452] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  637731.68 ms /    47 tokens
[2024-09-30 19:34:56.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:34:56.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 107
[2024-09-30 19:34:56.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, your helpful assistant developed to assist you with information and tasks. How can I help?<|end|>
[2024-09-30 19:34:56.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:34:56.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, your helpful assistant developed to assist you with information and tasks. How can I help?
[2024-09-30 19:34:56.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:34:56.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:34:56.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:34:56.462] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:34:56.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:34:56.462] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:34:56.463] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:34:56.463] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:34:56.463] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:34:56.463] [info] rag_api_server in src/main.rs:517: response_body_size: 411
[2024-09-30 19:34:56.463] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:34:56.463] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:35:46.488] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:35288, local_addr: 0.0.0.0:8081
[2024-09-30 19:35:46.488] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:35:46.488] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:35:46.488] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:35:46.488] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:35:46.489] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-3d1a1fce-0285-4809-9d15-7cb58ea1cb50
[2024-09-30 19:35:46.489] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:35:46.489] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:35:46.489] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:35:46.489] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:35:46.489] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:35:46.489] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:35:46.489] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:35:46.489] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:35:46.489] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:35:46.489] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:35:46.489] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:35:46.489] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:35:46.489] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:35:46.489] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:35:46.489] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:35:46.489] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:35:46.491] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:35:46.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:35:46.491] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:35:46.494] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:35:46.494] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:35:46.494] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:35:46.495] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:35:46.495] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:35:46.495] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:35:46.495] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:35:46.495] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:35:46.495] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:35:46.495] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:35:46.500] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:35:46.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:35:46.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:35:46.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:35:46.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:35:46.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:35:46.528] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:35:46.528] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  687707.56 ms
[2024-09-30 19:35:46.528] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:35:46.528] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      25.30 ms /     6 tokens (    4.22 ms per token,   237.20 tokens per second)
[2024-09-30 19:35:46.528] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:35:46.528] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  687707.50 ms /     7 tokens
[2024-09-30 19:35:46.529] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:35:46.529] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:35:46.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:35:46.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:35:46.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:35:46.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:35:46.534] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:35:46.534] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:35:46.534] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:35:46.534] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:35:46.534] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:35:46.534] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:35:46.535] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:35:46.552] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:35:46.552] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:35:46.552] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:35:46.552] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-3d1a1fce-0285-4809-9d15-7cb58ea1cb50
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:35:46.552] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:35:46.553] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:35:46.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:35:46.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:35:46.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:35:46.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:35:46.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:35:46.553] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:35:47.194] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:35:47.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:35:47.195] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:35:47.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:35:47.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:35:47.198] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:35:47.202] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:35:47.202] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:35:47.202] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:35:47.202] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:35:47.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:35:47.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:35:47.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:35:47.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:35:47.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:35:47.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:35:47.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:35:47.204] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:35:47.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:35:47.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:35:47.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:35:47.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:35:47.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:35:47.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:35:47.432] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:35:47.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:35:47.432] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:35:47.434] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:35:47.434] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:35:47.434] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:35:47.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:35:47.439] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:35:47.439] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:35:47.439] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:35:47.439] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:35:47.439] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:35:47.439] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:35:47.439] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:35:47.439] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:35:47.685] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:35:47.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:35:47.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:35:47.686] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:35:47.686] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:35:47.686] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:35:52.701] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:35:52.701] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:35:52.701] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  691908.55 ms
[2024-09-30 19:35:52.701] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       2.60 ms /    13 runs   (    0.20 ms per token,  5001.92 tokens per second)
[2024-09-30 19:35:52.701] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2942.95 ms /    24 tokens (  122.62 ms per token,     8.16 tokens per second)
[2024-09-30 19:35:52.701] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2068.88 ms /    12 runs   (  172.41 ms per token,     5.80 tokens per second)
[2024-09-30 19:35:52.701] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  693980.68 ms /    36 tokens
[2024-09-30 19:35:52.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:35:52.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 47
[2024-09-30 19:35:52.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft.<|end|>
[2024-09-30 19:35:52.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:35:52.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft.
[2024-09-30 19:35:52.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:35:52.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:35:52.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:35:52.704] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 19:35:52.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 13
[2024-09-30 19:35:52.704] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:35:52.705] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:35:52.705] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:35:52.705] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:35:52.705] [info] rag_api_server in src/main.rs:517: response_body_size: 351
[2024-09-30 19:35:52.705] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:35:52.705] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:36:42.730] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:37856, local_addr: 0.0.0.0:8081
[2024-09-30 19:36:42.731] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:36:42.731] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:36:42.731] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:36:42.731] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:36:42.731] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-a3c3ed59-0ebd-47b6-96e5-e16169f81e56
[2024-09-30 19:36:42.731] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:36:42.731] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:36:42.731] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:36:42.732] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:36:42.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:36:42.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:36:42.732] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:36:42.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:36:42.732] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:36:42.732] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:36:42.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:36:42.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:36:42.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:36:42.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:36:42.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:36:42.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:36:42.734] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:36:42.734] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:36:42.734] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:36:42.737] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:36:42.737] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:36:42.737] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:36:42.738] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:36:42.738] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:36:42.738] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:36:42.738] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:36:42.738] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:36:42.738] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:36:42.738] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:36:42.746] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:36:42.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:36:42.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:36:42.748] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:36:42.748] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:36:42.748] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:36:42.781] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:36:42.781] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  743960.42 ms
[2024-09-30 19:36:42.781] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:36:42.781] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      32.53 ms /     6 tokens (    5.42 ms per token,   184.46 tokens per second)
[2024-09-30 19:36:42.781] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:36:42.781] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  743959.50 ms /     7 tokens
[2024-09-30 19:36:42.781] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:36:42.781] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:36:42.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:36:42.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:36:42.788] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:36:42.788] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:36:42.788] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:36:42.788] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:36:42.788] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:36:42.788] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:36:42.788] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:36:42.788] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:36:42.788] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:36:42.808] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:36:42.808] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:36:42.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:36:42.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:36:42.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:36:42.808] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:36:42.809] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:36:42.809] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:36:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-a3c3ed59-0ebd-47b6-96e5-e16169f81e56
[2024-09-30 19:36:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:36:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:36:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:36:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:36:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:36:42.809] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:36:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:36:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:36:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:36:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:36:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:36:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:36:43.392] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:36:43.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:36:43.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:36:43.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:36:43.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:36:43.397] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:36:43.404] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:36:43.404] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:36:43.404] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:36:43.404] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 19:36:43.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:36:43.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:36:43.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:36:43.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:36:43.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:36:43.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:36:43.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:36:43.405] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:36:43.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:36:43.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:36:43.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:36:43.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:36:43.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:36:43.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:36:43.660] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:36:43.660] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:36:43.660] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:36:43.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:36:43.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:36:43.661] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:36:43.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:36:43.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:36:43.667] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:36:43.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:36:43.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:36:43.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:36:43.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:36:43.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:36:43.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:36:43.874] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:36:43.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:36:43.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:36:43.875] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:36:43.875] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:36:43.875] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:36:52.791] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:36:52.791] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:36:52.791] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  749207.25 ms
[2024-09-30 19:36:52.791] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.87 ms /    28 runs   (    0.21 ms per token,  4769.20 tokens per second)
[2024-09-30 19:36:52.791] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    4052.08 ms /    24 tokens (  168.84 ms per token,     5.92 tokens per second)
[2024-09-30 19:36:52.791] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4855.55 ms /    27 runs   (  179.84 ms per token,     5.56 tokens per second)
[2024-09-30 19:36:52.791] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  754069.68 ms /    51 tokens
[2024-09-30 19:36:52.794] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:36:52.795] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 129
[2024-09-30 19:36:52.795] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, a virtual assistant designed to provide information and help you with various tasks. How can I assist you today?<|end|>
[2024-09-30 19:36:52.795] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:36:52.795] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, a virtual assistant designed to provide information and help you with various tasks. How can I assist you today?
[2024-09-30 19:36:52.795] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:36:52.795] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:36:52.795] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:36:52.795] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:36:52.795] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:36:52.795] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:36:52.796] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:36:52.796] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:36:52.796] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:36:52.796] [info] rag_api_server in src/main.rs:517: response_body_size: 433
[2024-09-30 19:36:52.796] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:36:52.796] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:37:42.834] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47678, local_addr: 0.0.0.0:8081
[2024-09-30 19:37:42.835] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:37:42.835] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:37:42.835] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:37:42.835] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:37:42.835] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-7b0a5e9a-bc4a-49a7-af81-74e8d23dfdbb
[2024-09-30 19:37:42.835] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:37:42.835] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:37:42.835] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:37:42.835] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:37:42.835] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:37:42.835] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:37:42.835] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:37:42.835] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:37:42.835] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:37:42.835] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:37:42.835] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:37:42.835] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:37:42.835] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:37:42.835] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:37:42.835] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:37:42.835] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:37:42.838] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:37:42.838] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:37:42.838] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:37:42.841] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:37:42.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:37:42.846] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:37:42.846] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:37:42.846] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:37:42.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:37:42.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:37:42.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:37:42.875] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:37:42.875] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  804054.11 ms
[2024-09-30 19:37:42.875] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:37:42.875] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      26.04 ms /     6 tokens (    4.34 ms per token,   230.39 tokens per second)
[2024-09-30 19:37:42.875] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:37:42.875] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  804053.50 ms /     7 tokens
[2024-09-30 19:37:42.875] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:37:42.875] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:37:42.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:37:42.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:37:42.880] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:37:42.881] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:37:42.881] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:37:42.881] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:37:42.881] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:37:42.881] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:37:42.881] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:37:42.881] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:37:42.881] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:37:42.899] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:37:42.899] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:37:42.899] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:37:42.899] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-7b0a5e9a-bc4a-49a7-af81-74e8d23dfdbb
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:37:42.899] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:37:42.899] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:37:42.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:37:42.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:37:42.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:37:42.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:37:42.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:37:42.899] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:37:43.354] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:37:43.354] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:37:43.354] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:37:43.357] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:37:43.357] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:37:43.357] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:37:43.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:37:43.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:37:43.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:37:43.360] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:37:43.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:37:43.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:37:43.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:37:43.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:37:43.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:37:43.360] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:37:43.361] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:37:43.361] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:37:43.361] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:37:43.361] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:37:43.361] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:37:43.361] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:37:43.361] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:37:43.361] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:37:43.522] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:37:43.522] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:37:43.522] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:37:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:37:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:37:43.524] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:37:43.526] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:37:43.526] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:37:43.527] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:37:43.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:37:43.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:37:43.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:37:43.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:37:43.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:37:43.527] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:37:43.731] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:37:43.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:37:43.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:37:43.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:37:43.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:37:43.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:37:51.769] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:37:51.769] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:37:51.769] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  808097.38 ms
[2024-09-30 19:37:51.769] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.77 ms /    28 runs   (    0.21 ms per token,  4851.85 tokens per second)
[2024-09-30 19:37:51.769] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3085.21 ms /    24 tokens (  128.55 ms per token,     7.78 tokens per second)
[2024-09-30 19:37:51.769] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4943.59 ms /    27 runs   (  183.10 ms per token,     5.46 tokens per second)
[2024-09-30 19:37:51.769] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  813047.68 ms /    51 tokens
[2024-09-30 19:37:51.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:37:51.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 120
[2024-09-30 19:37:51.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft to help users with various queries and tasks. How can I assist you today?<|end|>
[2024-09-30 19:37:51.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:37:51.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft to help users with various queries and tasks. How can I assist you today?
[2024-09-30 19:37:51.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:37:51.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:37:51.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:37:51.772] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:37:51.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:37:51.772] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:37:51.773] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:37:51.773] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:37:51.773] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:37:51.773] [info] rag_api_server in src/main.rs:517: response_body_size: 424
[2024-09-30 19:37:51.773] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:37:51.773] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:38:41.799] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:49404, local_addr: 0.0.0.0:8081
[2024-09-30 19:38:41.799] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:38:41.799] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:38:41.800] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:38:41.800] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:38:41.800] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-03641dc5-6afc-4225-99f5-5d1e468661a7
[2024-09-30 19:38:41.800] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:38:41.800] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:38:41.800] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:38:41.800] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:38:41.800] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:38:41.800] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:38:41.800] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:38:41.800] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:38:41.800] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:38:41.800] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:38:41.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:38:41.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:38:41.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:38:41.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:38:41.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:38:41.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:38:41.803] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:38:41.803] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:38:41.803] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:38:41.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:38:41.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:38:41.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:38:41.806] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:38:41.806] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:38:41.806] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:38:41.806] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:38:41.806] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:38:41.806] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:38:41.806] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:38:41.817] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:38:41.817] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:38:41.817] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:38:41.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:38:41.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:38:41.819] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:38:41.857] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:38:41.857] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  863036.86 ms
[2024-09-30 19:38:41.857] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:38:41.857] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      36.82 ms /     6 tokens (    6.14 ms per token,   162.94 tokens per second)
[2024-09-30 19:38:41.857] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:38:41.858] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  863036.50 ms /     7 tokens
[2024-09-30 19:38:41.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:38:41.858] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:38:41.864] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:38:41.864] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:38:41.864] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:38:41.864] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:38:41.864] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:38:41.864] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:38:41.864] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:38:41.864] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:38:41.864] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:38:41.864] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:38:41.864] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:38:41.886] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:38:41.886] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:38:41.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:38:41.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:38:41.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:38:41.886] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:38:41.886] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:38:41.886] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:38:41.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-03641dc5-6afc-4225-99f5-5d1e468661a7
[2024-09-30 19:38:41.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:38:41.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:38:41.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:38:41.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:38:41.887] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:38:41.887] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:38:41.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:38:41.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:38:41.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:38:41.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:38:41.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:38:41.887] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:38:42.800] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:38:42.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:38:42.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:38:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:38:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:38:42.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:38:42.808] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:38:42.808] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:38:42.808] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:38:42.808] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:38:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:38:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:38:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:38:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:38:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:38:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:38:42.809] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:38:42.809] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:38:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:38:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:38:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:38:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:38:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:38:42.809] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:38:43.192] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:38:43.192] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:38:43.192] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:38:43.193] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:38:43.193] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:38:43.193] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:38:43.196] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:38:43.196] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:38:43.196] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:38:43.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:38:43.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:38:43.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:38:43.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:38:43.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:38:43.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:38:43.381] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:38:43.381] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:38:43.381] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:38:43.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:38:43.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:38:43.382] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:38:53.235] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:38:53.235] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:38:53.235] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  867755.30 ms
[2024-09-30 19:38:53.235] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       7.97 ms /    37 runs   (    0.22 ms per token,  4642.99 tokens per second)
[2024-09-30 19:38:53.235] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3093.41 ms /    24 tokens (  128.89 ms per token,     7.76 tokens per second)
[2024-09-30 19:38:53.235] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    6748.98 ms /    36 runs   (  187.47 ms per token,     5.33 tokens per second)
[2024-09-30 19:38:53.235] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  874513.68 ms /    60 tokens
[2024-09-30 19:38:53.237] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:38:53.237] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 210
[2024-09-30 19:38:53.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am an advanced artificial intelligence assistant designed to interact with users and provide information, assistance, or companionship. My purpose is to be helpful and friendly in my responses.<|end|>
[2024-09-30 19:38:53.237] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:38:53.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am an advanced artificial intelligence assistant designed to interact with users and provide information, assistance, or companionship. My purpose is to be helpful and friendly in my responses.
[2024-09-30 19:38:53.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:38:53.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:38:53.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:38:53.238] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 37
[2024-09-30 19:38:53.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 37
[2024-09-30 19:38:53.238] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:38:53.238] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:38:53.238] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:38:53.238] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:38:53.238] [info] rag_api_server in src/main.rs:517: response_body_size: 514
[2024-09-30 19:38:53.238] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:38:53.238] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:39:43.262] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:39124, local_addr: 0.0.0.0:8081
[2024-09-30 19:39:43.263] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:39:43.263] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:39:43.263] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:39:43.263] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:39:43.263] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-7f8d2450-cedb-4145-b4f8-71dee5ea2834
[2024-09-30 19:39:43.263] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:39:43.263] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:39:43.263] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:39:43.263] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:39:43.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:39:43.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:39:43.263] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:39:43.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:39:43.263] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:39:43.263] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:39:43.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:39:43.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:39:43.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:39:43.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:39:43.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:39:43.263] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:39:43.266] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:39:43.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:39:43.266] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:39:43.267] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:39:43.267] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:39:43.267] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:39:43.268] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:39:43.268] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:39:43.268] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:39:43.268] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:39:43.268] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:39:43.268] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:39:43.268] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:39:43.273] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:39:43.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:39:43.273] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:39:43.275] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:39:43.275] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:39:43.275] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:39:43.307] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:39:43.307] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  924486.33 ms
[2024-09-30 19:39:43.307] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:39:43.307] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      29.80 ms /     6 tokens (    4.97 ms per token,   201.31 tokens per second)
[2024-09-30 19:39:43.307] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:39:43.307] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  924485.50 ms /     7 tokens
[2024-09-30 19:39:43.307] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:39:43.308] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:39:43.313] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:39:43.313] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:39:43.313] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:39:43.313] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:39:43.313] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:39:43.313] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:39:43.314] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:39:43.314] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:39:43.314] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:39:43.314] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:39:43.314] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:39:43.333] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:39:43.333] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:39:43.333] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:39:43.333] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-7f8d2450-cedb-4145-b4f8-71dee5ea2834
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:39:43.333] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:39:43.333] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:39:43.333] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:39:43.333] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:39:43.333] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:39:43.333] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:39:43.333] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:39:43.333] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:39:43.956] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:39:43.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:39:43.956] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:39:43.961] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:39:43.961] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:39:43.961] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:39:43.965] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:39:43.965] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:39:43.965] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:39:43.965] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 37
[2024-09-30 19:39:43.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:39:43.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:39:43.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:39:43.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:39:43.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:39:43.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:39:43.965] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:39:43.965] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:39:43.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:39:43.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:39:43.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:39:43.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:39:43.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:39:43.965] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:39:44.373] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:39:44.373] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:39:44.373] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:39:44.374] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:39:44.374] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:39:44.374] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:39:44.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:39:44.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:39:44.379] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:39:44.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:39:44.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:39:44.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:39:44.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:39:44.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:39:44.379] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:39:44.666] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:39:44.666] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:39:44.666] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:39:44.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:39:44.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:39:44.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:39:53.763] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:39:53.763] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:39:53.763] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  929435.44 ms
[2024-09-30 19:39:53.763] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.54 ms /    30 runs   (    0.22 ms per token,  4587.86 tokens per second)
[2024-09-30 19:39:53.763] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3488.57 ms /    24 tokens (  145.36 ms per token,     6.88 tokens per second)
[2024-09-30 19:39:53.763] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    5598.42 ms /    29 runs   (  193.05 ms per token,     5.18 tokens per second)
[2024-09-30 19:39:53.763] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  935041.68 ms /    53 tokens
[2024-09-30 19:39:53.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:39:53.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 154
[2024-09-30 19:39:53.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence assistant designed to engage in conversations and provide information or support. How may I assist you today?<|end|>
[2024-09-30 19:39:53.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:39:53.766] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence assistant designed to engage in conversations and provide information or support. How may I assist you today?
[2024-09-30 19:39:53.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:39:53.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:39:53.766] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:39:53.767] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 30
[2024-09-30 19:39:53.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 30
[2024-09-30 19:39:53.767] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:39:53.767] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:39:53.767] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:39:53.768] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:39:53.768] [info] rag_api_server in src/main.rs:517: response_body_size: 458
[2024-09-30 19:39:53.768] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:39:53.768] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:40:43.794] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:50902, local_addr: 0.0.0.0:8081
[2024-09-30 19:40:43.795] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:40:43.795] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:40:43.795] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:40:43.795] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:40:43.795] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-5f4ceec3-3a14-402e-a5ce-2f652b3fd413
[2024-09-30 19:40:43.795] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:40:43.795] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:40:43.795] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:40:43.796] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:40:43.796] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:40:43.796] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:40:43.796] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:40:43.796] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:40:43.796] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:40:43.796] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:40:43.796] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:40:43.796] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:40:43.796] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:40:43.796] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:40:43.796] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:40:43.796] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:40:43.798] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:40:43.798] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:40:43.798] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:40:43.800] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:40:43.800] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:40:43.804] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:40:43.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:40:43.804] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:40:43.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:40:43.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:40:43.805] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:40:43.834] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:40:43.834] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  985013.01 ms
[2024-09-30 19:40:43.834] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:40:43.834] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      27.40 ms /     6 tokens (    4.57 ms per token,   218.96 tokens per second)
[2024-09-30 19:40:43.834] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:40:43.834] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  985012.50 ms /     7 tokens
[2024-09-30 19:40:43.834] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:40:43.834] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:40:43.839] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:40:43.839] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:40:43.839] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:40:43.839] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:40:43.839] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:40:43.839] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:40:43.839] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:40:43.839] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:40:43.839] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:40:43.839] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:40:43.839] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:40:43.858] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:40:43.858] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:40:43.858] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:40:43.858] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-5f4ceec3-3a14-402e-a5ce-2f652b3fd413
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:40:43.858] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:40:43.858] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:40:43.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:40:43.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:40:43.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:40:43.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:40:43.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:40:43.858] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:40:44.401] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:40:44.401] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:40:44.401] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:40:44.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:40:44.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:40:44.404] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:40:44.406] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:40:44.406] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:40:44.406] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:40:44.407] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 30
[2024-09-30 19:40:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:40:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:40:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:40:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:40:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:40:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:40:44.407] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:40:44.407] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:40:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:40:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:40:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:40:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:40:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:40:44.407] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:40:44.608] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:40:44.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:40:44.608] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:40:44.609] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:40:44.609] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:40:44.609] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:40:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:40:44.613] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:40:44.613] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:40:44.613] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:40:44.613] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:40:44.613] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:40:44.613] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:40:44.613] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:40:44.613] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:40:44.861] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:40:44.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:40:44.861] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:40:44.862] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:40:44.862] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:40:44.862] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:40:50.784] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:40:50.784] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:40:50.784] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time =  988984.01 ms
[2024-09-30 19:40:50.784] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       3.87 ms /    20 runs   (    0.19 ms per token,  5169.29 tokens per second)
[2024-09-30 19:40:50.784] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2842.01 ms /    24 tokens (  118.42 ms per token,     8.44 tokens per second)
[2024-09-30 19:40:50.784] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3074.66 ms /    19 runs   (  161.82 ms per token,     6.18 tokens per second)
[2024-09-30 19:40:50.784] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time =  992063.68 ms /    43 tokens
[2024-09-30 19:40:50.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:40:50.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 19:40:50.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 19:40:50.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:40:50.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 19:40:50.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:40:50.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:40:50.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:40:50.787] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:40:50.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:40:50.787] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:40:50.787] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:40:50.787] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:40:50.788] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:40:50.788] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 19:40:50.788] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:40:50.788] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:41:40.814] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:53696, local_addr: 0.0.0.0:8081
[2024-09-30 19:41:40.815] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:41:40.815] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:41:40.815] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:41:40.815] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:41:40.816] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-2377109e-0760-44ef-978b-b44482d2442e
[2024-09-30 19:41:40.816] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:41:40.816] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:41:40.816] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:41:40.816] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:41:40.816] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:41:40.816] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:41:40.816] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:41:40.816] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:41:40.816] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:41:40.816] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:41:40.816] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:41:40.816] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:41:40.816] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:41:40.816] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:41:40.817] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:41:40.817] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:41:40.821] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:41:40.821] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:41:40.821] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:41:40.824] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:41:40.824] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:41:40.838] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:41:40.838] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:41:40.838] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:41:40.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:41:40.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:41:40.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:41:40.900] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:41:40.900] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1042079.50 ms
[2024-09-30 19:41:40.900] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:41:40.900] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      57.25 ms /     6 tokens (    9.54 ms per token,   104.80 tokens per second)
[2024-09-30 19:41:40.900] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:41:40.900] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1042079.50 ms /     7 tokens
[2024-09-30 19:41:40.901] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:41:40.901] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:41:40.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:41:40.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:41:40.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:41:40.907] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:41:40.907] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:41:40.907] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:41:40.907] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:41:40.907] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:41:40.907] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:41:40.907] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:41:40.907] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:41:40.925] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:41:40.925] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:41:40.925] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:41:40.925] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-2377109e-0760-44ef-978b-b44482d2442e
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:41:40.925] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:41:40.925] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:41:40.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:41:40.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:41:40.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:41:40.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:41:40.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:41:40.925] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:41:42.173] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:41:42.173] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:41:42.173] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:41:42.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:41:42.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:41:42.176] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:41:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:41:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:41:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:41:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:41:42.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:41:42.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:41:42.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:41:42.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:41:42.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:41:42.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:41:42.179] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:41:42.179] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:41:42.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:41:42.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:41:42.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:41:42.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:41:42.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:41:42.179] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:41:42.383] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:41:42.383] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:41:42.383] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:41:42.384] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:41:42.384] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:41:42.384] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:41:42.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:41:42.388] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:41:42.388] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:41:42.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:41:42.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:41:42.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:41:42.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:41:42.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:41:42.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:41:42.633] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:41:42.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:41:42.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:41:42.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:41:42.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:41:42.634] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:41:51.503] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:41:51.503] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:41:51.503] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1047339.17 ms
[2024-09-30 19:41:51.503] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       6.65 ms /    32 runs   (    0.21 ms per token,  4814.93 tokens per second)
[2024-09-30 19:41:51.503] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3425.08 ms /    24 tokens (  142.71 ms per token,     7.01 tokens per second)
[2024-09-30 19:41:51.503] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    5434.62 ms /    31 runs   (  175.31 ms per token,     5.70 tokens per second)
[2024-09-30 19:41:51.503] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1052781.68 ms /    55 tokens
[2024-09-30 19:41:51.505] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:41:51.505] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 150
[2024-09-30 19:41:51.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, an AI developed by Microsoft. My purpose is to assist and provide information on a wide range of topics through conversation.<|end|>
[2024-09-30 19:41:51.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:41:51.505] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, an AI developed by Microsoft. My purpose is to assist and provide information on a wide range of topics through conversation.
[2024-09-30 19:41:51.505] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:41:51.505] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:41:51.505] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:41:51.505] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 32
[2024-09-30 19:41:51.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 32
[2024-09-30 19:41:51.506] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:41:51.506] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:41:51.506] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:41:51.506] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:41:51.506] [info] rag_api_server in src/main.rs:517: response_body_size: 454
[2024-09-30 19:41:51.506] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:41:51.506] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:42:41.533] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:50740, local_addr: 0.0.0.0:8081
[2024-09-30 19:42:41.534] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:42:41.534] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:42:41.534] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:42:41.534] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:42:41.534] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-fea167f7-199b-4bca-911a-bb183d2d2b19
[2024-09-30 19:42:41.534] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:42:41.534] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:42:41.534] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:42:41.534] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:42:41.534] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:42:41.535] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:42:41.535] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:42:41.535] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:42:41.535] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:42:41.535] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:42:41.535] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:42:41.535] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:42:41.535] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:42:41.535] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:42:41.535] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:42:41.535] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:42:41.537] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:42:41.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:42:41.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:42:41.539] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:42:41.539] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:42:41.539] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:42:41.540] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:42:41.540] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:42:41.540] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:42:41.540] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:42:41.540] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:42:41.540] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:42:41.540] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:42:41.544] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:42:41.544] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:42:41.544] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:42:41.546] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:42:41.546] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:42:41.546] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:42:41.577] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:42:41.577] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1102756.78 ms
[2024-09-30 19:42:41.577] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:42:41.577] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      30.48 ms /     6 tokens (    5.08 ms per token,   196.86 tokens per second)
[2024-09-30 19:42:41.577] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:42:41.577] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1102756.50 ms /     7 tokens
[2024-09-30 19:42:41.578] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:42:41.578] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:42:41.585] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:42:41.585] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:42:41.585] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:42:41.585] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:42:41.585] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:42:41.585] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:42:41.585] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:42:41.585] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:42:41.585] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:42:41.586] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:42:41.586] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:42:41.604] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:42:41.604] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:42:41.604] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:42:41.604] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-fea167f7-199b-4bca-911a-bb183d2d2b19
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:42:41.604] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:42:41.604] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:42:41.604] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:42:41.604] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:42:41.604] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:42:41.604] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:42:41.604] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:42:41.604] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:42:42.134] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:42:42.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:42:42.134] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:42:42.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:42:42.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:42:42.137] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:42:42.142] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:42:42.142] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:42:42.142] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:42:42.143] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 32
[2024-09-30 19:42:42.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:42:42.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:42:42.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:42:42.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:42:42.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:42:42.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:42:42.143] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:42:42.144] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:42:42.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:42:42.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:42:42.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:42:42.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:42:42.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:42:42.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:42:42.375] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:42:42.375] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:42:42.375] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:42:42.376] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:42:42.376] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:42:42.376] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:42:42.379] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:42:42.380] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:42:42.380] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:42:42.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:42:42.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:42:42.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:42:42.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:42:42.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:42:42.380] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:42:42.590] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:42:42.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:42:42.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:42:42.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:42:42.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:42:42.591] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:42:49.651] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:42:49.651] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:42:49.651] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1106963.09 ms
[2024-09-30 19:42:49.651] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.01 ms /    25 runs   (    0.20 ms per token,  4991.02 tokens per second)
[2024-09-30 19:42:49.651] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3091.99 ms /    24 tokens (  128.83 ms per token,     7.76 tokens per second)
[2024-09-30 19:42:49.651] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3961.33 ms /    24 runs   (  165.06 ms per token,     6.06 tokens per second)
[2024-09-30 19:42:49.651] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1110930.68 ms /    48 tokens
[2024-09-30 19:42:49.654] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:42:49.654] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 113
[2024-09-30 19:42:49.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I'm Phi, an artificial intelligence designed to interact with and assist users. How may I help you today?<|end|>
[2024-09-30 19:42:49.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:42:49.654] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I'm Phi, an artificial intelligence designed to interact with and assist users. How may I help you today?
[2024-09-30 19:42:49.654] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:42:49.654] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:42:49.654] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:42:49.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 19:42:49.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 25
[2024-09-30 19:42:49.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:42:49.655] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:42:49.655] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:42:49.655] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:42:49.655] [info] rag_api_server in src/main.rs:517: response_body_size: 417
[2024-09-30 19:42:49.655] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:42:49.655] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:43:39.677] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:38714, local_addr: 0.0.0.0:8081
[2024-09-30 19:43:39.678] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:43:39.678] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:43:39.678] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:43:39.678] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:43:39.678] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-a066ea67-f699-4b1f-aae2-cf8b8667f308
[2024-09-30 19:43:39.678] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:43:39.679] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:43:39.679] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:43:39.679] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:43:39.679] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:43:39.679] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:43:39.679] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:43:39.679] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:43:39.679] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:43:39.679] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:43:39.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:43:39.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:43:39.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:43:39.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:43:39.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:43:39.679] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:43:39.681] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:43:39.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:43:39.681] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:43:39.683] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:43:39.683] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:43:39.688] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:43:39.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:43:39.688] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:43:39.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:43:39.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:43:39.690] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:43:39.724] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:43:39.724] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1160903.54 ms
[2024-09-30 19:43:39.724] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:43:39.724] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      32.83 ms /     6 tokens (    5.47 ms per token,   182.78 tokens per second)
[2024-09-30 19:43:39.724] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:43:39.724] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1160903.50 ms /     7 tokens
[2024-09-30 19:43:39.725] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:43:39.725] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:43:39.730] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:43:39.730] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:43:39.730] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:43:39.730] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:43:39.730] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:43:39.730] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:43:39.731] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:43:39.731] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:43:39.731] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:43:39.731] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:43:39.731] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:43:39.750] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:43:39.750] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:43:39.750] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:43:39.750] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-a066ea67-f699-4b1f-aae2-cf8b8667f308
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:43:39.750] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:43:39.751] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:43:39.751] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:43:39.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:43:39.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:43:39.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:43:39.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:43:39.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:43:39.751] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:43:40.405] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:43:40.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:43:40.405] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:43:40.408] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:43:40.408] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:43:40.408] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:43:40.412] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:43:40.412] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:43:40.412] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:43:40.412] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 19:43:40.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:43:40.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:43:40.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:43:40.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:43:40.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:43:40.412] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:43:40.413] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:43:40.413] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:43:40.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:43:40.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:43:40.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:43:40.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:43:40.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:43:40.413] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:43:40.614] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:43:40.614] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:43:40.614] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:43:40.615] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:43:40.615] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:43:40.615] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:43:40.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:43:40.618] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:43:40.618] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:43:40.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:43:40.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:43:40.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:43:40.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:43:40.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:43:40.618] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:43:40.841] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:43:40.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:43:40.841] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:43:40.842] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:43:40.842] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:43:40.842] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:43:47.652] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:43:47.652] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:43:47.652] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1165402.35 ms
[2024-09-30 19:43:47.652] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.16 ms /    20 runs   (    0.21 ms per token,  4803.07 tokens per second)
[2024-09-30 19:43:47.652] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3280.77 ms /    24 tokens (  136.70 ms per token,     7.32 tokens per second)
[2024-09-30 19:43:47.652] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3524.29 ms /    19 runs   (  185.49 ms per token,     5.39 tokens per second)
[2024-09-30 19:43:47.652] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1168931.68 ms /    43 tokens
[2024-09-30 19:43:47.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:43:47.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 19:43:47.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 19:43:47.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:43:47.655] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 19:43:47.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:43:47.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:43:47.655] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:43:47.656] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:43:47.656] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:43:47.656] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:43:47.656] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:43:47.656] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:43:47.656] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:43:47.656] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 19:43:47.656] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:43:47.656] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:44:37.684] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:53348, local_addr: 0.0.0.0:8081
[2024-09-30 19:44:37.684] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:44:37.684] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:44:37.685] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:44:37.685] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:44:37.685] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-f3a658ba-e5eb-458e-a64a-51c9e315bd7a
[2024-09-30 19:44:37.685] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:44:37.685] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:44:37.685] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:44:37.685] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:44:37.685] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:44:37.685] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:44:37.685] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:44:37.685] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:44:37.685] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:44:37.685] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:44:37.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:44:37.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:44:37.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:44:37.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:44:37.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:44:37.685] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:44:37.689] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:44:37.689] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:44:37.689] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:44:37.691] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:44:37.691] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:44:37.691] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:44:37.692] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:44:37.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:44:37.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:44:37.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:44:37.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:44:37.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:44:37.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:44:37.696] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:44:37.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:44:37.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:44:37.698] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:44:37.698] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:44:37.698] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:44:37.723] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:44:37.723] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1218902.18 ms
[2024-09-30 19:44:37.723] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:44:37.723] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      24.48 ms /     6 tokens (    4.08 ms per token,   245.08 tokens per second)
[2024-09-30 19:44:37.723] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:44:37.723] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1218901.50 ms /     7 tokens
[2024-09-30 19:44:37.723] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:44:37.723] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:44:37.728] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:44:37.728] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:44:37.728] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:44:37.728] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:44:37.728] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:44:37.728] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:44:37.728] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:44:37.728] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:44:37.728] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:44:37.729] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:44:37.729] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:44:37.745] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:44:37.745] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:44:37.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:44:37.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:44:37.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:44:37.745] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:44:37.745] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:44:37.746] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:44:37.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-f3a658ba-e5eb-458e-a64a-51c9e315bd7a
[2024-09-30 19:44:37.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:44:37.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:44:37.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:44:37.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:44:37.746] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:44:37.746] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:44:37.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:44:37.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:44:37.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:44:37.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:44:37.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:44:37.746] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:44:38.196] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:44:38.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:44:38.196] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:44:38.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:44:38.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:44:38.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:44:38.203] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:44:38.203] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:44:38.203] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:44:38.203] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:44:38.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:44:38.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:44:38.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:44:38.203] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:44:38.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:44:38.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:44:38.204] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:44:38.204] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:44:38.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:44:38.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:44:38.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:44:38.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:44:38.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:44:38.204] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:44:38.388] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:44:38.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:44:38.388] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:44:38.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:44:38.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:44:38.389] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:44:38.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:44:38.392] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:44:38.392] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:44:38.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:44:38.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:44:38.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:44:38.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:44:38.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:44:38.392] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:44:38.575] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:44:38.575] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:44:38.575] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:44:38.576] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:44:38.576] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:44:38.576] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:44:52.103] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:44:52.103] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:44:52.103] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1222751.74 ms
[2024-09-30 19:44:52.103] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =      11.54 ms /    52 runs   (    0.22 ms per token,  4506.46 tokens per second)
[2024-09-30 19:44:52.103] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2895.77 ms /    24 tokens (  120.66 ms per token,     8.29 tokens per second)
[2024-09-30 19:44:52.103] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =   10616.32 ms /    51 runs   (  208.16 ms per token,     4.80 tokens per second)
[2024-09-30 19:44:52.103] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1233382.68 ms /    75 tokens
[2024-09-30 19:44:52.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:44:52.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 238
[2024-09-30 19:44:52.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, a sophisticated AI developed by Microsoft. My purpose is to assist and provide information across various topics while ensuring your queries are handled with care and respect for privacy. How can I help you today?<|end|>
[2024-09-30 19:44:52.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:44:52.106] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, a sophisticated AI developed by Microsoft. My purpose is to assist and provide information across various topics while ensuring your queries are handled with care and respect for privacy. How can I help you today?
[2024-09-30 19:44:52.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:44:52.107] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:44:52.107] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:44:52.107] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 52
[2024-09-30 19:44:52.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 52
[2024-09-30 19:44:52.107] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:44:52.107] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:44:52.107] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:44:52.107] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:44:52.107] [info] rag_api_server in src/main.rs:517: response_body_size: 542
[2024-09-30 19:44:52.107] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:44:52.107] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:45:42.136] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47570, local_addr: 0.0.0.0:8081
[2024-09-30 19:45:42.137] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:45:42.137] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:45:42.137] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:45:42.137] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:45:42.137] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-7c041454-192d-434b-a862-4c36ffeec527
[2024-09-30 19:45:42.137] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:45:42.137] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:45:42.137] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:45:42.137] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:45:42.137] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:45:42.137] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:45:42.137] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:45:42.137] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:45:42.137] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:45:42.137] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:45:42.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:45:42.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:45:42.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:45:42.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:45:42.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:45:42.138] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:45:42.140] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:45:42.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:45:42.140] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:45:42.142] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:45:42.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:45:42.146] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:45:42.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:45:42.146] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:45:42.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:45:42.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:45:42.148] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:45:42.173] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:45:42.173] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1283352.55 ms
[2024-09-30 19:45:42.173] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:45:42.173] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      23.94 ms /     6 tokens (    3.99 ms per token,   250.63 tokens per second)
[2024-09-30 19:45:42.173] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:45:42.173] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1283352.50 ms /     7 tokens
[2024-09-30 19:45:42.174] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:45:42.174] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:45:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:45:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:45:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:45:42.179] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:45:42.180] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:45:42.180] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:45:42.180] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:45:42.180] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:45:42.180] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:45:42.180] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:45:42.180] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:45:42.198] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:45:42.198] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:45:42.198] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:45:42.198] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-7c041454-192d-434b-a862-4c36ffeec527
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:45:42.198] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:45:42.199] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:45:42.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:45:42.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:45:42.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:45:42.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:45:42.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:45:42.199] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:45:42.665] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:45:42.665] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:45:42.665] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:45:42.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:45:42.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:45:42.668] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:45:42.671] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:45:42.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:45:42.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:45:42.672] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 52
[2024-09-30 19:45:42.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:45:42.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:45:42.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:45:42.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:45:42.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:45:42.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:45:42.672] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:45:42.672] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:45:42.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:45:42.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:45:42.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:45:42.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:45:42.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:45:42.672] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:45:42.844] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:45:42.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:45:42.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:45:42.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:45:42.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:45:42.845] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:45:42.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:45:42.849] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:45:42.849] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:45:42.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:45:42.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:45:42.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:45:42.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:45:42.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:45:42.849] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:45:43.021] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:45:43.021] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:45:43.021] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:45:43.023] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:45:43.023] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:45:43.023] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:45:48.886] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:45:48.886] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:45:48.886] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1287080.80 ms
[2024-09-30 19:45:48.886] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.01 ms /    20 runs   (    0.20 ms per token,  4983.80 tokens per second)
[2024-09-30 19:45:48.886] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2778.57 ms /    24 tokens (  115.77 ms per token,     8.64 tokens per second)
[2024-09-30 19:45:48.886] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3079.85 ms /    19 runs   (  162.10 ms per token,     6.17 tokens per second)
[2024-09-30 19:45:48.886] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1290165.68 ms /    43 tokens
[2024-09-30 19:45:48.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:45:48.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 73
[2024-09-30 19:45:48.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I help you today?<|end|>
[2024-09-30 19:45:48.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:45:48.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I help you today?
[2024-09-30 19:45:48.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:45:48.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:45:48.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:45:48.889] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:45:48.889] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:45:48.890] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:45:48.890] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:45:48.890] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:45:48.890] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:45:48.890] [info] rag_api_server in src/main.rs:517: response_body_size: 377
[2024-09-30 19:45:48.890] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:45:48.890] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:46:38.916] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:58206, local_addr: 0.0.0.0:8081
[2024-09-30 19:46:38.916] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:46:38.916] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:46:38.916] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:46:38.917] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:46:38.917] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-fc1aacc0-1984-4407-82c3-20d813507c1e
[2024-09-30 19:46:38.917] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:46:38.917] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:46:38.917] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:46:38.917] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:46:38.917] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:46:38.917] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:46:38.917] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:46:38.917] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:46:38.917] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:46:38.917] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:46:38.917] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:46:38.917] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:46:38.917] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:46:38.917] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:46:38.917] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:46:38.917] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:46:38.920] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:46:38.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:46:38.920] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:46:38.923] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:46:38.923] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:46:38.923] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:46:38.924] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:46:38.924] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:46:38.924] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:46:38.924] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:46:38.924] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:46:38.924] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:46:38.924] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:46:38.930] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:46:38.930] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:46:38.930] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:46:38.932] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:46:38.932] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:46:38.932] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:46:38.964] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:46:38.964] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1340143.33 ms
[2024-09-30 19:46:38.964] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:46:38.964] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      30.55 ms /     6 tokens (    5.09 ms per token,   196.37 tokens per second)
[2024-09-30 19:46:38.964] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:46:38.964] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1340142.50 ms /     7 tokens
[2024-09-30 19:46:38.964] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:46:38.964] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:46:38.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:46:38.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:46:38.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:46:38.970] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:46:38.970] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:46:38.970] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:46:38.970] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:46:38.970] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:46:38.970] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:46:38.971] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:46:38.971] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:46:38.989] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:46:38.989] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:46:38.990] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:46:38.990] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-fc1aacc0-1984-4407-82c3-20d813507c1e
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:46:38.990] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:46:38.991] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:46:38.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:46:38.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:46:38.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:46:38.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:46:38.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:46:38.991] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:46:39.569] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:46:39.569] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:46:39.569] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:46:39.572] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:46:39.572] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:46:39.572] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:46:39.577] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:46:39.577] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:46:39.577] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:46:39.577] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:46:39.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:46:39.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:46:39.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:46:39.577] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:46:39.578] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:46:39.578] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:46:39.578] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:46:39.578] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:46:39.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:46:39.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:46:39.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:46:39.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:46:39.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:46:39.578] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:46:39.767] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:46:39.767] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:46:39.767] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:46:39.767] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:46:39.767] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:46:39.767] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:46:39.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:46:39.770] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:46:39.770] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:46:39.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:46:39.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:46:39.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:46:39.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:46:39.770] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:46:39.771] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:46:39.967] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:46:39.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:46:39.967] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:46:39.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:46:39.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:46:39.968] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:46:49.464] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:46:49.464] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:46:49.464] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1344409.84 ms
[2024-09-30 19:46:49.464] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       7.86 ms /    33 runs   (    0.24 ms per token,  4197.94 tokens per second)
[2024-09-30 19:46:49.464] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3161.98 ms /    24 tokens (  131.75 ms per token,     7.59 tokens per second)
[2024-09-30 19:46:49.464] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    6323.90 ms /    32 runs   (  197.62 ms per token,     5.06 tokens per second)
[2024-09-30 19:46:49.464] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1350743.68 ms /    56 tokens
[2024-09-30 19:46:49.468] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:46:49.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 117
[2024-09-30 19:46:49.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I'm Phi, an AI designed to help you with a variety of tasks and inquiries. How can I assist you today?<|end|>
[2024-09-30 19:46:49.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:46:49.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I'm Phi, an AI designed to help you with a variety of tasks and inquiries. How can I assist you today?
[2024-09-30 19:46:49.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:46:49.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:46:49.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:46:49.469] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 33
[2024-09-30 19:46:49.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 33
[2024-09-30 19:46:49.469] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:46:49.470] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:46:49.470] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:46:49.470] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:46:49.470] [info] rag_api_server in src/main.rs:517: response_body_size: 421
[2024-09-30 19:46:49.470] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:46:49.470] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:47:39.499] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:39920, local_addr: 0.0.0.0:8081
[2024-09-30 19:47:39.499] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:47:39.499] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:47:39.499] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:47:39.499] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:47:39.500] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-0cbedf14-d145-4323-9c71-f2b0006bf592
[2024-09-30 19:47:39.500] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:47:39.500] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:47:39.500] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:47:39.500] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:47:39.500] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:47:39.500] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:47:39.500] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:47:39.500] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:47:39.500] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:47:39.500] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:47:39.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:47:39.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:47:39.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:47:39.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:47:39.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:47:39.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:47:39.502] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:47:39.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:47:39.502] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:47:39.504] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:47:39.504] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:47:39.511] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:47:39.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:47:39.511] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:47:39.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:47:39.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:47:39.513] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:47:39.559] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:47:39.559] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1400738.67 ms
[2024-09-30 19:47:39.559] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:47:39.559] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      43.92 ms /     6 tokens (    7.32 ms per token,   136.60 tokens per second)
[2024-09-30 19:47:39.559] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:47:39.559] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1400738.50 ms /     7 tokens
[2024-09-30 19:47:39.560] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:47:39.560] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:47:39.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:47:39.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:47:39.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:47:39.568] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:47:39.569] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:47:39.569] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:47:39.569] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:47:39.569] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:47:39.569] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:47:39.569] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:47:39.569] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:47:39.588] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:47:39.589] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:47:39.589] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:47:39.589] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-0cbedf14-d145-4323-9c71-f2b0006bf592
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:47:39.589] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:47:39.590] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:47:39.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:47:39.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:47:39.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:47:39.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:47:39.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:47:39.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:47:40.144] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:47:40.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:47:40.144] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:47:40.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:47:40.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:47:40.150] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:47:40.153] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:47:40.153] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:47:40.153] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:47:40.153] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 33
[2024-09-30 19:47:40.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:47:40.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:47:40.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:47:40.153] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:47:40.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:47:40.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:47:40.154] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:47:40.154] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:47:40.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:47:40.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:47:40.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:47:40.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:47:40.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:47:40.154] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:47:40.357] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:47:40.357] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:47:40.357] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:47:40.358] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:47:40.358] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:47:40.358] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:47:40.362] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:47:40.362] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:47:40.362] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:47:40.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:47:40.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:47:40.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:47:40.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:47:40.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:47:40.362] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:47:40.602] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:47:40.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:47:40.602] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:47:40.603] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:47:40.603] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:47:40.603] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:47:47.479] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:47:47.479] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:47:47.479] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1405225.21 ms
[2024-09-30 19:47:47.479] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       3.83 ms /    20 runs   (    0.19 ms per token,  5221.93 tokens per second)
[2024-09-30 19:47:47.479] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3342.58 ms /    24 tokens (  139.27 ms per token,     7.18 tokens per second)
[2024-09-30 19:47:47.479] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3528.16 ms /    19 runs   (  185.69 ms per token,     5.39 tokens per second)
[2024-09-30 19:47:47.479] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1408757.68 ms /    43 tokens
[2024-09-30 19:47:47.481] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:47:47.481] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 19:47:47.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 19:47:47.481] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:47:47.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 19:47:47.482] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:47:47.482] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:47:47.482] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:47:47.482] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:47:47.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:47:47.482] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:47:47.483] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:47:47.483] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:47:47.483] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:47:47.483] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 19:47:47.483] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:47:47.483] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:48:37.510] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:35812, local_addr: 0.0.0.0:8081
[2024-09-30 19:48:37.511] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:48:37.511] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:48:37.511] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:48:37.511] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:48:37.511] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-34a10536-8135-4917-b66d-13bf89d20ffb
[2024-09-30 19:48:37.511] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:48:37.511] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:48:37.511] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:48:37.511] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:48:37.512] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:48:37.512] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:48:37.512] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:48:37.512] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:48:37.512] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:48:37.512] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:48:37.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:48:37.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:48:37.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:48:37.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:48:37.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:48:37.512] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:48:37.515] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:48:37.515] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:48:37.515] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:48:37.518] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:48:37.518] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:48:37.523] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:48:37.523] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:48:37.523] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:48:37.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:48:37.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:48:37.525] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:48:37.556] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:48:37.556] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1458735.57 ms
[2024-09-30 19:48:37.556] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:48:37.556] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      29.74 ms /     6 tokens (    4.96 ms per token,   201.76 tokens per second)
[2024-09-30 19:48:37.556] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:48:37.556] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1458735.50 ms /     7 tokens
[2024-09-30 19:48:37.557] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:48:37.557] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:48:37.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:48:37.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:48:37.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:48:37.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:48:37.564] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:48:37.564] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:48:37.564] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:48:37.564] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:48:37.564] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:48:37.564] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:48:37.564] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:48:37.581] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:48:37.581] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:48:37.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:48:37.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:48:37.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:48:37.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:48:37.581] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:48:37.581] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:48:37.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-34a10536-8135-4917-b66d-13bf89d20ffb
[2024-09-30 19:48:37.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:48:37.581] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:48:37.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:48:37.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:48:37.582] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:48:37.582] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:48:37.582] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:48:37.582] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:48:37.582] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:48:37.582] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:48:37.582] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:48:37.582] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:48:38.125] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:48:38.125] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:48:38.125] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:48:38.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:48:38.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:48:38.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:48:38.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:48:38.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:48:38.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:48:38.134] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:48:38.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:48:38.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:48:38.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:48:38.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:48:38.134] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:48:38.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:48:38.135] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:48:38.135] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:48:38.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:48:38.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:48:38.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:48:38.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:48:38.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:48:38.135] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:48:38.340] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:48:38.340] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:48:38.340] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:48:38.341] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:48:38.341] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:48:38.341] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:48:38.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:48:38.345] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:48:38.345] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:48:38.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:48:38.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:48:38.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:48:38.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:48:38.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:48:38.345] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:48:38.564] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:48:38.564] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:48:38.564] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:48:38.565] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:48:38.565] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:48:38.565] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:48:44.280] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:48:44.280] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:48:44.280] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1463197.57 ms
[2024-09-30 19:48:44.280] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       2.42 ms /    13 runs   (    0.19 ms per token,  5371.90 tokens per second)
[2024-09-30 19:48:44.280] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3353.01 ms /    24 tokens (  139.71 ms per token,     7.16 tokens per second)
[2024-09-30 19:48:44.280] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2358.84 ms /    12 runs   (  196.57 ms per token,     5.09 tokens per second)
[2024-09-30 19:48:44.280] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1465558.68 ms /    36 tokens
[2024-09-30 19:48:44.283] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:48:44.283] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 47
[2024-09-30 19:48:44.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft.<|end|>
[2024-09-30 19:48:44.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:48:44.283] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft.
[2024-09-30 19:48:44.283] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:48:44.283] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:48:44.283] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:48:44.283] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 19:48:44.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 13
[2024-09-30 19:48:44.284] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:48:44.284] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:48:44.284] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:48:44.284] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:48:44.284] [info] rag_api_server in src/main.rs:517: response_body_size: 351
[2024-09-30 19:48:44.284] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:48:44.284] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:49:34.311] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:60932, local_addr: 0.0.0.0:8081
[2024-09-30 19:49:34.312] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:49:34.312] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:49:34.312] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:49:34.312] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:49:34.312] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-7ed7947a-ba70-4f65-bdf6-4ec4f3359670
[2024-09-30 19:49:34.312] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:49:34.312] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:49:34.312] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:49:34.312] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:49:34.312] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:49:34.312] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:49:34.313] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:49:34.313] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:49:34.313] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:49:34.313] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:49:34.313] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:49:34.313] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:49:34.313] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:49:34.313] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:49:34.313] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:49:34.313] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:49:34.316] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:49:34.316] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:49:34.316] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:49:34.318] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:49:34.318] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:49:34.318] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:49:34.318] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:49:34.319] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:49:34.319] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:49:34.319] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:49:34.319] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:49:34.319] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:49:34.319] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:49:34.322] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:49:34.322] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:49:34.322] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:49:34.324] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:49:34.324] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:49:34.324] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:49:34.351] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:49:34.351] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1515530.40 ms
[2024-09-30 19:49:34.351] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:49:34.351] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      25.98 ms /     6 tokens (    4.33 ms per token,   230.95 tokens per second)
[2024-09-30 19:49:34.351] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:49:34.351] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1515529.50 ms /     7 tokens
[2024-09-30 19:49:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:49:34.351] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:49:34.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:49:34.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:49:34.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:49:34.358] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:49:34.358] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:49:34.358] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:49:34.358] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:49:34.358] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:49:34.358] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:49:34.358] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:49:34.358] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:49:34.376] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:49:34.376] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:49:34.377] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:49:34.377] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-7ed7947a-ba70-4f65-bdf6-4ec4f3359670
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:49:34.377] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:49:34.377] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:49:34.377] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:49:34.377] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:49:34.377] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:49:34.377] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:49:34.377] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:49:34.377] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:49:34.844] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:49:34.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:49:34.844] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:49:34.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:49:34.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:49:34.848] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:49:34.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:49:34.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:49:34.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:49:34.852] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 13
[2024-09-30 19:49:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:49:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:49:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:49:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:49:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:49:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:49:34.852] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:49:34.853] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:49:34.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:49:34.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:49:34.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:49:34.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:49:34.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:49:34.853] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:49:35.048] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:49:35.049] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:49:35.049] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:49:35.050] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:49:35.050] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:49:35.050] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:49:35.053] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:49:35.053] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:49:35.053] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:49:35.053] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:49:35.053] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:49:35.053] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:49:35.053] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:49:35.053] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:49:35.053] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:49:35.245] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:49:35.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:49:35.245] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:49:35.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:49:35.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:49:35.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:49:41.559] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:49:41.559] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:49:41.559] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1519579.15 ms
[2024-09-30 19:49:41.559] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.20 ms /    20 runs   (    0.21 ms per token,  4763.04 tokens per second)
[2024-09-30 19:49:41.559] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3052.40 ms /    24 tokens (  127.18 ms per token,     7.86 tokens per second)
[2024-09-30 19:49:41.559] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3253.80 ms /    19 runs   (  171.25 ms per token,     5.84 tokens per second)
[2024-09-30 19:49:41.559] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1522837.68 ms /    43 tokens
[2024-09-30 19:49:41.562] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:49:41.562] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 19:49:41.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 19:49:41.562] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:49:41.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 19:49:41.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:49:41.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:49:41.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:49:41.563] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:49:41.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:49:41.563] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:49:41.564] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:49:41.564] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:49:41.564] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:49:41.564] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 19:49:41.564] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:49:41.564] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:50:31.588] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:58636, local_addr: 0.0.0.0:8081
[2024-09-30 19:50:31.589] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:50:31.589] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:50:31.589] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:50:31.590] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:50:31.590] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-d1651ba8-b3de-4365-83ff-73f3b9217969
[2024-09-30 19:50:31.590] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:50:31.590] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:50:31.590] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:50:31.590] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:50:31.590] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:50:31.590] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:50:31.590] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:50:31.590] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:50:31.590] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:50:31.590] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:50:31.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:50:31.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:50:31.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:50:31.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:50:31.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:50:31.590] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:50:31.594] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:50:31.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:50:31.594] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:50:31.597] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:50:31.597] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:50:31.603] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:50:31.603] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:50:31.603] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:50:31.606] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:50:31.606] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:50:31.606] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:50:31.643] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:50:31.643] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1572822.31 ms
[2024-09-30 19:50:31.643] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:50:31.643] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      35.73 ms /     6 tokens (    5.95 ms per token,   167.94 tokens per second)
[2024-09-30 19:50:31.643] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:50:31.643] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1572821.50 ms /     7 tokens
[2024-09-30 19:50:31.643] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:50:31.643] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:50:31.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:50:31.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:50:31.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:50:31.649] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:50:31.649] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:50:31.649] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:50:31.650] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:50:31.650] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:50:31.650] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:50:31.650] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:50:31.650] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:50:31.667] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:50:31.667] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:50:31.667] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:50:31.667] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-d1651ba8-b3de-4365-83ff-73f3b9217969
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:50:31.667] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:50:31.667] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:50:31.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:50:31.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:50:31.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:50:31.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:50:31.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:50:31.667] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:50:32.280] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:50:32.280] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:50:32.280] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:50:32.286] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:50:32.287] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:50:32.287] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:50:32.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:50:32.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:50:32.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:50:32.298] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:50:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:50:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:50:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:50:32.298] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:50:32.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:50:32.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:50:32.299] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:50:32.299] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:50:32.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:50:32.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:50:32.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:50:32.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:50:32.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:50:32.299] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:50:32.628] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:50:32.629] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:50:32.629] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:50:32.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:50:32.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:50:32.630] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:50:32.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:50:32.633] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:50:32.633] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:50:32.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:50:32.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:50:32.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:50:32.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:50:32.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:50:32.633] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:50:32.807] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:50:32.807] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:50:32.807] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:50:32.808] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:50:32.808] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:50:32.808] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:50:41.399] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:50:41.399] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:50:41.400] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1577391.50 ms
[2024-09-30 19:50:41.400] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.91 ms /    24 runs   (    0.20 ms per token,  4890.97 tokens per second)
[2024-09-30 19:50:41.400] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3303.44 ms /    24 tokens (  137.64 ms per token,     7.27 tokens per second)
[2024-09-30 19:50:41.400] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    5281.33 ms /    23 runs   (  229.62 ms per token,     4.35 tokens per second)
[2024-09-30 19:50:41.400] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1582678.68 ms /    47 tokens
[2024-09-30 19:50:41.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:50:41.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 127
[2024-09-30 19:50:41.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence developed by Microsoft to engage in conversations and help users with information.<|end|>
[2024-09-30 19:50:41.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:50:41.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence developed by Microsoft to engage in conversations and help users with information.
[2024-09-30 19:50:41.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:50:41.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:50:41.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:50:41.405] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:50:41.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:50:41.405] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:50:41.406] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:50:41.406] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:50:41.406] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:50:41.406] [info] rag_api_server in src/main.rs:517: response_body_size: 431
[2024-09-30 19:50:41.406] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:50:41.406] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:51:31.429] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:57808, local_addr: 0.0.0.0:8081
[2024-09-30 19:51:31.430] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:51:31.430] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:51:31.430] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:51:31.430] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:51:31.430] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-21d17466-362f-456d-8adf-1b3234b59b9b
[2024-09-30 19:51:31.430] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:51:31.430] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:51:31.430] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:51:31.430] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:51:31.430] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:51:31.430] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:51:31.430] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:51:31.430] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:51:31.431] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:51:31.431] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:51:31.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:51:31.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:51:31.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:51:31.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:51:31.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:51:31.431] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:51:31.433] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:51:31.433] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:51:31.433] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:51:31.435] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:51:31.435] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:51:31.440] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:51:31.440] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:51:31.440] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:51:31.441] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:51:31.441] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:51:31.441] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:51:31.471] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:51:31.471] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1632650.38 ms
[2024-09-30 19:51:31.471] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:51:31.471] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      27.54 ms /     6 tokens (    4.59 ms per token,   217.83 tokens per second)
[2024-09-30 19:51:31.471] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:51:31.471] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1632649.50 ms /     7 tokens
[2024-09-30 19:51:31.471] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:51:31.471] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:51:31.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:51:31.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:51:31.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:51:31.478] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:51:31.478] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:51:31.478] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:51:31.478] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:51:31.478] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:51:31.478] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:51:31.478] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:51:31.479] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:51:31.499] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:51:31.499] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:51:31.499] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:51:31.499] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-21d17466-362f-456d-8adf-1b3234b59b9b
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:51:31.499] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:51:31.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:51:31.500] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:51:31.500] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:51:31.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:51:31.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:51:31.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:51:31.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:51:31.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:51:31.500] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:51:31.977] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:51:31.977] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:51:31.977] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:51:31.980] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:51:31.980] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:51:31.980] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:51:31.983] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:51:31.983] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:51:31.983] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:51:31.983] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:51:31.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:51:31.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:51:31.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:51:31.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:51:31.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:51:31.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:51:31.983] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:51:31.984] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:51:31.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:51:31.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:51:31.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:51:31.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:51:31.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:51:31.984] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:51:32.246] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:51:32.246] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:51:32.246] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:51:32.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:51:32.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:51:32.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:51:32.249] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:51:32.250] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:51:32.250] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:51:32.250] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:51:32.250] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:51:32.250] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:51:32.250] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:51:32.250] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:51:32.250] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:51:32.465] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:51:32.465] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:51:32.465] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:51:32.466] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:51:32.466] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:51:32.466] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:51:43.030] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:51:43.030] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:51:43.030] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1637081.45 ms
[2024-09-30 19:51:43.030] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       7.71 ms /    32 runs   (    0.24 ms per token,  4151.53 tokens per second)
[2024-09-30 19:51:43.030] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3336.12 ms /    24 tokens (  139.01 ms per token,     7.19 tokens per second)
[2024-09-30 19:51:43.030] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    7218.00 ms /    31 runs   (  232.84 ms per token,     4.29 tokens per second)
[2024-09-30 19:51:43.030] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1644308.68 ms /    55 tokens
[2024-09-30 19:51:43.032] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:51:43.032] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 141
[2024-09-30 19:51:43.032] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  Hello! I am Phi, an AI digital assistant designed to help you with various tasks and answer your queries. How can I assist you today?<|end|>
[2024-09-30 19:51:43.032] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:51:43.032] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
Hello! I am Phi, an AI digital assistant designed to help you with various tasks and answer your queries. How can I assist you today?
[2024-09-30 19:51:43.032] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:51:43.033] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:51:43.033] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:51:43.033] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 32
[2024-09-30 19:51:43.033] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 32
[2024-09-30 19:51:43.033] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:51:43.033] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:51:43.033] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:51:43.033] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:51:43.033] [info] rag_api_server in src/main.rs:517: response_body_size: 445
[2024-09-30 19:51:43.033] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:51:43.033] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:52:33.059] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:47230, local_addr: 0.0.0.0:8081
[2024-09-30 19:52:33.060] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:52:33.060] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:52:33.060] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:52:33.060] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:52:33.060] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-57a01ece-5ba6-45d6-88bf-706bc5351d55
[2024-09-30 19:52:33.060] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:52:33.060] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:52:33.061] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:52:33.061] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:52:33.061] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:52:33.061] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:52:33.061] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:52:33.061] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:52:33.061] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:52:33.061] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:52:33.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:52:33.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:52:33.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:52:33.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:52:33.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:52:33.061] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:52:33.063] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:52:33.063] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:52:33.063] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:52:33.066] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:52:33.066] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:52:33.071] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:52:33.071] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:52:33.071] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:52:33.073] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:52:33.073] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:52:33.073] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:52:33.105] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:52:33.105] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1694284.84 ms
[2024-09-30 19:52:33.105] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:52:33.105] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      31.62 ms /     6 tokens (    5.27 ms per token,   189.73 tokens per second)
[2024-09-30 19:52:33.105] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:52:33.105] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1694284.50 ms /     7 tokens
[2024-09-30 19:52:33.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:52:33.106] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:52:33.111] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:52:33.111] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:52:33.111] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:52:33.111] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:52:33.111] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:52:33.111] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:52:33.111] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:52:33.111] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:52:33.111] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:52:33.111] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:52:33.111] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:52:33.130] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:52:33.130] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:52:33.130] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:52:33.130] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-57a01ece-5ba6-45d6-88bf-706bc5351d55
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:52:33.130] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:52:33.130] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:52:33.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:52:33.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:52:33.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:52:33.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:52:33.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:52:33.130] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:52:33.651] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:52:33.651] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:52:33.651] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:52:33.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:52:33.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:52:33.654] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:52:33.657] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:52:33.657] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:52:33.657] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:52:33.657] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 32
[2024-09-30 19:52:33.658] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:52:33.658] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:52:33.658] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:52:33.658] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:52:33.658] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:52:33.658] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:52:33.658] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:52:33.658] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:52:33.658] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:52:33.658] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:52:33.658] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:52:33.658] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:52:33.658] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:52:33.658] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:52:33.869] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:52:33.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:52:33.869] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:52:33.870] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:52:33.870] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:52:33.870] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:52:33.873] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:52:33.874] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:52:33.874] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:52:33.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:52:33.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:52:33.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:52:33.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:52:33.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:52:33.874] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:52:34.070] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:52:34.070] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:52:34.070] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:52:34.071] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:52:34.071] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:52:34.071] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:52:42.002] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:52:42.002] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:52:42.002] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1698370.37 ms
[2024-09-30 19:52:42.002] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.96 ms /    28 runs   (    0.21 ms per token,  4700.35 tokens per second)
[2024-09-30 19:52:42.002] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    3019.85 ms /    24 tokens (  125.83 ms per token,     7.95 tokens per second)
[2024-09-30 19:52:42.002] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    4904.04 ms /    27 runs   (  181.63 ms per token,     5.51 tokens per second)
[2024-09-30 19:52:42.002] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1703281.68 ms /    51 tokens
[2024-09-30 19:52:42.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:52:42.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 108
[2024-09-30 19:52:42.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI created by Microsoft to assist and engage with users like you. How can I help today?<|end|>
[2024-09-30 19:52:42.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:52:42.007] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI created by Microsoft to assist and engage with users like you. How can I help today?
[2024-09-30 19:52:42.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:52:42.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:52:42.007] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:52:42.008] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:52:42.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:52:42.008] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:52:42.008] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:52:42.009] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:52:42.009] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:52:42.009] [info] rag_api_server in src/main.rs:517: response_body_size: 412
[2024-09-30 19:52:42.009] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:52:42.009] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:53:32.034] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:33878, local_addr: 0.0.0.0:8081
[2024-09-30 19:53:32.035] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:53:32.035] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:53:32.035] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:53:32.035] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:53:32.035] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-be506dbe-620e-4265-9de1-3c0e4448d247
[2024-09-30 19:53:32.035] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:53:32.035] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:53:32.035] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:53:32.035] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:53:32.036] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:53:32.036] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:53:32.036] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:53:32.036] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:53:32.036] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:53:32.036] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:53:32.036] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:53:32.036] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:53:32.036] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:53:32.036] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:53:32.036] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:53:32.036] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:53:32.038] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:53:32.038] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:53:32.038] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:53:32.040] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:53:32.040] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:53:32.044] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:53:32.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:53:32.044] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:53:32.046] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:53:32.046] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:53:32.046] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:53:32.069] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:53:32.069] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1753248.16 ms
[2024-09-30 19:53:32.069] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:53:32.069] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.99 ms /     6 tokens (    3.66 ms per token,   272.90 tokens per second)
[2024-09-30 19:53:32.069] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:53:32.069] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1753247.50 ms /     7 tokens
[2024-09-30 19:53:32.069] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:53:32.069] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:53:32.074] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:53:32.074] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:53:32.074] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:53:32.074] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:53:32.074] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:53:32.074] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:53:32.074] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:53:32.074] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:53:32.074] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:53:32.074] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:53:32.074] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:53:32.091] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:53:32.091] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:53:32.091] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:53:32.091] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-be506dbe-620e-4265-9de1-3c0e4448d247
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:53:32.091] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:53:32.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:53:32.092] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:53:32.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:53:32.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:53:32.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:53:32.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:53:32.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:53:32.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:53:32.531] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:53:32.531] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:53:32.531] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:53:32.534] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:53:32.534] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:53:32.534] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:53:32.537] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:53:32.537] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:53:32.537] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:53:32.537] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 28
[2024-09-30 19:53:32.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:53:32.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:53:32.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:53:32.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:53:32.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:53:32.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:53:32.537] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:53:32.537] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:53:32.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:53:32.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:53:32.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:53:32.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:53:32.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:53:32.537] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:53:32.692] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:53:32.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:53:32.692] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:53:32.694] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:53:32.694] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:53:32.694] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:53:32.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:53:32.696] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:53:32.696] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:53:32.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:53:32.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:53:32.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:53:32.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:53:32.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:53:32.696] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:53:32.855] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:53:32.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:53:32.855] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:53:32.856] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:53:32.856] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:53:32.856] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:53:38.446] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:53:38.446] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:53:38.446] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1756834.78 ms
[2024-09-30 19:53:38.446] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       3.75 ms /    20 runs   (    0.19 ms per token,  5331.91 tokens per second)
[2024-09-30 19:53:38.446] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2699.35 ms /    24 tokens (  112.47 ms per token,     8.89 tokens per second)
[2024-09-30 19:53:38.446] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    2886.07 ms /    19 runs   (  151.90 ms per token,     6.58 tokens per second)
[2024-09-30 19:53:38.446] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1759725.68 ms /    43 tokens
[2024-09-30 19:53:38.449] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:53:38.449] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 75
[2024-09-30 19:53:38.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an AI developed by Microsoft. How can I assist you today?<|end|>
[2024-09-30 19:53:38.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:53:38.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an AI developed by Microsoft. How can I assist you today?
[2024-09-30 19:53:38.449] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:53:38.449] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:53:38.449] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:53:38.449] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:53:38.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:53:38.449] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:53:38.450] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:53:38.450] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:53:38.450] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:53:38.450] [info] rag_api_server in src/main.rs:517: response_body_size: 379
[2024-09-30 19:53:38.450] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:53:38.450] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:54:28.472] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46094, local_addr: 0.0.0.0:8081
[2024-09-30 19:54:28.473] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:54:28.473] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:54:28.473] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:54:28.473] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:54:28.473] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-33df38e7-915c-4b4b-862c-3518d28dc4d6
[2024-09-30 19:54:28.473] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:54:28.473] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:54:28.473] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:54:28.473] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:54:28.473] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:54:28.473] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:54:28.473] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:54:28.473] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:54:28.474] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:54:28.474] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:54:28.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:54:28.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:54:28.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:54:28.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:54:28.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:54:28.474] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:54:28.476] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:54:28.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:54:28.476] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:54:28.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:54:28.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:54:28.477] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:54:28.478] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:54:28.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:54:28.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:54:28.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:54:28.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:54:28.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:54:28.478] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:54:28.481] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:54:28.481] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:54:28.481] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:54:28.483] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:54:28.483] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:54:28.483] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:54:28.506] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:54:28.506] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1809685.57 ms
[2024-09-30 19:54:28.506] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:54:28.506] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      22.43 ms /     6 tokens (    3.74 ms per token,   267.53 tokens per second)
[2024-09-30 19:54:28.506] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:54:28.506] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1809685.50 ms /     7 tokens
[2024-09-30 19:54:28.507] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:54:28.507] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:54:28.511] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:54:28.511] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:54:28.511] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:54:28.511] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:54:28.511] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:54:28.511] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:54:28.512] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:54:28.512] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:54:28.512] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:54:28.512] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:54:28.512] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:54:28.528] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:54:28.528] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:54:28.528] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:54:28.528] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-33df38e7-915c-4b4b-862c-3518d28dc4d6
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:54:28.528] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:54:28.528] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:54:28.528] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:54:28.528] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:54:28.528] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:54:28.528] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:54:28.528] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:54:28.528] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:54:28.927] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:54:28.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:54:28.927] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:54:28.930] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:54:28.930] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:54:28.930] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:54:28.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:54:28.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:54:28.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:54:28.933] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 20
[2024-09-30 19:54:28.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:54:28.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:54:28.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:54:28.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:54:28.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:54:28.933] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:54:28.934] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:54:28.934] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:54:28.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:54:28.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:54:28.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:54:28.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:54:28.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:54:28.934] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:54:29.088] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:54:29.088] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:54:29.088] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:54:29.089] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:54:29.089] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:54:29.089] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:54:29.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:54:29.092] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:54:29.092] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:54:29.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:54:29.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:54:29.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:54:29.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:54:29.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:54:29.092] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:54:29.246] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:54:29.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:54:29.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:54:29.247] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:54:29.248] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:54:29.248] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:54:35.226] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:54:35.226] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:54:35.226] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1813069.97 ms
[2024-09-30 19:54:35.226] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       4.80 ms /    24 runs   (    0.20 ms per token,  5001.04 tokens per second)
[2024-09-30 19:54:35.226] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2542.71 ms /    24 tokens (  105.95 ms per token,     9.44 tokens per second)
[2024-09-30 19:54:35.226] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3429.21 ms /    23 runs   (  149.10 ms per token,     6.71 tokens per second)
[2024-09-30 19:54:35.226] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1816504.68 ms /    47 tokens
[2024-09-30 19:54:35.228] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:54:35.228] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 117
[2024-09-30 19:54:35.228] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, an artificial intelligence designed to assist with information and tasks. How can I help you today?<|end|>
[2024-09-30 19:54:35.228] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:54:35.228] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, an artificial intelligence designed to assist with information and tasks. How can I help you today?
[2024-09-30 19:54:35.228] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:54:35.229] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:54:35.229] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:54:35.229] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:54:35.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:54:35.229] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:54:35.229] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:54:35.229] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:54:35.229] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:54:35.229] [info] rag_api_server in src/main.rs:517: response_body_size: 421
[2024-09-30 19:54:35.229] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:54:35.229] [info] rag_api_server in src/main.rs:521: response_is_success: true
[2024-09-30 19:55:25.252] [info] rag_api_server in src/main.rs:443: remote_addr: 0.0.0.0:46210, local_addr: 0.0.0.0:8081
[2024-09-30 19:55:25.253] [info] rag_api_server in src/main.rs:498: method: GET, http_version: HTTP/1.1
[2024-09-30 19:55:25.253] [info] rag_api_server in src/main.rs:499: endpoint: /v1/chat/completions
[2024-09-30 19:55:25.253] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:189: Handling the coming rag query request
[2024-09-30 19:55:25.253] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:212: Prepare the chat completion request.
[2024-09-30 19:55:25.253] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:248: user: chatcmpl-ed13be2b-3cbf-4413-8861-1bd0401e4acd
[2024-09-30 19:55:25.253] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:262: Compute embeddings for user query.
[2024-09-30 19:55:25.253] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:291: query text: Who are you?
[2024-09-30 19:55:25.253] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:51: Get the names of the embedding models.
[2024-09-30 19:55:25.253] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:98: Compute embeddings for the user query.
[2024-09-30 19:55:25.253] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:55:25.253] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:55:25.253] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:28: Computing embeddings
[2024-09-30 19:55:25.253] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:55:25.253] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:55:25.253] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:133: Compute embeddings for 1 chunks
[2024-09-30 19:55:25.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:55:25.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:55:25.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:55:25.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:55:25.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:55:25.253] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:55:25.255] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:55:25.255] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:55:25.255] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:55:25.258] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:153: compute embeddings for chunk 1
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 512
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 512
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 512
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 1000.0
[2024-09-30 19:55:25.258] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:55:25.262] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB
[2024-09-30 19:55:25.262] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB
[2024-09-30 19:55:25.262] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
[2024-09-30 19:55:25.264] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =    23.00 MiB
[2024-09-30 19:55:25.264] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 453
[2024-09-30 19:55:25.264] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:55:25.287] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:55:25.287] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1866465.93 ms
[2024-09-30 19:55:25.287] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:55:25.287] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =      21.62 ms /     6 tokens (    3.60 ms per token,   277.59 tokens per second)
[2024-09-30 19:55:25.287] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
[2024-09-30 19:55:25.287] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1866465.50 ms /     7 tokens
[2024-09-30 19:55:25.287] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:55:25.287] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 11273
[2024-09-30 19:55:25.292] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Nomic-embed-text-v1.5.
[2024-09-30 19:55:25.292] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Nomic-embed-text-v1.5
[2024-09-30 19:55:25.292] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 95
[2024-09-30 19:55:25.292] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 6, completion tokens: 0
[2024-09-30 19:55:25.292] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:211: token usage of embeddings: 6 prompt tokens, 0 comletion tokens
[2024-09-30 19:55:25.292] [info] llama_core::embeddings in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/embeddings.rs:123: Embeddings computed successfully.
[2024-09-30 19:55:25.292] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:133: Retrieve context.
[2024-09-30 19:55:25.292] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:135: qdrant_url: http://127.0.0.1:6333, qdrant_collection_name: default, limit: 1, score_threshold: 0.5
[2024-09-30 19:55:25.292] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:55:25.292] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:55:25.292] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:281: Search similar points from the qdrant instance.
[2024-09-30 19:55:25.308] [info] llama_core::rag in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/rag.rs:294: Number of similar points found: 0
[2024-09-30 19:55:25.308] [warning] rag_api_server::backend::ggml in src/backend/ggml.rs:436: No point retrieved (score < threshold 0.5)
[2024-09-30 19:55:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:45: tool choice: Some(None)
[2024-09-30 19:55:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:46: tools: None
[2024-09-30 19:55:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:47: stream mode: Some(false)
[2024-09-30 19:55:25.308] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:622: Processing chat completion request in non-stream mode.
[2024-09-30 19:55:25.308] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:698: Get the running mode.
[2024-09-30 19:55:25.308] [info] llama_core in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/lib.rs:723: running mode: rag
[2024-09-30 19:55:25.309] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:644: user: chatcmpl-ed13be2b-3cbf-4413-8861-1bd0401e4acd
[2024-09-30 19:55:25.309] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1604: Check model metadata.
[2024-09-30 19:55:25.309] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:55:25.309] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1903: Build the chat prompt from the chat messages.
[2024-09-30 19:55:25.309] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2267: Get the model metadata.
[2024-09-30 19:55:25.309] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:55:25.309] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:55:25.309] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:55:25.309] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:55:25.309] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:55:25.309] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:55:25.309] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:55:25.309] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:55:25.732] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:55:25.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:55:25.732] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:55:25.735] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:55:25.735] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:55:25.735] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:55:25.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:55:25.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:55:25.738] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:55:25.739] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 24
[2024-09-30 19:55:25.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:655: prompt:
<|system|>
You are a friendly chatbot.<|end|>
<|user|>
Who are you?<|end|>
<|assistant|>
[2024-09-30 19:55:25.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:656: available_completion_tokens: 820
[2024-09-30 19:55:25.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:657: tool_use: false
[2024-09-30 19:55:25.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1709: available_completion_tokens: 820, max_tokens from request: 1024, n_predict: 1024
[2024-09-30 19:55:25.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1719: update n_predict from 1024 to 820
[2024-09-30 19:55:25.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2327: Update the model metadata.
[2024-09-30 19:55:25.739] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:2201: Set prompt to the chat model named Phi-3-mini-4k-instruct.
[2024-09-30 19:55:25.739] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:55:25.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:55:25.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:55:25.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:55:25.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:55:25.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:55:25.739] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:55:25.936] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:55:25.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:55:25.936] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:55:25.938] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:55:25.938] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:55:25.938] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:55:25.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:681: Compute chat completion.
[2024-09-30 19:55:25.942] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:742: Compute chat completion by the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:55:25.942] [warning] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32
[2024-09-30 19:55:25.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ctx      = 4096
[2024-09-30 19:55:25.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_batch    = 32
[2024-09-30 19:55:25.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: n_ubatch   = 16
[2024-09-30 19:55:25.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: flash_attn = 0
[2024-09-30 19:55:25.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_base  = 10000.0
[2024-09-30 19:55:25.942] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: freq_scale = 1
[2024-09-30 19:55:26.142] [info] [WASI-NN] llama.cpp: llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB
[2024-09-30 19:55:26.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB
[2024-09-30 19:55:26.142] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
[2024-09-30 19:55:26.143] [info] [WASI-NN] llama.cpp: llama_new_context_with_model:        CPU compute buffer size =     9.63 MiB
[2024-09-30 19:55:26.143] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph nodes  = 1286
[2024-09-30 19:55:26.143] [info] [WASI-NN] llama.cpp: llama_new_context_with_model: graph splits = 1
[2024-09-30 19:55:32.708] [info] [WASI-NN] GGML backend: EOS token found
[2024-09-30 19:55:32.708] [info] [WASI-NN] llama.cpp: 
[2024-09-30 19:55:32.708] [info] [WASI-NN] llama.cpp: llama_print_timings:        load time = 1870083.38 ms
[2024-09-30 19:55:32.708] [info] [WASI-NN] llama.cpp: llama_print_timings:      sample time =       5.16 ms /    25 runs   (    0.21 ms per token,  4842.15 tokens per second)
[2024-09-30 19:55:32.708] [info] [WASI-NN] llama.cpp: llama_print_timings: prompt eval time =    2660.66 ms /    24 tokens (  110.86 ms per token,     9.02 tokens per second)
[2024-09-30 19:55:32.708] [info] [WASI-NN] llama.cpp: llama_print_timings:        eval time =    3897.33 ms /    24 runs   (  162.39 ms per token,     6.16 tokens per second)
[2024-09-30 19:55:32.708] [info] [WASI-NN] llama.cpp: llama_print_timings:       total time = 1873986.68 ms /    48 tokens
[2024-09-30 19:55:32.710] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:55:32.710] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 113
[2024-09-30 19:55:32.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:761: raw generation:  I am Phi, a digital assistant created to help you with information and tasks. How may I assist you today?<|end|>
[2024-09-30 19:55:32.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:1753: Post-process the generated output.
[2024-09-30 19:55:32.710] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:769: post-processed generation:
I am Phi, a digital assistant created to help you with information and tasks. How may I assist you today?
[2024-09-30 19:55:32.710] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:243: Get token info from the model named Phi-3-mini-4k-instruct.
[2024-09-30 19:55:32.710] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:169: Get the output buffer generated by the model named Phi-3-mini-4k-instruct
[2024-09-30 19:55:32.710] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:187: Output buffer size: 97
[2024-09-30 19:55:32.711] [info] llama_core::utils in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/utils.rs:282: prompt tokens: 24, completion tokens: 25
[2024-09-30 19:55:32.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:775: prompt tokens: 24, completion tokens: 25
[2024-09-30 19:55:32.711] [info] llama_core::chat in /home/runner/.cargo/git/checkouts/llamaedge-4f9ea6368fc01861/6a4103b/crates/llama-core/src/chat.rs:670: End of the chat completion.
[2024-09-30 19:55:32.711] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:502: Finish chat completions in non-stream mode
[2024-09-30 19:55:32.711] [info] rag_api_server::backend::ggml in src/backend/ggml.rs:529: Send the rag query response
[2024-09-30 19:55:32.711] [info] rag_api_server in src/main.rs:515: response_version: HTTP/1.1
[2024-09-30 19:55:32.711] [info] rag_api_server in src/main.rs:517: response_body_size: 417
[2024-09-30 19:55:32.711] [info] rag_api_server in src/main.rs:519: response_status: 200
[2024-09-30 19:55:32.711] [info] rag_api_server in src/main.rs:521: response_is_success: true
